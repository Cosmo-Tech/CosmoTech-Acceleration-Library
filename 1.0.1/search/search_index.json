{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cosmotech Acceleration library","text":"<p>Acceleration library for CosmoTech cloud-based solution development.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The CosmoTech Acceleration Library (CoAL) provides a comprehensive set of tools and utilities to accelerate the development of solutions based on the CosmoTech platform. It offers a unified interface for interacting with CosmoTech APIs, managing data, and integrating with various cloud services.</p>"},{"location":"#main-components","title":"Main Components","text":""},{"location":"#csm-data","title":"csm-data","text":"<p><code>csm-data</code> is a powerful CLI tool designed to help CosmoTech solution modelers and integrators interact with multiple systems. It provides ready-to-use commands to send and retrieve data from various systems where a CosmoTech API could be integrated.</p> <pre><code># Get help on available commands\ncsm-data --help\n\n# Get help on specific command groups\ncsm-data api --help\n</code></pre>"},{"location":"#datastore","title":"datastore","text":"<p>The datastore provides a way to maintain local data during simulations and comes with <code>csm-data</code> commands to easily send those data to target systems. It offers:</p> <ul> <li>Format flexibility (Python dictionaries, CSV files, Pandas DataFrames, PyArrow Tables)</li> <li>Persistent storage in SQLite</li> <li>SQL query capabilities</li> <li>Simplified data pipeline management</li> </ul> <pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\n# Initialize and reset the data store\nmy_datastore = Store(reset=True)\n\n# Create and store data\nmy_data = [{\"foo\": \"bar\"}, {\"foo\": \"barbar\"}, {\"foo\": \"world\"}, {\"foo\": \"bar\"}]\nstore_pylist(\"my_data\", my_data)\n\n# Query the data\nresults = my_datastore.execute_query(\"SELECT foo, count(*) as line_count FROM my_data GROUP BY foo\").to_pylist()\nprint(results)\n# &gt; [{'foo': 'bar', 'line_count': 2}, {'foo': 'barbar', 'line_count': 1}, {'foo': 'world', 'line_count': 1}]\n</code></pre>"},{"location":"#cosmotech-api-integration","title":"CosmoTech API Integration","text":"<p>CoAL provides comprehensive tools for interacting with the CosmoTech API, allowing you to:</p> <ul> <li>Authenticate with different identity providers (API Key, Azure Entra, Keycloak)</li> <li>Manage workspaces and files</li> <li>Work with the Twin Data Layer for graph data</li> <li>Handle runners and runs</li> <li>Process and transform data</li> <li>Build end-to-end workflows</li> </ul> <pre><code>import os\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Get the API client\napi_client, connection_type = get_api_client()\nprint(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\nfrom cosmotech_api.api.organization_api import OrganizationApi\norg_api = OrganizationApi(api_client)\n\n# List organizations\norganizations = org_api.find_all_organizations()\nfor org in organizations:\n    print(f\"Organization: {org.name} (ID: {org.id})\")\n\n# Don't forget to close the client when done\napi_client.close()\n</code></pre>"},{"location":"#other-components","title":"Other Components","text":"<ul> <li>coal: Core library with modules for API interaction, data management, etc.</li> <li>csm_data: CLI tool for data management and integration with various systems</li> <li>orchestrator_plugins: Plugins that integrate with external orchestration systems</li> <li>translation: Internationalization support for multiple languages</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install cosmotech-acceleration-library\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Check out the tutorials directory for comprehensive examples of how to use the library:</p> <ul> <li>CosmoTech API Integration</li> <li>Data Store Usage</li> <li>csm-data CLI</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#cloud-service-integration","title":"Cloud Service Integration","text":"<p>CoAL provides built-in support for various cloud services:</p> <ul> <li>Azure: Azure Data Explorer (ADX), Azure Storage, Azure Functions</li> <li>AWS: S3 buckets, and more</li> <li>Database Systems: PostgreSQL, SingleStore, and others</li> </ul>"},{"location":"#data-management","title":"Data Management","text":"<ul> <li>Load and transform data from various sources</li> <li>Store and query data locally</li> <li>Export data to different formats and destinations</li> <li>Manage datasets in the CosmoTech platform</li> </ul>"},{"location":"#orchestration-integration","title":"Orchestration Integration","text":"<ul> <li>Provides plugins that integrate with external orchestration systems</li> <li>Supports data transfer between orchestration steps</li> <li>Offers utilities for handling parameters and configurations</li> <li>Enables seamless integration with the CosmoTech platform during orchestrated workflows</li> </ul>"},{"location":"#documentation-and-tutorials","title":"Documentation and Tutorials","text":"<p>Comprehensive documentation is available at https://cosmo-tech.github.io/CosmoTech-Acceleration-Library/</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>CosmoTech API: Learn how to interact with the CosmoTech API directly: authentication, workspaces, Twin Data Layer, and more.</li> <li>Data Store: The datastore is your friend to keep data between orchestration steps. It comes with multiple ways to interact with it.</li> <li>csm-data: Make full use of <code>csm-data</code> commands to connect to services during your orchestration runs.</li> </ul>"},{"location":"#testing-and-code-coverage","title":"Testing and Code Coverage","text":"<p>The CosmoTech Acceleration Library maintains a comprehensive test suite to ensure reliability and stability. We use pytest for testing and pytest-cov for coverage reporting.</p>"},{"location":"#running-tests","title":"Running Tests","text":"<p>To run the test suite:</p> <pre><code># Install test dependencies\npip install -e \".[test]\"\n\n# Run tests with coverage reporting\npytest tests/unit/coal/ --cov=cosmotech.coal --cov-report=term-missing --cov-report=html\n</code></pre>"},{"location":"#coverage-reports","title":"Coverage Reports","text":"<p>After running tests with coverage, you can view detailed HTML reports:</p> <pre><code># Open the HTML coverage report\nopen coverage_html_report/index.html\n</code></pre> <p></p> <p>We maintain high test coverage to ensure code quality and reliability. All pull requests are expected to maintain or improve the current coverage levels.</p>"},{"location":"#test-generation-tools","title":"Test Generation Tools","text":"<p>To help maintain test coverage, we provide tools to identify untested functions and generate test files:</p> <pre><code># Find functions without tests\npython find_untested_functions.py\n\n# Generate test files for a specific module\npython generate_test_files.py --module cosmotech/coal/module/file.py\n\n# Generate test files for all untested functions\npython generate_test_files.py --all\n</code></pre> <p>These tools help ensure that every function has at least one test, which is a requirement for contributions to the project.</p>"},{"location":"#contact","title":"Contact","text":"<p>For support, feature requests, or contributions, please use the GitHub repository.</p>"},{"location":"dependencies/","title":"List of dependencies","text":"<p>Azure connection requirements </p> <p>Keycloak connection </p> <p>Cosmotech specific requirements </p> <p>Commands requirements </p> <p>Orchestrator templates requirements </p> <p>Data store requirements </p> <p>CLI requirements </p> <p>Other requirements   Documentation generation   Extra requirements   Test requirements   Development requirements </p>"},{"location":"pull_request/","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, make sure you've completed all the necessary steps:</p> <ol> <li>Code Quality<ul> <li> Code follows the project's style guidelines (Black formatting)</li> <li> All linting checks pass</li> <li> Code is well-documented with docstrings</li> <li> Code is efficient and follows best practices</li> <li> No unnecessary dependencies are added</li> </ul> </li> <li>Testing<ul> <li> All unit tests pass</li> <li> Test coverage meets or exceeds 80%</li> <li> All functions have at least one test</li> <li> Edge cases and error conditions are tested</li> <li> Mocks are used for external services</li> </ul> </li> <li>Documentation<ul> <li> API documentation is updated</li> <li> Command help text is clear and comprehensive</li> <li> Translation strings are added for all user-facing text</li> <li> Usage examples are provided</li> <li> Any necessary tutorials are created or updated</li> </ul> </li> <li>Integration<ul> <li> New functionality integrates well with existing code</li> <li> No breaking changes to existing APIs</li> <li> Dependencies are properly specified in pyproject.toml</li> <li> Command is registered in the appropriate init.py file</li> </ul> </li> <li>Pull Request Description<ul> <li> Clear description of the changes</li> <li> Explanation of why the changes are needed</li> <li> Any potential issues or limitations</li> <li> References to related issues or discussions</li> </ul> </li> </ol>"},{"location":"csm-data/","title":"csm-data","text":"<p>Help command</p> <pre><code>&gt; csm-data --help\n\n Usage: csm-data [OPTIONS] COMMAND [ARGS]...                                    \n\n Cosmo Tech Data Interface                                                      \n Command toolkit providing quick implementation of data connections to use      \n inside the Cosmo Tech Platform                                                 \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --log-level    LVL  Either CRITICAL, ERROR, WARNING, INFO or DEBUG           \u2502\n\u2502                     ENV: LOG_LEVEL                                           \u2502\n\u2502 --version           Print version number and return.                         \u2502\n\u2502 --web-help          Open the web documentation                               \u2502\n\u2502 --help              Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 adx-send-data        Send data to ADX                                        \u2502\n\u2502 adx-send-runnerdata  Uses environment variables to send content of CSV files \u2502\n\u2502                      to ADX Requires a valid Azure connection either with:   \u2502\n\u2502                                                                              \u2502\n\u2502                       \u2022 The AZ cli command: az login                         \u2502\n\u2502                       \u2022 A triplet of env var AZURE_TENANT_ID,                \u2502\n\u2502                         AZURE_CLIENT_ID, AZURE_CLIENT_SECRET                 \u2502\n\u2502 api                  Cosmo Tech API helper command                           \u2502\n\u2502 az-storage-upload    Upload a folder to an Azure Storage Blob                \u2502\n\u2502 s3-bucket-delete     Delete S3 bucket content to a given folder              \u2502\n\u2502 s3-bucket-download   Download S3 bucket content to a given folder            \u2502\n\u2502 s3-bucket-upload     Upload a folder to a S3 Bucket                          \u2502\n\u2502 store                CoAL Data Store command group                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/adx-send-data/","title":"adx-send-data","text":"<p>Help command</p> <pre><code>&gt; csm-data adx-send-data --help\n\n Usage: csm-data adx-send-data [OPTIONS]                                        \n\n Send data to ADX                                                               \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --adx-uri           URI   The ADX cluster path (URI info can be found     \u2502\n\u2502                              into ADX cluster page)                          \u2502\n\u2502                              ENV: AZURE_DATA_EXPLORER_RESOURCE_URI           \u2502\n\u2502                              [required]                                      \u2502\n\u2502 *  --adx-ingest-uri    URI   The ADX cluster ingest path (URI info can be    \u2502\n\u2502                              found into ADX cluster page)                    \u2502\n\u2502                              ENV: AZURE_DATA_EXPLORER_RESOURCE_INGEST_URI    \u2502\n\u2502                              [required]                                      \u2502\n\u2502 *  --database-name     NAME  The targeted database name                      \u2502\n\u2502                              ENV: AZURE_DATA_EXPLORER_DATABASE_NAME          \u2502\n\u2502                              [required]                                      \u2502\n\u2502    --wait/--no-wait          Toggle waiting for the ingestion results        \u2502\n\u2502                              ENV: CSM_DATA_ADX_WAIT_INGESTION                \u2502\n\u2502                              DEFAULT: no-wait                                \u2502\n\u2502    --tag               TEXT  The ADX tag to use for the ingestion            \u2502\n\u2502                              ENV: CSM_DATA_ADX_TAG                           \u2502\n\u2502 *  --store-folder      PATH  The folder containing the datastore containing  \u2502\n\u2502                              the data to send                                \u2502\n\u2502                              ENV: CSM_PARAMETERS_ABSOLUTE_PATH               \u2502\n\u2502                              [required]                                      \u2502\n\u2502    --help                    Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/adx-send-runnerdata/","title":"adx-send-runnerdata","text":"<p>Help command</p> <pre><code>&gt; csm-data adx-send-runnerdata --help\n\n Usage: csm-data adx-send-runnerdata [OPTIONS]                                  \n\n Uses environment variables to send content of CSV files to ADX Requires a      \n valid Azure connection either with:                                            \n\n  \u2022 The AZ cli command: az login                                                \n  \u2022 A triplet of env var AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET  \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --dataset-absolute-path            PATH  A local folder to store the main \u2502\n\u2502                                             dataset content                  \u2502\n\u2502                                             ENV: CSM_DATASET_ABSOLUTE_PATH   \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --parameters-absolute-path         PATH  A local folder to store the      \u2502\n\u2502                                             parameters content               \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_PARAMETERS_ABSOLUTE_PATH     \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --runner-id                        UUID  the Runner Id to add to records  \u2502\n\u2502                                             ENV: CSM_RUNNER_ID               \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --adx-uri                          URI   the ADX cluster path (URI info   \u2502\n\u2502                                             can be found into ADX cluster    \u2502\n\u2502                                             page)                            \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_RESOURCE_URI \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --adx-ingest-uri                   URI   The ADX cluster ingest path (URI \u2502\n\u2502                                             info can be found into ADX       \u2502\n\u2502                                             cluster page)                    \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_RESOURCE_IN\u2026 \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --database-name                    NAME  The targeted database name       \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_DATABASE_NA\u2026 \u2502\n\u2502                                             [required]                       \u2502\n\u2502    --send-parameters/--no-send-pa\u2026          whether or not to send           \u2502\n\u2502                                             parameters (parameters path is   \u2502\n\u2502                                             mandatory then)                  \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_SEND_DATAWAREHOUSE_PARAMETE\u2026 \u2502\n\u2502                                             DEFAULT: no-send-parameters      \u2502\n\u2502    --send-datasets/--no-send-data\u2026          whether or not to send datasets  \u2502\n\u2502                                             (parameters path is mandatory    \u2502\n\u2502                                             then)                            \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_SEND_DATAWAREHOUSE_DATASETS  \u2502\n\u2502                                             DEFAULT: no-send-datasets        \u2502\n\u2502    --wait/--no-wait                         Toggle waiting for the ingestion \u2502\n\u2502                                             results                          \u2502\n\u2502                                             ENV: WAIT_FOR_INGESTION          \u2502\n\u2502                                             DEFAULT: no-wait                 \u2502\n\u2502    --help                                   Show this message and exit.      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/az-storage-upload/","title":"az-storage-upload","text":"<p>Help command</p> <pre><code>&gt; csm-data az-storage-upload --help\n\n Usage: csm-data az-storage-upload [OPTIONS]                                    \n\n Upload a folder to an Azure Storage Blob                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder               PATH    The folder/file to upload to the    \u2502\n\u2502                                          target blob storage                 \u2502\n\u2502                                          ENV: CSM_DATASET_ABSOLUTE_PATH      \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --recursive/--no-recursive            Recursively send the content of     \u2502\n\u2502                                          every folder inside the starting    \u2502\n\u2502                                          folder to the blob storage          \u2502\n\u2502 *  --blob-name                   BUCKET  The blob name in the Azure Storage  \u2502\n\u2502                                          service to upload to                \u2502\n\u2502                                          ENV: AZURE_STORAGE_BLOB_NAME        \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --prefix                      PREFIX  A prefix by which all uploaded      \u2502\n\u2502                                          files should start with in the blob \u2502\n\u2502                                          storage                             \u2502\n\u2502                                          ENV: CSM_DATA_BLOB_PREFIX           \u2502\n\u2502    --az-storage-sas-url          URL     SAS url allowing access to the AZ   \u2502\n\u2502                                          storage container                   \u2502\n\u2502                                          ENV: AZURE_STORAGE_SAS_URL          \u2502\n\u2502    --web-help                            Open the web documentation          \u2502\n\u2502    --help                                Show this message and exit.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-delete/","title":"s3-bucket-delete","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-delete --help\n\n Usage: csm-data s3-bucket-delete [OPTIONS]                                     \n\n Delete S3 bucket content to a given folder                                     \n Will delete everything in the bucket unless a prefix is set, then only file    \n following the given prefix will be deleted                                     \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bucket-name         BUCKET  The bucket on S3 to delete                  \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_NAME                   \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --prefix-filter       PREFIX  A prefix by which all deleted files should  \u2502\n\u2502                                  start in the bucket                         \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_PREFIX                 \u2502\n\u2502    --use-ssl/--no-ssl            Use SSL to secure connection to S3          \u2502\n\u2502 *  --s3-url              URL     URL to connect to the S3 system             \u2502\n\u2502                                  ENV: AWS_ENDPOINT_URL                       \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --access-id           ID      Identity used to connect to the S3 system   \u2502\n\u2502                                  ENV: AWS_ACCESS_KEY_ID                      \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --secret-key          ID      Secret tied to the ID used to connect to    \u2502\n\u2502                                  the S3 system                               \u2502\n\u2502                                  ENV: AWS_SECRET_ACCESS_KEY                  \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --ssl-cert-bundle     PATH    Path to an alternate CA Bundle to validate  \u2502\n\u2502                                  SSL connections                             \u2502\n\u2502                                  ENV: CSM_S3_CA_BUNDLE                       \u2502\n\u2502    --web-help                    Open the web documentation                  \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-download/","title":"s3-bucket-download","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-download --help\n\n Usage: csm-data s3-bucket-download [OPTIONS]                                   \n\n Download S3 bucket content to a given folder                                   \n Will download everything in the bucket unless a prefix is set, then only file  \n following the given prefix will be downloaded                                  \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --target-folder       PATH    The folder in which to download the bucket  \u2502\n\u2502                                  content                                     \u2502\n\u2502                                  ENV: CSM_DATASET_ABSOLUTE_PATH              \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --bucket-name         BUCKET  The bucket on S3 to download                \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_NAME                   \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --prefix-filter       PREFIX  A prefix by which all downloaded files      \u2502\n\u2502                                  should start in the bucket                  \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_PREFIX                 \u2502\n\u2502    --use-ssl/--no-ssl            Use SSL to secure connection to S3          \u2502\n\u2502 *  --s3-url              URL     URL to connect to the S3 system             \u2502\n\u2502                                  ENV: AWS_ENDPOINT_URL                       \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --access-id           ID      Identity used to connect to the S3 system   \u2502\n\u2502                                  ENV: AWS_ACCESS_KEY_ID                      \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --secret-key          ID      Secret tied to the ID used to connect to    \u2502\n\u2502                                  the S3 system                               \u2502\n\u2502                                  ENV: AWS_SECRET_ACCESS_KEY                  \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --ssl-cert-bundle     PATH    Path to an alternate CA Bundle to validate  \u2502\n\u2502                                  SSL connections                             \u2502\n\u2502                                  ENV: CSM_S3_CA_BUNDLE                       \u2502\n\u2502    --web-help                    Open the web documentation                  \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-upload/","title":"s3-bucket-upload","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-upload --help\n\n Usage: csm-data s3-bucket-upload [OPTIONS]                                     \n\n Upload a folder to a S3 Bucket                                                 \n Will upload everything from a given folder to a S3 bucket. If a single file is \n passed only it will be uploaded, and recursive will be ignored                 \n\n Giving a prefix will add it to every upload (finishing the prefix with a \"/\"   \n will allow to upload in a folder inside the bucket)                            \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder               PATH    The folder/file to upload to the    \u2502\n\u2502                                          target bucket                       \u2502\n\u2502                                          ENV: CSM_DATASET_ABSOLUTE_PATH      \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --recursive/--no-recursive            Recursively send the content of     \u2502\n\u2502                                          every folder inside the starting    \u2502\n\u2502                                          folder to the bucket                \u2502\n\u2502 *  --bucket-name                 BUCKET  The bucket on S3 to upload to       \u2502\n\u2502                                          ENV: CSM_DATA_BUCKET_NAME           \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --prefix                      PREFIX  A prefix by which all uploaded      \u2502\n\u2502                                          files should start with in the      \u2502\n\u2502                                          bucket                              \u2502\n\u2502                                          ENV: CSM_DATA_BUCKET_PREFIX         \u2502\n\u2502    --use-ssl/--no-ssl                    Use SSL to secure connection to S3  \u2502\n\u2502 *  --s3-url                      URL     URL to connect to the S3 system     \u2502\n\u2502                                          ENV: AWS_ENDPOINT_URL               \u2502\n\u2502                                          [required]                          \u2502\n\u2502 *  --access-id                   ID      Identity used to connect to the S3  \u2502\n\u2502                                          system                              \u2502\n\u2502                                          ENV: AWS_ACCESS_KEY_ID              \u2502\n\u2502                                          [required]                          \u2502\n\u2502 *  --secret-key                  ID      Secret tied to the ID used to       \u2502\n\u2502                                          connect to the S3 system            \u2502\n\u2502                                          ENV: AWS_SECRET_ACCESS_KEY          \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --ssl-cert-bundle             PATH    Path to an alternate CA Bundle to   \u2502\n\u2502                                          validate SSL connections            \u2502\n\u2502                                          ENV: CSM_S3_CA_BUNDLE               \u2502\n\u2502    --web-help                            Open the web documentation          \u2502\n\u2502    --help                                Show this message and exit.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/","title":"api","text":"<p>Help command</p> <pre><code>&gt; csm-data api --help\n\n Usage: csm-data api [OPTIONS] COMMAND [ARGS]...                                \n\n Cosmo Tech API helper command                                                  \n This command will inform you of which connection is available to use for the   \n Cosmo Tech API                                                                 \n\n If no connection is available, will list all possible set of parameters and    \n return an error code,                                                          \n\n You can use this command in a csm-orc template to make sure that API           \n connection is available.                                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --web-help      Open the web documentation                                   \u2502\n\u2502 --help          Show this message and exit.                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 postgres-send-runner-metadata  Send runner metadata to a PostgreSQL          \u2502\n\u2502                                database.                                     \u2502\n\u2502 rds-load-csv                   Load data from a runner's RDS database into a \u2502\n\u2502                                CSV file.                                     \u2502\n\u2502 rds-send-csv                   Send CSV files to a runner's RDS database.    \u2502\n\u2502 rds-send-store                 Send data from a store to a runner's RDS      \u2502\n\u2502                                database.                                     \u2502\n\u2502 run-load-data                  Download a runner data from the Cosmo Tech    \u2502\n\u2502                                API Requires a valid Azure connection either  \u2502\n\u2502                                with:                                         \u2502\n\u2502                                                                              \u2502\n\u2502                                 \u2022 The AZ cli command: az login               \u2502\n\u2502                                 \u2022 A triplet of env var AZURE_TENANT_ID,      \u2502\n\u2502                                   AZURE_CLIENT_ID, AZURE_CLIENT_SECRET       \u2502\n\u2502 runtemplate-load-handler       Uses environment variables to download cloud  \u2502\n\u2502                                based Template steps                          \u2502\n\u2502 tdl-load-files                 Query a twingraph and loads all the data from \u2502\n\u2502                                it                                            \u2502\n\u2502 tdl-send-files                 Reads a folder CSVs and send those to the     \u2502\n\u2502                                Cosmo Tech API as a Dataset                   \u2502\n\u2502 wsf-load-file                  Download files from a workspace.              \u2502\n\u2502 wsf-send-file                  Upload a file to a workspace.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/postgres-send-runner-metadata/","title":"postgres-send-runner-metadata","text":"<p>Help command</p> <pre><code>&gt; csm-data api postgres-send-runner-metadata --help\n\n Usage: csm-data api postgres-send-runner-metadata [OPTIONS]                    \n\n Send runner metadata to a PostgreSQL database.                                 \n Creates or updates a table in PostgreSQL with runner metadata including id,    \n name, last run id, and run template id. The table will be created if it        \n doesn't exist, and existing records will be updated based on the runner id.    \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id               o-XXXXXXXX  An organization id for the    \u2502\n\u2502                                                Cosmo Tech API                \u2502\n\u2502                                                ENV: CSM_ORGANIZATION_ID      \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --workspace-id                  w-XXXXXXXX  A workspace id for the Cosmo  \u2502\n\u2502                                                Tech API                      \u2502\n\u2502                                                ENV: CSM_WORKSPACE_ID         \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --runner-id                     r-XXXXXXXX  A runner id for the Cosmo     \u2502\n\u2502                                                Tech API                      \u2502\n\u2502                                                ENV: CSM_RUNNER_ID            \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --table-prefix                  PREFIX      Prefix to add to the table    \u2502\n\u2502                                                name                          \u2502\n\u2502 *  --postgres-host                 TEXT        PostgreSQL host URI           \u2502\n\u2502                                                ENV: POSTGRES_HOST_URI        \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --postgres-port                 INTEGER     PostgreSQL database port      \u2502\n\u2502                                                ENV: POSTGRES_HOST_PORT       \u2502\n\u2502 *  --postgres-db                   TEXT        PostgreSQL database name      \u2502\n\u2502                                                ENV: POSTGRES_DB_NAME         \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --postgres-schema               TEXT        PostgreSQL schema name        \u2502\n\u2502                                                ENV: POSTGRES_DB_SCHEMA       \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --postgres-user                 TEXT        PostgreSQL connection user    \u2502\n\u2502                                                name                          \u2502\n\u2502                                                ENV: POSTGRES_USER_NAME       \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --postgres-password             TEXT        PostgreSQL connection         \u2502\n\u2502                                                password                      \u2502\n\u2502                                                ENV: POSTGRES_USER_PASSWORD   \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --encode-password/--no-enco\u2026                csm_data.commands.store.post\u2026 \u2502\n\u2502                                                ENV:                          \u2502\n\u2502                                                CSM_PSQL_FORCE_PASSWORD_ENCO\u2026 \u2502\n\u2502                                                DEFAULT: encode-password      \u2502\n\u2502    --help                                      Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-load-csv/","title":"rds-load-csv","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-load-csv --help\n\n Usage: csm-data api rds-load-csv [OPTIONS]                                     \n\n Load data from a runner's RDS database into a CSV file.                        \n Executes a SQL query against the runner's RDS database and saves the results   \n to a CSV file. By default, it will list all tables in the public schema if no  \n specific query is provided.                                                    \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --target-folder      PATH        The folder where the csv will be written \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --file-name          NAME        A file name to write the query results   \u2502\n\u2502                                     DEFAULT: results                         \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --query              SQL_QUERY   SQL query to execute (defaults to        \u2502\n\u2502                                     listing all tables in public schema)     \u2502\n\u2502                                     DEFAULT: SELECT table_name FROM          \u2502\n\u2502                                     information_schema.tables WHERE          \u2502\n\u2502                                     table_schema='public'                    \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-send-csv/","title":"rds-send-csv","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-send-csv --help\n\n Usage: csm-data api rds-send-csv [OPTIONS]                                     \n\n Send CSV files to a runner's RDS database.                                     \n Takes all CSV files from a source folder and sends their content to the        \n runner's RDS database. Each CSV file will be sent to a table named after the   \n file (without the .csv extension). The table name will be prefixed with \"CD_\"  \n in the database.                                                               \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder      PATH        The folder containing csvs to send       \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-send-store/","title":"rds-send-store","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-send-store --help\n\n Usage: csm-data api rds-send-store [OPTIONS]                                   \n\n Send data from a store to a runner's RDS database.                             \n Takes all tables from a store and sends their content to the runner's RDS      \n database. Each table will be sent to a table with the same name, prefixed with \n \"CD_\" in the database. Null values in rows will be removed before sending.     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder       PATH        The folder containing the store files    \u2502\n\u2502                                     ENV: CSM_PARAMETERS_ABSOLUTE_PATH        \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/run-load-data/","title":"run-load-data","text":"<p>Help command</p> <pre><code>&gt; csm-data api run-load-data --help\n\n Usage: csm-data api run-load-data [OPTIONS]                                    \n\n Download a runner data from the Cosmo Tech API Requires a valid Azure          \n connection either with:                                                        \n\n  \u2022 The AZ cli command: az login                                                \n  \u2022 A triplet of env var AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET  \n Requires env var CSM_API_URL     The URL to a Cosmotech API                    \n Requires env var CSM_API_SCOPE   The identification scope of a Cosmotech API   \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id              o-##########  The id of an organization in \u2502\n\u2502                                                 the cosmotech api            \u2502\n\u2502                                                 ENV: CSM_ORGANIZATION_ID     \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --workspace-id                 w-##########  The id of a workspace in the \u2502\n\u2502                                                 cosmotech api                \u2502\n\u2502                                                 ENV: CSM_WORKSPACE_ID        \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --runner-id                    s-##########  The id of a runner in the    \u2502\n\u2502                                                 cosmotech api                \u2502\n\u2502                                                 ENV: CSM_RUNNER_ID           \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --dataset-absolute-path        PATH          A local folder to store the  \u2502\n\u2502                                                 main dataset content         \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 CSM_DATASET_ABSOLUTE_PATH    \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --parameters-absolute-path     PATH          A local folder to store the  \u2502\n\u2502                                                 parameters content           \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 CSM_PARAMETERS_ABSOLUTE_PATH \u2502\n\u2502                                                 [required]                   \u2502\n\u2502    --write-json/--no-write-js\u2026                  Whether to write the data in \u2502\n\u2502                                                 JSON format                  \u2502\n\u2502                                                 ENV: WRITE_JSON              \u2502\n\u2502                                                 DEFAULT: write-json          \u2502\n\u2502    --write-csv/--no-write-csv                   Whether to write the data in \u2502\n\u2502                                                 CSV format                   \u2502\n\u2502                                                 ENV: WRITE_CSV               \u2502\n\u2502                                                 DEFAULT: no-write-csv        \u2502\n\u2502    --fetch-dataset/--no-fetch\u2026                  Whether to fetch datasets    \u2502\n\u2502                                                 ENV: FETCH_DATASET           \u2502\n\u2502                                                 DEFAULT: fetch-dataset       \u2502\n\u2502    --parallel/--no-parallel                     Whether to fetch datasets in \u2502\n\u2502                                                 parallel                     \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 FETCH_DATASETS_IN_PARALLEL   \u2502\n\u2502                                                 DEFAULT: parallel            \u2502\n\u2502    --web-help                                   Open the web documentation   \u2502\n\u2502    --help                                       Show this message and exit.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/runtemplate-load-handler/","title":"runtemplate-load-handler","text":"<p>Help command</p> <pre><code>&gt; csm-data api runtemplate-load-handler --help\n\n Usage: csm-data api runtemplate-load-handler [OPTIONS]                         \n\n Uses environment variables to download cloud based Template steps              \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-##########         The id of an organization in    \u2502\n\u2502                                              the cosmotech api               \u2502\n\u2502                                              ENV: CSM_ORGANIZATION_ID        \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --workspace-id       w-##########         The id of a solution in the     \u2502\n\u2502                                              cosmotech api                   \u2502\n\u2502                                              ENV: CSM_WORKSPACE_ID           \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --run-template-id    NAME                 The name of the run template in \u2502\n\u2502                                              the cosmotech api               \u2502\n\u2502                                              ENV: CSM_RUN_TEMPLATE_ID        \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --handler-list       HANDLER,...,HANDLER  A list of handlers to download  \u2502\n\u2502                                              (comma separated)               \u2502\n\u2502                                              ENV: CSM_CONTAINER_MODE         \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --help                                    Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/tdl-load-files/","title":"tdl-load-files","text":"<p>Help command</p> <pre><code>&gt; csm-data api tdl-load-files --help\n\n Usage: csm-data api tdl-load-files [OPTIONS]                                   \n\n Query a twingraph and loads all the data from it                               \n Will create 1 csv file per node type / relationship type                       \n\n The twingraph must have been populated using the \"tdl-send-files\" command for  \n this to work correctly                                                         \n\n Requires a valid connection to the API to send the data                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --scenario-id        s-XXXXXXXX  A scenario id for the Cosmo Tech API     \u2502\n\u2502                                     ENV: CSM_SCENARIO_ID                     \u2502\n\u2502    --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502 *  --dir                PATH        Path to the directory to write the       \u2502\n\u2502                                     results to                               \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/tdl-send-files/","title":"tdl-send-files","text":"<p>Help command</p> <pre><code>&gt; csm-data api tdl-send-files --help\n\n Usage: csm-data api tdl-send-files [OPTIONS]                                   \n\n Reads a folder CSVs and send those to the Cosmo Tech API as a Dataset          \n CSVs must follow a given format:                                               \n\n  \u2022 Nodes files must have an id column                                          \n  \u2022 Relationship files must have id, src and dest columns                       \n\n Non-existing relationship (aka dest or src does not point to existing node)    \n won't trigger an error, the relationship will not be created instead.          \n\n Requires a valid connection to the API to send the data                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --api-url            URI         The URI to a Cosmo Tech API instance     \u2502\n\u2502                                     ENV: CSM_API_URL                         \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --dir                PATH        Path to the directory containing csvs to \u2502\n\u2502                                     send                                     \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --clear/--keep                   Flag to clear the target dataset first   \u2502\n\u2502                                     (if set to True will clear the dataset   \u2502\n\u2502                                     before sending anything, irreversibly)   \u2502\n\u2502                                     DEFAULT: clear                           \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/wsf-load-file/","title":"wsf-load-file","text":"<p>Help command</p> <pre><code>&gt; csm-data api wsf-load-file --help\n\n Usage: csm-data api wsf-load-file [OPTIONS]                                    \n\n Download files from a workspace.                                               \n Downloads files from a specified path in a workspace to a local target folder. \n If the workspace path ends with '/', it will be treated as a folder and all    \n files within will be downloaded.                                               \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --workspace-path     PATH        Path inside the workspace to load (end   \u2502\n\u2502                                     with '/' for a folder)                   \u2502\n\u2502 *  --target-folder      PATH        Folder in which to send the downloaded   \u2502\n\u2502                                     file                                     \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/wsf-send-file/","title":"wsf-send-file","text":"<p>Help command</p> <pre><code>&gt; csm-data api wsf-send-file --help\n\n Usage: csm-data api wsf-send-file [OPTIONS]                                    \n\n Upload a file to a workspace.                                                  \n Uploads a local file to a specified path in a workspace. If the workspace path \n ends with '/', the file will be uploaded to that folder with its original      \n name. Otherwise, the file will be uploaded with the name specified in the      \n workspace path.                                                                \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id     o-XXXXXXXX  An organization id for the Cosmo Tech   \u2502\n\u2502                                      API                                     \u2502\n\u2502                                      ENV: CSM_ORGANIZATION_ID                \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --workspace-id        w-XXXXXXXX  A workspace id for the Cosmo Tech API   \u2502\n\u2502                                      ENV: CSM_WORKSPACE_ID                   \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --file-path           PATH        Path to the file to send as a workspace \u2502\n\u2502                                      file                                    \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --workspace-path      PATH        Path inside the workspace to store the  \u2502\n\u2502                                      file (end with '/' for a folder)        \u2502\n\u2502                                      [required]                              \u2502\n\u2502    --overwrite/--keep                Flag to overwrite the target file if it \u2502\n\u2502                                      exists                                  \u2502\n\u2502                                      DEFAULT: overwrite                      \u2502\n\u2502    --web-help                        Open the web documentation              \u2502\n\u2502    --help                            Show this message and exit.             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/","title":"store","text":"<p>Help command</p> <pre><code>&gt; csm-data store --help\n\n Usage: csm-data store [OPTIONS] COMMAND [ARGS]...                              \n\n CoAL Data Store command group                                                  \n This group of commands will give you helper commands to interact with the      \n datastore                                                                      \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --web-help      Open the web documentation                                   \u2502\n\u2502 --help          Show this message and exit.                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 dump-to-azure          Dump a datastore to a Azure storage account.          \u2502\n\u2502 dump-to-postgresql     Running this command will dump your store to a given  \u2502\n\u2502                        postgresql database                                   \u2502\n\u2502 dump-to-s3             Dump a datastore to a S3                              \u2502\n\u2502 list-tables            Running this command will list the existing tables in \u2502\n\u2502                        your datastore                                        \u2502\n\u2502 load-csv-folder        Running this command will find all csvs in the given  \u2502\n\u2502                        folder and put them in the store                      \u2502\n\u2502 load-from-singlestore  Load data from SingleStore tables into the store.     \u2502\n\u2502                        Will download everything from a given SingleStore     \u2502\n\u2502                        database following some configuration into the store. \u2502\n\u2502 rds-send-store         Send data from a store to a runner's RDS database.    \u2502\n\u2502 reset                  Running this command will reset the state of your     \u2502\n\u2502                        store                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-azure/","title":"dump-to-azure","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-azure --help\n\n Usage: csm-data store dump-to-azure [OPTIONS]                                  \n\n Dump a datastore to a Azure storage account.                                   \n Will upload everything from a given data store to a Azure storage container.   \n\n 3 modes currently exists:                                                      \n\n  \u2022 sqlite: will dump the data store underlying database as is                  \n  \u2022 csv: will convert every table of the datastore to csv and send them as      \n    separate files                                                              \n  \u2022 parquet: will convert every table of the datastore to parquet and send them \n    as separate files                                                           \n\n Make use of the azure.storage.blob library to access the container             \n\n More information is available on this page:                                    \n [https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blob \n s-python?tabs=managed-identity%2Croles-azure-portal%2Csign-in-azure-cli&amp;pivots \n =blob-storage-quickstart-scratch]                                              \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder      PATH                  The folder containing the store \u2502\n\u2502                                              files                           \u2502\n\u2502                                              ENV:                            \u2502\n\u2502                                              CSM_PARAMETERS_ABSOLUTE_PATH    \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --output-type       [sqlite|csv|parquet]  Choose the type of file output  \u2502\n\u2502                                              to use (sqlite, csv, parquet)   \u2502\n\u2502 *  --account-name      TEXT                  The account name on Azure to    \u2502\n\u2502                                              upload to                       \u2502\n\u2502                                              ENV: AZURE_ACCOUNT_NAME         \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --container-name    TEXT                  The container name on Azure to  \u2502\n\u2502                                              upload to                       \u2502\n\u2502                                              ENV: AZURE_CONTAINER_NAME       \u2502\n\u2502    --prefix            PREFIX                A prefix by which all uploaded  \u2502\n\u2502                                              files should start with in the  \u2502\n\u2502                                              container                       \u2502\n\u2502                                              ENV: CSM_DATA_PREFIX            \u2502\n\u2502 *  --tenant-id         ID                    Tenant Identity used to connect \u2502\n\u2502                                              to Azure storage system         \u2502\n\u2502                                              ENV: AZURE_TENANT_ID            \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --client-id         ID                    Client Identity used to connect \u2502\n\u2502                                              to Azure storage system         \u2502\n\u2502                                              ENV: AZURE_CLIENT_ID            \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --client-secret     ID                    Client Secret tied to the ID    \u2502\n\u2502                                              used to connect to Azure        \u2502\n\u2502                                              storage system                  \u2502\n\u2502                                              ENV: AZURE_CLIENT_SECRET        \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --web-help                                Open the web documentation      \u2502\n\u2502    --help                                    Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-postgresql/","title":"dump-to-postgresql","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-postgresql --help\n\n Usage: csm-data store dump-to-postgresql [OPTIONS]                             \n\n Running this command will dump your store to a given postgresql database       \n Tables names from the store will be prepended with table-prefix in target      \n database                                                                       \n\n The postgresql user must have USAGE granted on the schema for this script to   \n work due to the use of the command COPY FROM STDIN                             \n\n\n\n You can simply give him that grant by running the command: GRANT USAGE ON      \n SCHEMA  TO                                                                     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder                    PATH     The folder containing the      \u2502\n\u2502                                               store files                    \u2502\n\u2502                                               ENV:                           \u2502\n\u2502                                               CSM_PARAMETERS_ABSOLUTE_PATH   \u2502\n\u2502                                               [required]                     \u2502\n\u2502    --table-prefix                    PREFIX   Prefix to add to the table     \u2502\n\u2502                                               name                           \u2502\n\u2502 *  --postgres-host                   TEXT     PostgreSQL host URI            \u2502\n\u2502                                               ENV: POSTGRES_HOST_URI         \u2502\n\u2502                                               [required]                     \u2502\n\u2502    --postgres-port                   INTEGER  PostgreSQL database port       \u2502\n\u2502                                               ENV: POSTGRES_HOST_PORT        \u2502\n\u2502 *  --postgres-db                     TEXT     PostgreSQL database name       \u2502\n\u2502                                               ENV: POSTGRES_DB_NAME          \u2502\n\u2502                                               [required]                     \u2502\n\u2502 *  --postgres-schema                 TEXT     PostgreSQL schema name         \u2502\n\u2502                                               ENV: POSTGRES_DB_SCHEMA        \u2502\n\u2502                                               [required]                     \u2502\n\u2502 *  --postgres-user                   TEXT     PostgreSQL connection user     \u2502\n\u2502                                               name                           \u2502\n\u2502                                               ENV: POSTGRES_USER_NAME        \u2502\n\u2502                                               [required]                     \u2502\n\u2502 *  --postgres-password               TEXT     PostgreSQL connection password \u2502\n\u2502                                               ENV: POSTGRES_USER_PASSWORD    \u2502\n\u2502                                               [required]                     \u2502\n\u2502    --replace/--append                         Append data on existing tables \u2502\n\u2502                                               DEFAULT: replace               \u2502\n\u2502    --encode-password/--no-encode\u2026             Force encoding of password to  \u2502\n\u2502                                               percent encoding               \u2502\n\u2502                                               ENV:                           \u2502\n\u2502                                               CSM_PSQL_FORCE_PASSWORD_ENCOD\u2026 \u2502\n\u2502                                               DEFAULT: encode-password       \u2502\n\u2502    --help                                     Show this message and exit.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-s3/","title":"dump-to-s3","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-s3 --help\n\n Usage: csm-data store dump-to-s3 [OPTIONS]                                     \n\n Dump a datastore to a S3                                                       \n Will upload everything from a given data store to a S3 bucket.                 \n\n 3 modes currently exists:                                                      \n\n  \u2022 sqlite: will dump the data store underlying database as is                  \n  \u2022 csv: will convert every table of the datastore to csv and send them as      \n    separate files                                                              \n  \u2022 parquet: will convert every table of the datastore to parquet and send them \n    as separate files                                                           \n\n Giving a prefix will add it to every upload (finishing the prefix with a \"/\"   \n will allow to upload in a folder inside the bucket)                            \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder        PATH                  The folder containing the     \u2502\n\u2502                                                store files                   \u2502\n\u2502                                                ENV:                          \u2502\n\u2502                                                CSM_PARAMETERS_ABSOLUTE_PATH  \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --output-type         [sqlite|csv|parquet]  Choose the type of file       \u2502\n\u2502                                                output to use (sqlite, csv,   \u2502\n\u2502                                                parquet)                      \u2502\n\u2502 *  --bucket-name         BUCKET                The bucket on S3 to upload to \u2502\n\u2502                                                ENV: CSM_DATA_BUCKET_NAME     \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --prefix              PREFIX                A prefix by which all         \u2502\n\u2502                                                uploaded files should start   \u2502\n\u2502                                                with in the bucket            \u2502\n\u2502                                                ENV: CSM_DATA_BUCKET_PREFIX   \u2502\n\u2502    --use-ssl/--no-ssl                          Use SSL to secure connection  \u2502\n\u2502                                                to S3                         \u2502\n\u2502 *  --s3-url              URL                   URL to connect to the S3      \u2502\n\u2502                                                system                        \u2502\n\u2502                                                ENV: AWS_ENDPOINT_URL         \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --access-id           ID                    Identity used to connect to   \u2502\n\u2502                                                the S3 system                 \u2502\n\u2502                                                ENV: AWS_ACCESS_KEY_ID        \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --secret-key          ID                    Secret tied to the ID used to \u2502\n\u2502                                                connect to the S3 system      \u2502\n\u2502                                                ENV: AWS_SECRET_ACCESS_KEY    \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --ssl-cert-bundle     PATH                  Path to an alternate CA       \u2502\n\u2502                                                Bundle to validate SSL        \u2502\n\u2502                                                connections                   \u2502\n\u2502                                                ENV: CSM_S3_CA_BUNDLE         \u2502\n\u2502    --web-help                                  Open the web documentation    \u2502\n\u2502    --help                                      Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/list-tables/","title":"list-tables","text":"<p>Help command</p> <pre><code>&gt; csm-data store list-tables --help\n\n Usage: csm-data store list-tables [OPTIONS]                                    \n\n Running this command will list the existing tables in your datastore           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder          PATH  The folder containing the store files       \u2502\n\u2502                                  ENV: CSM_PARAMETERS_ABSOLUTE_PATH           \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --schema/--no-schema          Display the schema of the tables            \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/load-csv-folder/","title":"load-csv-folder","text":"<p>Help command</p> <pre><code>&gt; csm-data store load-csv-folder --help\n\n Usage: csm-data store load-csv-folder [OPTIONS]                                \n\n Running this command will find all csvs in the given folder and put them in    \n the store                                                                      \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder    PATH  The folder containing the store files             \u2502\n\u2502                            ENV: CSM_PARAMETERS_ABSOLUTE_PATH                 \u2502\n\u2502                            [required]                                        \u2502\n\u2502 *  --csv-folder      PATH  The folder containing the csv files to store      \u2502\n\u2502                            ENV: CSM_DATASET_ABSOLUTE_PATH                    \u2502\n\u2502                            [required]                                        \u2502\n\u2502    --help                  Show this message and exit.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/load-from-singlestore/","title":"load-from-singlestore","text":"<p>Help command</p> <pre><code>&gt; csm-data store load-from-singlestore --help\n\n Usage: csm-data store load-from-singlestore [OPTIONS]                          \n\n Load data from SingleStore tables into the store. Will download everything     \n from a given SingleStore database following some configuration into the store. \n Make use of the singlestoredb to access to SingleStore                         \n\n More information is available on this page:                                    \n [https://docs.singlestore.com/cloud/developer-resources/connect-with-applicati \n on-development-tools/connect-with-python/connect-using-the-singlestore-python- \n client/]                                                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --singlestore-host        TEXT     SingleStore instance URI               \u2502\n\u2502                                       ENV: SINGLE_STORE_HOST                 \u2502\n\u2502                                       [required]                             \u2502\n\u2502    --singlestore-port        INTEGER  SingleStore port                       \u2502\n\u2502                                       ENV: SINGLE_STORE_PORT                 \u2502\n\u2502 *  --singlestore-db          TEXT     SingleStore database name              \u2502\n\u2502                                       ENV: SINGLE_STORE_DB                   \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-user        TEXT     SingleStore connection user name       \u2502\n\u2502                                       ENV: SINGLE_STORE_USERNAME             \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-password    TEXT     SingleStore connection password        \u2502\n\u2502                                       ENV: SINGLE_STORE_PASSWORD             \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-tables      TEXT     SingleStore table names to fetched     \u2502\n\u2502                                       (separated by comma)                   \u2502\n\u2502                                       ENV: SINGLE_STORE_TABLES               \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --store-folder            PATH     The folder containing the store files  \u2502\n\u2502                                       ENV: CSM_PARAMETERS_ABSOLUTE_PATH      \u2502\n\u2502                                       [required]                             \u2502\n\u2502    --help                             Show this message and exit.            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/rds-send-store/","title":"rds-send-store","text":"<p>Help command</p> <pre><code>&gt; csm-data store rds-send-store --help\n\n Usage: csm-data store rds-send-store [OPTIONS]                                 \n\n Send data from a store to a runner's RDS database.                             \n Takes all tables from a store and sends their content to the runner's RDS      \n database. Each table will be sent to a table with the same name, prefixed with \n \"CD_\" in the database. Null values in rows will be removed before sending.     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder       PATH        The folder containing the store files    \u2502\n\u2502                                     ENV: CSM_PARAMETERS_ABSOLUTE_PATH        \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/reset/","title":"reset","text":"<p>Help command</p> <pre><code>&gt; csm-data store reset --help\n\n Usage: csm-data store reset [OPTIONS]                                          \n\n Running this command will reset the state of your store                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder    PATH  The folder containing the store files             \u2502\n\u2502                            ENV: CSM_PARAMETERS_ABSOLUTE_PATH                 \u2502\n\u2502                            [required]                                        \u2502\n\u2502    --help                  Show this message and exit.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>A list of tutorials for concepts added to CoAL</p> <p> csm-data</p> <p>Make full use of <code>csm-data</code> commands to connect to services during your orchestration runs</p> <p> csm-data</p> <p> Data store</p> <p>The datastore is your friend to keep data between orchestration steps. It comes with multiple ways to interact with it.</p> <p> Datastore</p> <p> CosmoTech API</p> <p>Learn how to interact with the CosmoTech API directly: authentication, workspaces, Twin Data Layer, and more.</p> <p> CosmoTech API</p> <p> Contributing to CoAL</p> <p>Learn how to contribute to CoAL: from setting up your development environment to implementing new features and submitting pull requests.</p> <p> Contributing</p>"},{"location":"tutorials/contributing/","title":"Contributing to CoAL","text":"<p>Objective</p> <ul> <li>Set up your development environment with Black and pre-commit hooks</li> <li>Understand the CoAL architecture and contribution workflow</li> <li>Learn how to implement a new feature with a practical example</li> <li>Master the process of writing unit tests and documentation</li> <li>Successfully submit a pull request</li> </ul>"},{"location":"tutorials/contributing/#introduction","title":"Introduction","text":"<p>Contributing to the CosmoTech Acceleration Library (CoAL) is a great way to enhance the platform's capabilities and share your expertise with the community. This tutorial will guide you through the entire process of contributing a new feature to CoAL, from setting up your development environment to submitting a pull request.</p> <p>We'll use a practical example throughout this tutorial: implementing a new store write functionality for MongoDB and creating a corresponding csm-data command. This example will demonstrate all the key aspects of the contribution process, including:</p> <ul> <li>Setting up your development environment</li> <li>Understanding the CoAL architecture</li> <li>Implementing new functionality</li> <li>Creating CLI commands</li> <li>Writing unit tests</li> <li>Documenting your work</li> <li>Submitting a pull request</li> </ul> <p>By the end of this tutorial, you'll have a solid understanding of how to contribute to CoAL and be ready to implement your own features.</p>"},{"location":"tutorials/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Before you start contributing, you need to set up your development environment. This includes forking and cloning the repository, installing dependencies, and configuring code formatting tools.</p>"},{"location":"tutorials/contributing/#forking-and-cloning-the-repository","title":"Forking and Cloning the Repository","text":"<ol> <li>Fork the CosmoTech-Acceleration-Library repository on GitHub</li> <li> <p>Clone your fork locally:</p> <pre><code>git clone https://github.com/your-username/CosmoTech-Acceleration-Library.git\ncd CosmoTech-Acceleration-Library\n</code></pre> </li> <li> <p>Add the upstream repository as a remote:</p> <pre><code>git remote add upstream https://github.com/Cosmo-Tech/CosmoTech-Acceleration-Library.git\n</code></pre> </li> </ol>"},{"location":"tutorials/contributing/#installing-dependencies","title":"Installing Dependencies","text":"<p>Install the package in development mode along with all development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This will install the package in editable mode, allowing you to make changes to the code without reinstalling it. It will also install all the development dependencies specified in the <code>pyproject.toml</code> file.</p>"},{"location":"tutorials/contributing/#setting-up-black-for-code-formatting","title":"Setting Up Black for Code Formatting","text":"<p>CoAL uses Black for code formatting to ensure consistent code style across the codebase. Black is configured in the <code>pyproject.toml</code> file with specific settings for line length, target Python version, and file exclusions.</p> <p>To manually run Black on your codebase:</p> <pre><code># Format all Python files in the project\npython -m black .\n\n# Format a specific directory\npython -m black cosmotech/coal/\n\n# Check if files would be reformatted without actually changing them\npython -m black --check .\n\n# Show diff of changes without writing files\npython -m black --diff .\n</code></pre>"},{"location":"tutorials/contributing/#configuring-pre-commit-hooks","title":"Configuring Pre-commit Hooks","text":"<p>CoAL uses pre-commit hooks to automatically run checks before each commit, including Black formatting, trailing whitespace removal, and test coverage verification.</p> <p>To install pre-commit:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now, when you commit changes, the pre-commit hooks will automatically run and check your code. If any issues are found, the commit will be aborted, and you'll need to fix the issues before committing again.</p> <p>The pre-commit configuration includes:</p> <ul> <li>Trailing whitespace removal</li> <li>End-of-file fixer</li> <li>YAML syntax checking</li> <li>Black code formatting</li> <li>Pytest checks with coverage requirements</li> <li>Verification that all functions have tests</li> </ul>"},{"location":"tutorials/contributing/#understanding-the-coal-architecture","title":"Understanding the CoAL Architecture","text":"<p>Before implementing a new feature, it's important to understand the architecture of CoAL and how its components interact.</p>"},{"location":"tutorials/contributing/#core-modules","title":"Core Modules","text":"<p>CoAL is organized into several key modules:</p> <ul> <li>coal: The core library functionality<ul> <li>store: Data storage and retrieval</li> <li>cosmotech_api: Interaction with the CosmoTech API</li> <li>aws: AWS integration</li> <li>azure: Azure integration</li> <li>postgresql: PostgreSQL integration</li> <li>utils: Utility functions</li> </ul> </li> <li>csm_data: CLI commands for data operations</li> <li>orchestrator_plugins: Plugins for csm-orc</li> <li>translation: Translation resources</li> </ul>"},{"location":"tutorials/contributing/#store-module-architecture","title":"Store Module Architecture","text":"<p>The store module provides a unified interface for data storage and retrieval. It's built around the <code>Store</code> class, which provides methods for:</p> <ul> <li>Adding and retrieving tables</li> <li>Executing SQL queries</li> <li>Listing available tables</li> <li>Resetting the store</li> </ul> <p>The store module also includes adapters for different data formats:</p> <ul> <li>native_python: Python dictionaries and lists</li> <li>csv: CSV files</li> <li>pandas: Pandas DataFrames</li> <li>pyarrow: PyArrow Tables</li> </ul> <p>External storage systems are implemented as separate modules that interact with the core <code>Store</code> class:</p> <ul> <li>postgresql: PostgreSQL integration</li> <li>singlestore: SingleStore integration</li> </ul>"},{"location":"tutorials/contributing/#cli-command-structure","title":"CLI Command Structure","text":"<p>The <code>csm_data</code> CLI is organized into command groups, each focused on specific types of operations:</p> <ul> <li>api: Commands for interacting with the CosmoTech API</li> <li>store: Commands for working with the CoAL datastore</li> <li>s3-bucket-*: Commands for S3 bucket operations</li> <li>adx-send-scenariodata: Command for sending scenario data to Azure Data Explorer</li> <li>az-storage-upload: Command for uploading to Azure Storage</li> </ul> <p>Each command is implemented as a separate Python file in the appropriate directory, using the Click library for command-line interface creation.</p>"},{"location":"tutorials/contributing/#implementing-a-new-store-feature","title":"Implementing a New Store Feature","text":"<p>Now that we understand the architecture, let's implement a new store feature: MongoDB integration. This will allow users to write data from the CoAL datastore to MongoDB.</p>"},{"location":"tutorials/contributing/#creating-the-module-structure","title":"Creating the Module Structure","text":"<p>First, we'll create a new module for MongoDB integration:</p> <pre><code>mkdir -p cosmotech/coal/mongodb\ntouch cosmotech/coal/mongodb/__init__.py\ntouch cosmotech/coal/mongodb/store.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-the-core-functionality","title":"Implementing the Core Functionality","text":"<p>Now, let's implement the core functionality in <code>cosmotech/coal/mongodb/store.py</code>:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\n\"\"\"\nMongoDB store operations module.\n\nThis module provides functions for interacting with MongoDB databases\nfor store operations.\n\"\"\"\n\nfrom time import perf_counter\nimport pyarrow\nimport pymongo\n\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.utils.logger import LOGGER\nfrom cosmotech.orchestrator.utils.translate import T\n\n\ndef send_pyarrow_table_to_mongodb(\n    data: pyarrow.Table,\n    collection_name: str,\n    mongodb_uri: str,\n    mongodb_db: str,\n    replace: bool = True,\n) -&gt; int:\n    \"\"\"\n    Send a PyArrow table to MongoDB.\n\n    Args:\n        data: PyArrow table to send\n        collection_name: MongoDB collection name\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        replace: Whether to replace existing collection\n\n    Returns:\n        Number of documents inserted\n    \"\"\"\n    # Convert PyArrow table to list of dictionaries\n    records = data.to_pylist()\n\n    # Connect to MongoDB\n    client = pymongo.MongoClient(mongodb_uri)\n    db = client[mongodb_db]\n\n    # Drop collection if replace is True and collection exists\n    if replace and collection_name in db.list_collection_names():\n        db[collection_name].drop()\n\n    # Insert records\n    if records:\n        result = db[collection_name].insert_many(records)\n        return len(result.inserted_ids)\n\n    return 0\n\n\ndef dump_store_to_mongodb(\n    store_folder: str,\n    mongodb_uri: str,\n    mongodb_db: str,\n    collection_prefix: str = \"Cosmotech_\",\n    replace: bool = True,\n) -&gt; None:\n    \"\"\"\n    Dump Store data to a MongoDB database.\n\n    Args:\n        store_folder: Folder containing the Store\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        collection_prefix: Collection prefix\n        replace: Whether to replace existing collections\n    \"\"\"\n    _s = Store(store_location=store_folder)\n\n    tables = list(_s.list_tables())\n    if len(tables):\n        LOGGER.info(T(\"coal.logs.database.sending_data\").format(table=mongodb_db))\n        total_rows = 0\n        _process_start = perf_counter()\n        for table_name in tables:\n            _s_time = perf_counter()\n            target_collection_name = f\"{collection_prefix}{table_name}\"\n            LOGGER.info(T(\"coal.logs.database.table_entry\").format(table=target_collection_name))\n            data = _s.get_table(table_name)\n            if not len(data):\n                LOGGER.info(T(\"coal.logs.database.no_rows\"))\n                continue\n            _dl_time = perf_counter()\n            rows = send_pyarrow_table_to_mongodb(\n                data,\n                target_collection_name,\n                mongodb_uri,\n                mongodb_db,\n                replace,\n            )\n            total_rows += rows\n            _up_time = perf_counter()\n            LOGGER.info(T(\"coal.logs.database.row_count\").format(count=rows))\n            LOGGER.debug(\n                T(\"coal.logs.progress.operation_timing\").format(\n                    operation=\"Load from datastore\", time=f\"{_dl_time - _s_time:0.3}\"\n                )\n            )\n            LOGGER.debug(\n                T(\"coal.logs.progress.operation_timing\").format(\n                    operation=\"Send to MongoDB\", time=f\"{_up_time - _dl_time:0.3}\"\n                )\n            )\n        _process_end = perf_counter()\n        LOGGER.info(\n            T(\"coal.logs.database.rows_fetched\").format(\n                table=\"all tables\",\n                count=total_rows,\n                time=f\"{_process_end - _process_start:0.3}\",\n            )\n        )\n    else:\n        LOGGER.info(T(\"coal.logs.database.store_empty\"))\n</code></pre>"},{"location":"tutorials/contributing/#updating-the-package-initialization","title":"Updating the Package Initialization","text":"<p>Next, we need to update the <code>__init__.py</code> file to expose our new function:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.coal.mongodb.store import dump_store_to_mongodb\n\n__all__ = [\"dump_store_to_mongodb\"]\n</code></pre>"},{"location":"tutorials/contributing/#adding-dependencies","title":"Adding Dependencies","text":"<p>We need to add pymongo as a dependency. Update the <code>pyproject.toml</code> file to include pymongo in the optional dependencies:</p> <pre><code>[project.optional-dependencies]\nmongodb = [\"pymongo&gt;=4.3.3\"]\n</code></pre>"},{"location":"tutorials/contributing/#creating-a-new-csm-data-command","title":"Creating a new CSM-DATA Command","text":"<p>Now that we have implemented the core functionality, let's create a new csm-data command to expose this functionality to users.</p>"},{"location":"tutorials/contributing/#creating-the-command-file","title":"Creating the Command File","text":"<p>Create a new file for the command:</p> <pre><code>touch cosmotech/csm_data/commands/store/dump_to_mongodb.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-the-command","title":"Implementing the Command","text":"<p>Now, let's implement the command:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.csm_data.utils.click import click\nfrom cosmotech.csm_data.utils.decorators import web_help, translate_help\nfrom cosmotech.orchestrator.utils.translate import T\n\n\n@click.command()\n@web_help(\"csm-data/store/dump-to-mongodb\")\n@translate_help(\"csm_data.commands.store.dump_to_mongodb.description\")\n@click.option(\n    \"--store-folder\",\n    envvar=\"CSM_PARAMETERS_ABSOLUTE_PATH\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.store_folder\"),\n    metavar=\"PATH\",\n    type=str,\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--collection-prefix\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.collection_prefix\"),\n    metavar=\"PREFIX\",\n    type=str,\n    default=\"Cosmotech_\",\n)\n@click.option(\n    \"--mongodb-uri\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.mongodb_uri\"),\n    envvar=\"MONGODB_URI\",\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--mongodb-db\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.mongodb_db\"),\n    envvar=\"MONGODB_DB_NAME\",\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--replace/--append\",\n    \"replace\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.replace\"),\n    default=True,\n    is_flag=True,\n    show_default=True,\n)\ndef dump_to_mongodb(\n    store_folder,\n    collection_prefix: str,\n    mongodb_uri,\n    mongodb_db,\n    replace: bool,\n):\n    # Import the function at the start of the command\n    from cosmotech.coal.mongodb import dump_store_to_mongodb\n\n    dump_store_to_mongodb(\n        store_folder=store_folder,\n        collection_prefix=collection_prefix,\n        mongodb_uri=mongodb_uri,\n        mongodb_db=mongodb_db,\n        replace=replace,\n    )\n</code></pre>"},{"location":"tutorials/contributing/#registering-the-command","title":"Registering the Command","text":"<p>Update the <code>cosmotech/csm_data/commands/store/__init__.py</code> file to register the new command:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.csm_data.commands.store.dump_to_azure import dump_to_azure\nfrom cosmotech.csm_data.commands.store.dump_to_postgresql import dump_to_postgresql\nfrom cosmotech.csm_data.commands.store.dump_to_s3 import dump_to_s3\nfrom cosmotech.csm_data.commands.store.dump_to_mongodb import dump_to_mongodb  # Add this line\nfrom cosmotech.csm_data.commands.store.list_tables import list_tables\nfrom cosmotech.csm_data.commands.store.load_csv_folder import load_csv_folder\nfrom cosmotech.csm_data.commands.store.load_from_singlestore import load_from_singlestore\nfrom cosmotech.csm_data.commands.store.reset import reset\nfrom cosmotech.csm_data.commands.store.store import store\n\n__all__ = [\n    \"dump_to_azure\",\n    \"dump_to_postgresql\",\n    \"dump_to_s3\",\n    \"dump_to_mongodb\",  # Add this line\n    \"list_tables\",\n    \"load_csv_folder\",\n    \"load_from_singlestore\",\n    \"reset\",\n    \"store\",\n]\n</code></pre>"},{"location":"tutorials/contributing/#adding-translation-strings","title":"Adding Translation Strings","text":"<p>Create translation files for the new command:</p> <ol> <li>For English (en-US):</li> </ol> <pre><code>touch cosmotech/translation/csm_data/en-US/commands/store/dump_to_mongodb.yml\n</code></pre> <pre><code>commands:\n  store:\n    dump_to_mongodb:\n      description: |\n        Dump store data to MongoDB.\n      parameters:\n        store_folder: Folder containing the store\n        collection_prefix: Prefix for MongoDB collections\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        replace: Replace existing collections\n</code></pre> <ol> <li>For French (fr-FR):</li> </ol> <pre><code>touch cosmotech/translation/csm_data/fr-FR/commands/store/dump_to_mongodb.yml\n</code></pre> <pre><code>commands:\n  store:\n    dump_to_mongodb:\n      description: |\n        Exporter les donn\u00e9es du store vers MongoDB.\n      parameters:\n        store_folder: Dossier contenant le store\n        collection_prefix: Pr\u00e9fixe pour les collections MongoDB\n        mongodb_uri: URI de connexion MongoDB\n        mongodb_db: Nom de la base de donn\u00e9es MongoDB\n        replace: Remplacer les collections existantes\n</code></pre>"},{"location":"tutorials/contributing/#writing-unit-tests","title":"Writing Unit Tests","text":"<p>Testing is a critical part of the contribution process. All new functionality must be thoroughly tested to ensure it works as expected and to prevent regressions.</p>"},{"location":"tutorials/contributing/#creating-test-files","title":"Creating Test Files","text":"<p>Create test files for the new functionality:</p> <pre><code>mkdir -p tests/unit/coal/mongodb\ntouch tests/unit/coal/mongodb/__init__.py\ntouch tests/unit/coal/mongodb/test_store.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-unit-tests","title":"Implementing Unit Tests","text":"<p>Now, let's implement the unit tests for the MongoDB store functionality:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nimport os\nimport tempfile\nfrom unittest.mock import patch, MagicMock\n\nimport pyarrow\nimport pytest\n\nfrom cosmotech.coal.mongodb.store import send_pyarrow_table_to_mongodb, dump_store_to_mongodb\nfrom cosmotech.coal.store.store import Store\n\n\n@pytest.fixture\ndef sample_table():\n    \"\"\"Create a sample PyArrow table for testing.\"\"\"\n    data = {\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [30, 25, 35],\n    }\n    return pyarrow.Table.from_pydict(data)\n\n\n@pytest.fixture\ndef temp_store():\n    \"\"\"Create a temporary store for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        store = Store(store_location=temp_dir)\n        yield store, temp_dir\n\n\nclass TestSendPyarrowTableToMongoDB:\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = []\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_replace(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = [\"test_collection\"]\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.drop.assert_called_once()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_append(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = [\"test_collection\"]\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            False,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.drop.assert_not_called()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_empty(self, mock_client):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = []\n\n        # Create an empty table\n        empty_table = pyarrow.Table.from_pydict({})\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            empty_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 0\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.insert_many.assert_not_called()\n\n\nclass TestDumpStoreToMongoDB:\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb(self, mock_send, temp_store):\n        store, temp_dir = temp_store\n\n        # Add a table to the store\n        sample_data = pyarrow.Table.from_pydict(\n            {\n                \"id\": [1, 2, 3],\n                \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n                \"age\": [30, 25, 35],\n            }\n        )\n        store.add_table(\"test_table\", sample_data)\n\n        # Set up mock\n        mock_send.return_value = 3\n\n        # Call the function\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was called correctly\n        mock_send.assert_called_once()\n        args, kwargs = mock_send.call_args\n        assert kwargs[\"collection_name\"] == \"Cosmotech_test_table\"\n        assert kwargs[\"mongodb_uri\"] == \"mongodb://localhost:27017\"\n        assert kwargs[\"mongodb_db\"] == \"test_db\"\n        assert kwargs[\"replace\"] is True\n\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb_empty(self, mock_send, temp_store):\n        _, temp_dir = temp_store\n\n        # Call the function with an empty store\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was not called\n        mock_send.assert_not_called()\n\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb_multiple_tables(self, mock_send, temp_store):\n        store, temp_dir = temp_store\n\n        # Add multiple tables to the store\n        table1 = pyarrow.Table.from_pydict(\n            {\n                \"id\": [1, 2, 3],\n                \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            }\n        )\n        table2 = pyarrow.Table.from_pydict(\n            {\n                \"id\": [4, 5],\n                \"name\": [\"Dave\", \"Eve\"],\n            }\n        )\n        store.add_table(\"table1\", table1)\n        store.add_table(\"table2\", table2)\n\n        # Set up mock\n        mock_send.side_effect = [3, 2]\n\n        # Call the function\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was called correctly for each table\n        assert mock_send.call_count == 2\n        call_args_list = mock_send.call_args_list\n\n        # Check first call\n        args, kwargs = call_args_list[0]\n        assert kwargs[\"collection_name\"] in [\"Cosmotech_table1\", \"Cosmotech_table2\"]\n\n        # Check second call\n        args, kwargs = call_args_list[1]\n        assert kwargs[\"collection_name\"] in [\"Cosmotech_table1\", \"Cosmotech_table2\"]\n\n        # Ensure both tables were processed\n        collection_names = [\n            call_args_list[0][1][\"collection_name\"],\n            call_args_list[1][1][\"collection_name\"],\n        ]\n        assert \"Cosmotech_table1\" in collection_names\n        assert \"Cosmotech_table2\" in collection_names\n</code></pre>"},{"location":"tutorials/contributing/#running-the-tests","title":"Running the Tests","text":"<p>To run the tests, use pytest:</p> <pre><code># Run all tests\npytest tests/unit/coal/mongodb/\n\n# Run with coverage\npytest tests/unit/coal/mongodb/ --cov=cosmotech.coal.mongodb --cov-report=term-missing\n</code></pre> <p>Make sure all tests pass and that you have adequate code coverage (at least 80%).</p>"},{"location":"tutorials/contributing/#documentation","title":"Documentation","text":"<p>Documentation is a critical part of the contribution process. All new features must be documented to ensure users can understand and use them effectively.</p>"},{"location":"tutorials/contributing/#updating-cli-documentation","title":"Updating CLI Documentation","text":"<p>Let's add csm-data documentation for our new functionality. Create a new file:</p> <pre><code>touch docs/csm-data/store/dump-to-mongodb.md\n</code></pre> <p>Add the following content:</p> <pre><code>---\nhide:\n  - toc\ndescription: \"Command help: `csm-data store dump-to-mongodb`\"\n---\n# dump-to-mongodb\n\n!!! info \"Help command\"\n    ```text\n    ```\n</code></pre> <p>The documentation build system will generate the content that will be inserted in the file to add a minimal documentation. You can then add more elements as necessary.</p>"},{"location":"tutorials/contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, make sure you've completed all the necessary steps:</p> <ol> <li>Code Quality<ul> <li> Code follows the project's style guidelines (Black formatting)</li> <li> All linting checks pass</li> <li> Code is well-documented with docstrings</li> <li> Code is efficient and follows best practices</li> <li> No unnecessary dependencies are added</li> </ul> </li> <li>Testing<ul> <li> All unit tests pass</li> <li> Test coverage meets or exceeds 80%</li> <li> All functions have at least one test</li> <li> Edge cases and error conditions are tested</li> <li> Mocks are used for external services</li> </ul> </li> <li>Documentation<ul> <li> API documentation is updated</li> <li> Command help text is clear and comprehensive</li> <li> Translation strings are added for all user-facing text</li> <li> Usage examples are provided</li> <li> Any necessary tutorials are created or updated</li> </ul> </li> <li>Integration<ul> <li> New functionality integrates well with existing code</li> <li> No breaking changes to existing APIs</li> <li> Dependencies are properly specified in pyproject.toml</li> <li> Command is registered in the appropriate init.py file</li> </ul> </li> <li>Pull Request Description<ul> <li> Clear description of the changes</li> <li> Explanation of why the changes are needed</li> <li> Any potential issues or limitations</li> <li> References to related issues or discussions</li> </ul> </li> </ol>"},{"location":"tutorials/contributing/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now learned how to contribute to CoAL by implementing a new feature, creating a new csm-data command, writing unit tests, and documenting your work.</p> <p>By following this tutorial, you've gained practical experience with:</p> <ul> <li>Setting up your development environment with Black and pre-commit hooks</li> <li>Understanding the CoAL architecture</li> <li>Implementing new functionality</li> <li>Creating CLI commands</li> <li>Writing unit tests</li> <li>Documenting your work</li> <li>Preparing for a pull request</li> </ul> <p>You're now ready to contribute your own features to CoAL and help improve the platform for everyone.</p> <p>Remember that the CoAL community is here to help. If you have any questions or need assistance, don't hesitate to reach out through GitHub issues or discussions.</p> <p>Happy contributing!</p>"},{"location":"tutorials/cosmotech-api/","title":"Working with the CosmoTech API","text":"<p>Objective</p> <ul> <li>Understand how to authenticate and connect to the CosmoTech API</li> <li>Learn to work with workspaces for file management</li> <li>Master the Twin Data Layer for graph data operations</li> <li>Implement runner and run data management</li> <li>Build complete workflows integrating multiple API features</li> </ul>"},{"location":"tutorials/cosmotech-api/#introduction-to-the-cosmotech-api-integration","title":"Introduction to the CosmoTech API Integration","text":"<p>The CosmoTech Acceleration Library (CoAL) provides a comprehensive set of tools for interacting with the CosmoTech API. This integration allows you to:</p> <ul> <li>Authenticate with different identity providers</li> <li>Manage workspaces and files</li> <li>Work with the Twin Data Layer for graph data</li> <li>Handle runners and runs</li> <li>Process and transform data</li> <li>Build end-to-end workflows</li> </ul> <p>The API integration is organized into several modules, each focused on specific functionality:</p> <ul> <li>connection: Authentication and API client management</li> <li>workspace: Workspace file operations</li> <li>twin_data_layer: Graph data management</li> <li>runner: Runner and run data operations</li> </ul> <p>API vs CLI</p> <p>While the <code>csm-data</code> CLI provides command-line tools for many common operations, the direct API integration offers more flexibility and programmatic control. Use the API integration when you need to:</p> <ul> <li>Build custom workflows</li> <li>Integrate with other Python code</li> <li>Perform complex operations not covered by the CLI</li> <li>Implement real-time interactions with the platform</li> </ul>"},{"location":"tutorials/cosmotech-api/#authentication-and-connection","title":"Authentication and Connection","text":"<p>The first step in working with the CosmoTech API is establishing a connection. CoAL supports multiple authentication methods:</p> <ul> <li>API Key authentication</li> <li>Azure Entra (formerly Azure AD) authentication</li> <li>Keycloak authentication</li> </ul> <p>The <code>get_api_client()</code> function automatically detects which authentication method to use based on the environment variables you've set.</p> Basic connection setup<pre><code># Example: Setting up connections to the CosmoTech API\nimport os\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Method 1: Using API Key (set these environment variables before running)\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\nfrom cosmotech_api.api.organization_api import OrganizationApi\n\norg_api = OrganizationApi(api_client)\n\n# List organizations\norganizations = org_api.find_all_organizations()\nfor org in organizations:\n    print(f\"Organization: {org.name} (ID: {org.id})\")\n\n# Don't forget to close the client when done\napi_client.close()\n\n# Method 2: Using Azure Entra (set these environment variables before running)\n\"\"\"\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_SCOPE\"] = \"api://your-app-id/.default\"  # Replace with your API scope\nos.environ[\"AZURE_CLIENT_ID\"] = \"your-client-id\"  # Replace with your client ID\nos.environ[\"AZURE_CLIENT_SECRET\"] = \"your-client-secret\"  # Replace with your client secret\nos.environ[\"AZURE_TENANT_ID\"] = \"your-tenant-id\"  # Replace with your tenant ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\n# ...\n\n# Don't forget to close the client when done\napi_client.close()\n\"\"\"\n\n# Method 3: Using Keycloak (set these environment variables before running)\n\"\"\"\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"IDP_BASE_URL\"] = \"https://keycloak.example.com/auth/\"  # Replace with your Keycloak URL\nos.environ[\"IDP_TENANT_ID\"] = \"your-realm\"  # Replace with your realm\nos.environ[\"IDP_CLIENT_ID\"] = \"your-client-id\"  # Replace with your client ID\nos.environ[\"IDP_CLIENT_SECRET\"] = \"your-client-secret\"  # Replace with your client secret\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\n# ...\n\n# Don't forget to close the client when done\napi_client.close()\n\"\"\"\n</code></pre> <p>Environment Variables</p> <p>You can set environment variables in your code for testing, but in production environments, it's better to set them at the system or container level for security.</p>"},{"location":"tutorials/cosmotech-api/#api-key-authentication","title":"API Key Authentication","text":"<p>API Key authentication is the simplest method and requires two environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>CSM_API_KEY</code>: Your API key</li> </ul>"},{"location":"tutorials/cosmotech-api/#azure-entra-authentication","title":"Azure Entra Authentication","text":"<p>Azure Entra authentication uses service principal credentials and requires these environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>CSM_API_SCOPE</code>: The API scope (usually in the format <code>api://app-id/.default</code>)</li> <li><code>AZURE_CLIENT_ID</code>: Your client ID</li> <li><code>AZURE_CLIENT_SECRET</code>: Your client secret</li> <li><code>AZURE_TENANT_ID</code>: Your tenant ID</li> </ul>"},{"location":"tutorials/cosmotech-api/#keycloak-authentication","title":"Keycloak Authentication","text":"<p>Keycloak authentication requires these environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>IDP_BASE_URL</code>: The base URL of your Keycloak server</li> <li><code>IDP_TENANT_ID</code>: Your realm name</li> <li><code>IDP_CLIENT_ID</code>: Your client ID</li> <li><code>IDP_CLIENT_SECRET</code>: Your client secret</li> </ul> <p>API Client Lifecycle</p> <p>Always close the API client when you're done using it to release resources. The best practice is to use a <code>try</code>/<code>finally</code> block to ensure the client is closed even if an error occurs.</p>"},{"location":"tutorials/cosmotech-api/#working-with-workspaces","title":"Working with Workspaces","text":"<p>Workspaces in the CosmoTech platform provide a way to organize and share files. The CoAL library offers functions for listing, downloading, and uploading files in workspaces.</p> Workspace operations<pre><code># Example: Working with workspaces in the CosmoTech API\nimport os\nimport pathlib\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.workspace import (\n    list_workspace_files,\n    download_workspace_file,\n    upload_workspace_file,\n)\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization and workspace IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Example 1: List files in a workspace with a specific prefix\n    file_prefix = \"data/\"  # List files in the \"data\" directory\n    try:\n        files = list_workspace_files(api_client, organization_id, workspace_id, file_prefix)\n        print(f\"Files in workspace with prefix '{file_prefix}':\")\n        for file in files:\n            print(f\"  - {file}\")\n    except ValueError as e:\n        print(f\"Error listing files: {e}\")\n\n    # Example 2: Download a file from the workspace\n    file_to_download = \"data/sample.csv\"  # Replace with an actual file in your workspace\n    target_directory = pathlib.Path(\"./downloaded_files\")\n    target_directory.mkdir(exist_ok=True, parents=True)\n\n    try:\n        downloaded_file = download_workspace_file(\n            api_client, organization_id, workspace_id, file_to_download, target_directory\n        )\n        print(f\"Downloaded file to: {downloaded_file}\")\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n\n    # Example 3: Upload a file to the workspace\n    file_to_upload = \"./local_data/upload_sample.csv\"  # Replace with a local file path\n    workspace_destination = \"data/uploaded/\"  # Destination in the workspace (ending with / to keep filename)\n\n    try:\n        uploaded_file = upload_workspace_file(\n            api_client,\n            organization_id,\n            workspace_id,\n            file_to_upload,\n            workspace_destination,\n            overwrite=True,  # Set to False to prevent overwriting existing files\n        )\n        print(f\"Uploaded file as: {uploaded_file}\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#listing-files","title":"Listing Files","text":"<p>The <code>list_workspace_files</code> function allows you to list files in a workspace with a specific prefix:</p> <pre><code>files = list_workspace_files(api_client, organization_id, workspace_id, file_prefix)\n</code></pre> <p>This is useful for finding files in a specific directory or with a specific naming pattern.</p>"},{"location":"tutorials/cosmotech-api/#downloading-files","title":"Downloading Files","text":"<p>The <code>download_workspace_file</code> function downloads a file from the workspace to a local directory:</p> <pre><code>downloaded_file = download_workspace_file(\n    api_client, \n    organization_id, \n    workspace_id, \n    file_to_download, \n    target_directory\n)\n</code></pre> <p>If the file is in a subdirectory in the workspace, the function will create the necessary local subdirectories.</p>"},{"location":"tutorials/cosmotech-api/#uploading-files","title":"Uploading Files","text":"<p>The <code>upload_workspace_file</code> function uploads a local file to the workspace:</p> <pre><code>uploaded_file = upload_workspace_file(\n    api_client,\n    organization_id,\n    workspace_id,\n    file_to_upload,\n    workspace_destination,\n    overwrite=True\n)\n</code></pre> <p>The <code>workspace_destination</code> parameter can be: - A specific file path in the workspace - A directory path ending with <code>/</code>, in which case the original filename is preserved</p> <p>Workspace Paths</p> <p>When working with workspace paths:</p> <ul> <li>Use forward slashes (<code>/</code>) regardless of your operating system</li> <li>End directory paths with a trailing slash (<code>/</code>)</li> <li>Use relative paths from the workspace root</li> </ul>"},{"location":"tutorials/cosmotech-api/#twin-data-layer-operations","title":"Twin Data Layer Operations","text":"<p>The Twin Data Layer (TDL) is a graph database that stores nodes and relationships. CoAL provides tools for working with the TDL, particularly for preparing and sending CSV data.</p> Twin Data Layer operations<pre><code># Example: Working with the Twin Data Layer in the CosmoTech API\nimport os\nimport pathlib\nimport csv\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.twin_data_layer import CSVSourceFile\nfrom cosmotech_api.api.twin_graph_api import TwinGraphApi\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization and workspace IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\ntwin_graph_id = \"your-twin-graph-id\"  # Replace with your twin graph ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Create a TwinGraphApi instance\n    twin_graph_api = TwinGraphApi(api_client)\n\n    # Example 1: Create sample CSV files for nodes and relationships\n\n    # Create a directory for our sample data\n    data_dir = pathlib.Path(\"./tdl_sample_data\")\n    data_dir.mkdir(exist_ok=True, parents=True)\n\n    # Create a sample nodes CSV file (Person nodes)\n    persons_file = data_dir / \"Person.csv\"\n    with open(persons_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"id\", \"name\", \"age\", \"city\"])\n        writer.writerow([\"p1\", \"Alice\", \"30\", \"New York\"])\n        writer.writerow([\"p2\", \"Bob\", \"25\", \"San Francisco\"])\n        writer.writerow([\"p3\", \"Charlie\", \"35\", \"Chicago\"])\n\n    # Create a sample relationships CSV file (KNOWS relationships)\n    knows_file = data_dir / \"KNOWS.csv\"\n    with open(knows_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"src\", \"dest\", \"since\"])\n        writer.writerow([\"p1\", \"p2\", \"2020\"])\n        writer.writerow([\"p2\", \"p3\", \"2021\"])\n        writer.writerow([\"p3\", \"p1\", \"2019\"])\n\n    print(f\"Created sample CSV files in {data_dir}\")\n\n    # Example 2: Parse CSV files and generate Cypher queries\n\n    # Parse the nodes CSV file\n    person_csv = CSVSourceFile(persons_file)\n    print(f\"Parsed {person_csv.object_type} CSV file:\")\n    print(f\"  Is node: {person_csv.is_node}\")\n    print(f\"  Fields: {person_csv.fields}\")\n    print(f\"  ID column: {person_csv.id_column}\")\n\n    # Generate a Cypher query for creating nodes\n    person_query = person_csv.generate_query_insert()\n    print(f\"\\nGenerated Cypher query for {person_csv.object_type}:\")\n    print(person_query)\n\n    # Parse the relationships CSV file\n    knows_csv = CSVSourceFile(knows_file)\n    print(f\"\\nParsed {knows_csv.object_type} CSV file:\")\n    print(f\"  Is node: {knows_csv.is_node}\")\n    print(f\"  Fields: {knows_csv.fields}\")\n    print(f\"  Source column: {knows_csv.source_column}\")\n    print(f\"  Target column: {knows_csv.target_column}\")\n\n    # Generate a Cypher query for creating relationships\n    knows_query = knows_csv.generate_query_insert()\n    print(f\"\\nGenerated Cypher query for {knows_csv.object_type}:\")\n    print(knows_query)\n\n    # Example 3: Send data to the Twin Data Layer (commented out as it requires an actual twin graph)\n    \"\"\"\n    # For nodes, you would typically:\n    with open(persons_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": person_query,\n                    \"parameters\": params\n                }\n            )\n\n    # For relationships, you would typically:\n    with open(knows_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": knows_query,\n                    \"parameters\": params\n                }\n            )\n    \"\"\"\n\n    # Example 4: Query data from the Twin Data Layer (commented out as it requires an actual twin graph)\n    \"\"\"\n    # Execute a Cypher query to get all Person nodes\n    result = twin_graph_api.run_twin_graph_cypher_query(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        twin_graph_id=twin_graph_id,\n        twin_graph_cypher_query={\n            \"query\": \"MATCH (p:Person) RETURN p.id, p.name, p.age, p.city\",\n            \"parameters\": {}\n        }\n    )\n\n    # Process the results\n    print(\"\\nPerson nodes in the Twin Data Layer:\")\n    for record in result.records:\n        print(f\"  - {record}\")\n    \"\"\"\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#csv-file-format","title":"CSV File Format","text":"<p>The TDL expects CSV files in a specific format:</p> <ul> <li>Node files: Must have an <code>id</code> column and can have additional property columns</li> <li>Relationship files: Must have <code>src</code> and <code>dest</code> columns and can have additional property columns</li> </ul> <p>The filename (without the <code>.csv</code> extension) becomes the node label or relationship type in the graph.</p>"},{"location":"tutorials/cosmotech-api/#parsing-csv-files","title":"Parsing CSV Files","text":"<p>The <code>CSVSourceFile</code> class helps parse CSV files and determine if they represent nodes or relationships:</p> <pre><code>csv_file = CSVSourceFile(file_path)\nprint(f\"Is node: {csv_file.is_node}\")\nprint(f\"Fields: {csv_file.fields}\")\n</code></pre>"},{"location":"tutorials/cosmotech-api/#generating-cypher-queries","title":"Generating Cypher Queries","text":"<p>The <code>generate_query_insert</code> method creates Cypher queries for inserting data into the TDL:</p> <pre><code>query = csv_file.generate_query_insert()\n</code></pre> <p>These queries can then be executed using the TwinGraphApi:</p> <pre><code>twin_graph_api.run_twin_graph_cypher_query(\n    organization_id=organization_id,\n    workspace_id=workspace_id,\n    twin_graph_id=twin_graph_id,\n    twin_graph_cypher_query={\n        \"query\": query,\n        \"parameters\": params\n    }\n)\n</code></pre> <p>Node References</p> <p>When creating relationships, make sure the nodes referenced by the <code>src</code> and <code>dest</code> columns already exist in the graph. Otherwise, the relationship creation will fail.</p>"},{"location":"tutorials/cosmotech-api/#runner-and-run-management","title":"Runner and Run Management","text":"<p>Runners and runs are central concepts in the CosmoTech platform. CoAL provides functions for working with runner data, parameters, and associated datasets.</p> Runner operations<pre><code># Example: Working with runners and runs in the CosmoTech API\nimport os\nimport pathlib\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.runner import (\n    get_runner_data,\n    get_runner_parameters,\n    download_runner_data,\n    download_datasets,\n)\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization, workspace, and runner IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\nrunner_id = \"your-runner-id\"  # Replace with your runner ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Example 1: Get runner data\n    runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n    print(f\"Runner name: {runner_data.name}\")\n    print(f\"Runner ID: {runner_data.id}\")\n    print(f\"Runner state: {runner_data.state}\")\n\n    # Example 2: Get runner parameters\n    parameters = get_runner_parameters(runner_data)\n    print(\"\\nRunner parameters:\")\n    for param in parameters:\n        print(f\"  - {param['parameterId']}: {param['value']} (type: {param['varType']})\")\n\n    # Example 3: Download runner data (parameters and datasets)\n    # Create directories for parameters and datasets\n    param_dir = pathlib.Path(\"./runner_parameters\")\n    dataset_dir = pathlib.Path(\"./runner_datasets\")\n    param_dir.mkdir(exist_ok=True, parents=True)\n    dataset_dir.mkdir(exist_ok=True, parents=True)\n\n    # Download runner data\n    result = download_runner_data(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        runner_id=runner_id,\n        parameter_folder=str(param_dir),\n        dataset_folder=str(dataset_dir),\n        read_files=True,  # Read file contents\n        parallel=True,  # Download datasets in parallel\n        write_json=True,  # Write parameters as JSON\n        write_csv=True,  # Write parameters as CSV\n        fetch_dataset=True,  # Fetch datasets\n    )\n\n    print(\"\\nDownloaded runner data:\")\n    print(f\"  - Parameters saved to: {param_dir}\")\n    print(f\"  - Datasets saved to: {dataset_dir}\")\n\n    # Example 4: Working with specific datasets\n    if result[\"datasets\"]:\n        print(\"\\nDatasets associated with the runner:\")\n        for dataset_id, dataset_info in result[\"datasets\"].items():\n            print(f\"  - Dataset ID: {dataset_id}\")\n            print(f\"    Name: {dataset_info.get('name', 'N/A')}\")\n\n            # List files in the dataset\n            if \"files\" in dataset_info:\n                print(f\"    Files:\")\n                for file_info in dataset_info[\"files\"]:\n                    print(f\"      - {file_info.get('name', 'N/A')}\")\n    else:\n        print(\"\\nNo datasets associated with this runner.\")\n\n    # Example 5: Download specific datasets\n    \"\"\"\n    from cosmotech.coal.cosmotech_api.runner import get_dataset_ids_from_runner\n\n    # Get dataset IDs from the runner\n    dataset_ids = get_dataset_ids_from_runner(runner_data)\n\n    if dataset_ids:\n        # Create a directory for the datasets\n        specific_dataset_dir = pathlib.Path(\"./specific_datasets\")\n        specific_dataset_dir.mkdir(exist_ok=True, parents=True)\n\n        # Download the datasets\n        datasets = download_datasets(\n            organization_id=organization_id,\n            workspace_id=workspace_id,\n            dataset_ids=dataset_ids,\n            read_files=True,\n            parallel=True,\n        )\n\n        print(\"\\nDownloaded specific datasets:\")\n        for dataset_id, dataset_info in datasets.items():\n            print(f\"  - Dataset ID: {dataset_id}\")\n            print(f\"    Name: {dataset_info.get('name', 'N/A')}\")\n    \"\"\"\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#getting-runner-data","title":"Getting Runner Data","text":"<p>The <code>get_runner_data</code> function retrieves information about a runner:</p> <pre><code>runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n</code></pre>"},{"location":"tutorials/cosmotech-api/#working-with-parameters","title":"Working with Parameters","text":"<p>The <code>get_runner_parameters</code> function extracts parameters from runner data:</p> <pre><code>parameters = get_runner_parameters(runner_data)\n</code></pre>"},{"location":"tutorials/cosmotech-api/#downloading-runner-data","title":"Downloading Runner Data","text":"<p>The <code>download_runner_data</code> function downloads all data associated with a runner, including parameters and datasets:</p> <pre><code>result = download_runner_data(\n    organization_id=organization_id,\n    workspace_id=workspace_id,\n    runner_id=runner_id,\n    parameter_folder=str(param_dir),\n    dataset_folder=str(dataset_dir),\n    write_json=True,\n    write_csv=True,\n    fetch_dataset=True,\n)\n</code></pre> <p>This function: - Downloads parameters and writes them as JSON and/or CSV files - Downloads associated datasets - Organizes everything in the specified directories</p> <p>Dataset References</p> <p>Runners can reference datasets in two ways:</p> <ul> <li>Through parameters with the <code>%DATASETID%</code> variable type</li> <li>Through the <code>dataset_list</code> property</li> </ul> <p>The <code>download_runner_data</code> function handles both types of references.</p>"},{"location":"tutorials/cosmotech-api/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Putting it all together, here's a complete workflow that demonstrates how to use the CosmoTech API for a data processing pipeline:</p> Complete workflow<pre><code># Example: Complete workflow using the CosmoTech API\nimport os\nimport pathlib\nimport json\nimport csv\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.runner import (\n    get_runner_data,\n    download_runner_data,\n)\nfrom cosmotech.coal.cosmotech_api.workspace import (\n    list_workspace_files,\n    download_workspace_file,\n    upload_workspace_file,\n)\nfrom cosmotech.coal.cosmotech_api.twin_data_layer import CSVSourceFile\nfrom cosmotech_api.api.twin_graph_api import TwinGraphApi\nfrom cosmotech_api.api.dataset_api import DatasetApi\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization, workspace, and runner IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\nrunner_id = \"your-runner-id\"  # Replace with your runner ID\ntwin_graph_id = \"your-twin-graph-id\"  # Replace with your twin graph ID\n\n# Create directories for our workflow\nworkflow_dir = pathlib.Path(\"./workflow_example\")\nworkflow_dir.mkdir(exist_ok=True, parents=True)\n\ninput_dir = workflow_dir / \"input\"\nprocessed_dir = workflow_dir / \"processed\"\noutput_dir = workflow_dir / \"output\"\n\ninput_dir.mkdir(exist_ok=True, parents=True)\nprocessed_dir.mkdir(exist_ok=True, parents=True)\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Step 1: Download runner data (parameters and datasets)\n    print(\"\\n=== Step 1: Download Runner Data ===\")\n\n    runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n    print(f\"Runner name: {runner_data.name}\")\n\n    result = download_runner_data(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        runner_id=runner_id,\n        parameter_folder=str(input_dir / \"parameters\"),\n        dataset_folder=str(input_dir / \"datasets\"),\n        write_json=True,\n        write_csv=True,\n    )\n\n    print(f\"Downloaded runner data to {input_dir}\")\n\n    # Step 2: Process the data\n    print(\"\\n=== Step 2: Process Data ===\")\n\n    # For this example, we'll create a simple transformation:\n    # - Read a CSV file from the input\n    # - Transform it\n    # - Write the result to the processed directory\n\n    # Let's assume we have a \"customers.csv\" file in the input directory\n    customers_file = input_dir / \"datasets\" / \"customers.csv\"\n\n    # If the file doesn't exist for this example, create a sample one\n    if not customers_file.exists():\n        print(\"Creating sample customers.csv file for demonstration\")\n        customers_file.parent.mkdir(exist_ok=True, parents=True)\n        with open(customers_file, \"w\", newline=\"\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"id\", \"name\", \"age\", \"city\", \"spending\"])\n            writer.writerow([\"c1\", \"Alice\", \"30\", \"New York\", \"1500\"])\n            writer.writerow([\"c2\", \"Bob\", \"25\", \"San Francisco\", \"2000\"])\n            writer.writerow([\"c3\", \"Charlie\", \"35\", \"Chicago\", \"1200\"])\n\n    # Read the customers data\n    customers = []\n    with open(customers_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            customers.append(row)\n\n    print(f\"Read {len(customers)} customers from {customers_file}\")\n\n    # Process the data: calculate a loyalty score based on age and spending\n    for customer in customers:\n        age = int(customer[\"age\"])\n        spending = int(customer[\"spending\"])\n\n        # Simple formula: loyalty score = spending / 100 + (age - 20) / 10\n        loyalty_score = round(spending / 100 + (age - 20) / 10, 1)\n        customer[\"loyalty_score\"] = str(loyalty_score)\n\n    # Write the processed data\n    processed_file = processed_dir / \"customers_with_loyalty.csv\"\n    with open(processed_file, \"w\", newline=\"\") as f:\n        fieldnames = [\"id\", \"name\", \"age\", \"city\", \"spending\", \"loyalty_score\"]\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(customers)\n\n    print(f\"Processed data written to {processed_file}\")\n\n    # Step 3: Upload the processed file to the workspace\n    print(\"\\n=== Step 3: Upload Processed Data to Workspace ===\")\n\n    try:\n        uploaded_file = upload_workspace_file(\n            api_client,\n            organization_id,\n            workspace_id,\n            str(processed_file),\n            \"processed_data/\",  # Destination in the workspace\n            overwrite=True,\n        )\n        print(f\"Uploaded processed file as: {uploaded_file}\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\n    # Step 4: Create a dataset from the processed data\n    print(\"\\n=== Step 4: Create Dataset from Processed Data ===\")\n\n    # This step would typically involve:\n    # 1. Creating a dataset in the CosmoTech API\n    # 2. Uploading files to the dataset\n\n    \"\"\"\n    # Create a dataset\n    dataset_api = DatasetApi(api_client)\n\n    new_dataset = {\n        \"name\": \"Customers with Loyalty Scores\",\n        \"description\": \"Processed customer data with calculated loyalty scores\",\n        \"tags\": [\"processed\", \"customers\", \"loyalty\"]\n    }\n\n    try:\n        dataset = dataset_api.create_dataset(\n            organization_id=organization_id,\n            workspace_id=workspace_id,\n            dataset=new_dataset\n        )\n\n        dataset_id = dataset.id\n        print(f\"Created dataset with ID: {dataset_id}\")\n\n        # Upload the processed file to the dataset\n        # This would typically involve additional API calls\n        # ...\n\n    except Exception as e:\n        print(f\"Error creating dataset: {e}\")\n    \"\"\"\n\n    # Step 5: Send data to the Twin Data Layer\n    print(\"\\n=== Step 5: Send Data to Twin Data Layer ===\")\n\n    # Parse the processed CSV file for the Twin Data Layer\n    customer_csv = CSVSourceFile(processed_file)\n\n    # Generate a Cypher query for creating nodes\n    customer_query = customer_csv.generate_query_insert()\n    print(f\"Generated Cypher query for Customer nodes:\")\n    print(customer_query)\n\n    # In a real scenario, you would send this data to the Twin Data Layer\n    \"\"\"\n    twin_graph_api = TwinGraphApi(api_client)\n\n    # For each customer, create a node in the Twin Data Layer\n    with open(processed_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": customer_query,\n                    \"parameters\": params\n                }\n            )\n    \"\"\"\n\n    # Step 6: Generate a report\n    print(\"\\n=== Step 6: Generate Report ===\")\n\n    # Calculate some statistics\n    total_customers = len(customers)\n    avg_age = sum(int(c[\"age\"]) for c in customers) / total_customers\n    avg_spending = sum(int(c[\"spending\"]) for c in customers) / total_customers\n    avg_loyalty = sum(float(c[\"loyalty_score\"]) for c in customers) / total_customers\n\n    # Create a report\n    report = {\n        \"report_date\": \"2025-02-28\",\n        \"runner_id\": runner_id,\n        \"statistics\": {\n            \"total_customers\": total_customers,\n            \"average_age\": round(avg_age, 1),\n            \"average_spending\": round(avg_spending, 2),\n            \"average_loyalty_score\": round(avg_loyalty, 1),\n        },\n        \"top_customers\": sorted(customers, key=lambda c: float(c[\"loyalty_score\"]), reverse=True)[\n            :2\n        ],  # Top 2 customers by loyalty score\n    }\n\n    # Write the report to a JSON file\n    report_file = output_dir / \"customer_report.json\"\n    with open(report_file, \"w\") as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Report generated and saved to {report_file}\")\n\n    # Print a summary of the report\n    print(\"\\nReport Summary:\")\n    print(f\"Total Customers: {report['statistics']['total_customers']}\")\n    print(f\"Average Age: {report['statistics']['average_age']}\")\n    print(f\"Average Spending: {report['statistics']['average_spending']}\")\n    print(f\"Average Loyalty Score: {report['statistics']['average_loyalty_score']}\")\n    print(\"\\nTop Customers by Loyalty Score:\")\n    for i, customer in enumerate(report[\"top_customers\"], 1):\n        print(f\"{i}. {customer['name']} (Score: {customer['loyalty_score']})\")\n\n    print(\"\\nWorkflow completed successfully!\")\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre> <p>This workflow:</p> <ol> <li>Downloads runner data (parameters and datasets)</li> <li>Processes the data (calculates loyalty scores for customers)</li> <li>Uploads the processed data to the workspace</li> <li>Prepares the data for the Twin Data Layer</li> <li>Generates a report with statistics and insights</li> </ol> <p>Real-world Workflows</p> <p>In real-world scenarios, you might:</p> <ul> <li>Use more complex data transformations</li> <li>Integrate with external systems</li> <li>Implement error handling and retries</li> <li>Add logging and monitoring</li> <li>Parallelize operations for better performance</li> </ul>"},{"location":"tutorials/cosmotech-api/#best-practices-and-tips","title":"Best Practices and Tips","text":""},{"location":"tutorials/cosmotech-api/#authentication","title":"Authentication","text":"<ul> <li>Use environment variables for credentials</li> <li>Implement proper secret management in production</li> <li>Always close API clients when done</li> </ul>"},{"location":"tutorials/cosmotech-api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # API operations\nexcept cosmotech_api.exceptions.ApiException as e:\n    # Handle API errors\n    print(f\"API error: {e.status} - {e.reason}\")\nexcept Exception as e:\n    # Handle other errors\n    print(f\"Error: {e}\")\nfinally:\n    # Always close the client\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Download datasets in parallel when possible (<code>parallel=True</code>)</li> <li>Batch operations when sending multiple items to the API</li> <li>Use appropriate error handling and retries for network operations</li> </ul>"},{"location":"tutorials/cosmotech-api/#security","title":"Security","text":"<ul> <li>Never hardcode credentials in your code</li> <li>Use the principle of least privilege for API keys and service principals</li> <li>Validate and sanitize inputs before sending them to the API</li> </ul>"},{"location":"tutorials/cosmotech-api/#conclusion","title":"Conclusion","text":"<p>The CosmoTech API integration in CoAL provides a powerful way to interact with the CosmoTech platform programmatically. By leveraging these capabilities, you can:</p> <ul> <li>Automate workflows</li> <li>Integrate with other systems</li> <li>Build custom applications</li> <li>Process and analyze data</li> <li>Create end-to-end solutions</li> </ul> <p>Whether you're building data pipelines, creating custom interfaces, or integrating with existing systems, the CoAL library's API integration offers the tools you need to work effectively with the CosmoTech platform.</p>"},{"location":"tutorials/csm-data/","title":"CSM-DATA","text":"<p>Objective</p> <ul> <li>Understand what the csm-data CLI is and its capabilities</li> <li>Learn how to use the various command groups for different data management tasks</li> <li>Explore common use cases and workflows</li> <li>Master integration with CosmoTech platform services</li> </ul>"},{"location":"tutorials/csm-data/#what-is-csm-data","title":"What is csm-data?","text":"<p><code>csm-data</code> is a powerful Command Line Interface (CLI) bundled inside the CosmoTech Acceleration Library (CoAL). It provides a comprehensive set of commands designed to streamline interactions with various services used within a CosmoTech platform.</p> <p>The CLI is organized into several command groups, each focused on specific types of data operations:</p> <ul> <li>api: Commands for interacting with the CosmoTech API</li> <li>store: Commands for working with the CoAL datastore</li> <li>s3-bucket-*: Commands for S3 bucket operations (download, upload, delete)</li> <li>adx-send-runnerdata: Command for sending runner data to Azure Data Explorer</li> <li>az-storage-upload: Command for uploading to Azure Storage</li> </ul> <p>Getting Help</p> <p>You can get detailed help for any command using the <code>--help</code> flag: <pre><code>csm-data --help\ncsm-data api --help\ncsm-data api run-load-data --help\n</code></pre></p>"},{"location":"tutorials/csm-data/#why-use-csm-data","title":"Why use csm-data?","text":""},{"location":"tutorials/csm-data/#standardized-interactions","title":"Standardized Interactions","text":"<p>The <code>csm-data</code> CLI provides tested, standardized interactions with multiple services used in CosmoTech simulations. This eliminates the need to:</p> <ul> <li>Write custom code for common data operations</li> <li>Handle authentication and connection details for each service</li> <li>Manage error handling and retries</li> <li>Deal with format conversions between services</li> </ul>"},{"location":"tutorials/csm-data/#environment-variable-support","title":"Environment Variable Support","text":"<p>Most commands support environment variables, making them ideal for:</p> <ul> <li>Integration with orchestration tools like <code>csm-orc</code></li> <li>Use in Docker containers and cloud environments</li> <li>Secure handling of credentials and connection strings</li> <li>Consistent configuration across development and production</li> </ul>"},{"location":"tutorials/csm-data/#workflow-automation","title":"Workflow Automation","text":"<p>The commands are designed to work together in data processing pipelines, enabling you to:</p> <ul> <li>Download data from various sources</li> <li>Transform and process the data</li> <li>Store results in different storage systems</li> <li>Send data to visualization and analysis services</li> </ul>"},{"location":"tutorials/csm-data/#command-groups-and-use-cases","title":"Command Groups and Use Cases","text":""},{"location":"tutorials/csm-data/#api-commands","title":"API Commands","text":"<p>The <code>api</code> command group facilitates interaction with the CosmoTech API, allowing you to work with scenarios, datasets, and other API resources.</p>"},{"location":"tutorials/csm-data/#runner-data-management","title":"Runner Data Management","text":"Download run data<pre><code>csm-data api run-load-data \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --runner-id \"r-runner\" \\\n  --dataset-absolute-path \"/path/to/dataset/folder\" \\\n  --parameters-absolute-path \"/path/to/parameters/folder\" \\\n  --write-json \\\n  --write-csv \\\n  --fetch-dataset\n</code></pre> <p>This command: - Downloads scenario parameters and datasets from the CosmoTech API - Writes parameters as JSON and/or CSV files - Fetches associated datasets</p> <p>Common Use Case</p> <p>This command is particularly useful in container environments where you need to initialize your simulation with data from the platform. The environment variables are typically set by the platform when launching the container.</p>"},{"location":"tutorials/csm-data/#twin-data-layer-operations","title":"Twin Data Layer Operations","text":"Load files to Twin Data Layer<pre><code>csm-data api tdl-load-files \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --dataset-id \"d-dataset\" \\\n  --source-folder \"/path/to/source/files\"\n</code></pre> Send files to Twin Data Layer<pre><code>csm-data api tdl-send-files \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --dataset-id \"d-dataset\" \\\n  --source-folder \"/path/to/source/files\"\n</code></pre> <p>These commands facilitate working with the Twin Data Layer, allowing you to: - Load data from the Twin Data Layer to local files - Send local files to the Twin Data Layer</p>"},{"location":"tutorials/csm-data/#storage-commands","title":"Storage Commands","text":"<p>The <code>s3-bucket-*</code> commands provide a simple interface for working with S3-compatible storage:</p> DownloadUploadDelete Download from S3 bucket<pre><code>csm-data s3-bucket-download \\\n  --target-folder \"/path/to/download/to\" \\\n  --bucket-name \"my-bucket\" \\\n  --prefix-filter \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> Upload to S3 bucket<pre><code>csm-data s3-bucket-upload \\\n  --source-folder \"/path/to/upload/from\" \\\n  --bucket-name \"my-bucket\" \\\n  --target-prefix \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> Delete from S3 bucket<pre><code>csm-data s3-bucket-delete \\\n  --bucket-name \"my-bucket\" \\\n  --prefix-filter \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> <p>Environment Variables</p> <p>All these commands support environment variables for credentials and connection details, making them secure and easy to use in automated workflows: <pre><code>export AWS_ENDPOINT_URL=\"https://s3.example.com\"\nexport AWS_ACCESS_KEY_ID=\"access-key-id\"\nexport AWS_SECRET_ACCESS_KEY=\"secret-access-key\"\nexport CSM_DATA_BUCKET_NAME=\"my-bucket\"\n</code></pre></p>"},{"location":"tutorials/csm-data/#azure-data-explorer-integration","title":"Azure Data Explorer Integration","text":"<p>The <code>adx-send-runnerdata</code> command enables sending runner data to Azure Data Explorer for analysis and visualization:</p> Send runner data to ADX<pre><code>csm-data adx-send-runnerdata \\\n  --dataset-absolute-path \"/path/to/dataset/folder\" \\\n  --parameters-absolute-path \"/path/to/parameters/folder\" \\\n  --runner-id \"runner-id\" \\\n  --adx-uri \"https://adx.example.com\" \\\n  --adx-ingest-uri \"https://ingest-adx.example.com\" \\\n  --database-name \"my-database\" \\\n  --send-datasets \\\n  --wait\n</code></pre> <p>This command: - Creates tables in ADX based on CSV files in the dataset and/or parameters folders - Ingests the data into those tables - Adds a <code>run</code> column with the runner ID for tracking - Optionally waits for ingestion to complete</p> <p>Table Creation</p> <p>This command will create tables in ADX based on the CSV file names and headers. Ensure your CSV files have appropriate headers and follow naming conventions suitable for ADX tables.</p>"},{"location":"tutorials/csm-data/#datastore-commands","title":"Datastore Commands","text":"<p>The <code>store</code> command group provides tools for working with the CoAL datastore:</p> Load CSV folder into datastore<pre><code>csm-data store load-csv-folder \\\n  --folder-path \"/path/to/csv/folder\" \\\n  --reset\n</code></pre> Dump datastore to S3<pre><code>csm-data store dump-to-s3 \\\n  --bucket-name \"my-bucket\" \\\n  --target-prefix \"store-dump/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> <p>These commands allow you to: - Load data from CSV files into the datastore - Dump datastore contents to various destinations (S3, Azure, PostgreSQL) - List tables in the datastore - Reset the datastore</p>"},{"location":"tutorials/csm-data/#common-workflows-and-integration-patterns","title":"Common Workflows and Integration Patterns","text":""},{"location":"tutorials/csm-data/#runner-data-processing-pipeline","title":"Runner Data Processing Pipeline","text":"<p>A common workflow combines multiple commands to create a complete data processing pipeline:</p> Complete data processing pipeline<pre><code># 1. Download runner data from the API\ncsm-data api run-load-data \\\n  --organization-id \"$CSM_ORGANIZATION_ID\" \\\n  --workspace-id \"$CSM_WORKSPACE_ID\" \\\n  --runner-id \"$CSM_RUNNER_ID\" \\\n  --dataset-absolute-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --parameters-absolute-path \"$CSM_PARAMETERS_ABSOLUTE_PATH\" \\\n  --write-json \\\n  --fetch-dataset\n\n# 2. Load data into the datastore for processing\ncsm-data store load-csv-folder \\\n  --folder-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --reset\n\n# 3. Run your simulation (using your own code)\n# ...\n\n# 4. Send results to Azure Data Explorer for analysis\ncsm-data adx-send-runnerdata \\\n  --dataset-absolute-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --parameters-absolute-path \"$CSM_PARAMETERS_ABSOLUTE_PATH\" \\\n  --runner-id \"$CSM_RUNNER_ID\" \\\n  --adx-uri \"$AZURE_DATA_EXPLORER_RESOURCE_URI\" \\\n  --adx-ingest-uri \"$AZURE_DATA_EXPLORER_RESOURCE_INGEST_URI\" \\\n  --database-name \"$AZURE_DATA_EXPLORER_DATABASE_NAME\" \\\n  --send-datasets \\\n  --wait\n</code></pre>"},{"location":"tutorials/csm-data/#integration-with-csm-orc","title":"Integration with csm-orc","text":"<p>The <code>csm-data</code> commands integrate seamlessly with <code>csm-orc</code> for orchestration:</p> run.json for csm-orc<pre><code>{\n  \"steps\": [\n    {\n      \"id\": \"download-scenario-data\",\n      \"command\": \"csm-data\",\n      \"arguments\": [\n        \"api\", \"scenariorun-load-data\",\n        \"--write-json\",\n        \"--fetch-dataset\"\n      ],\n      \"useSystemEnvironment\": true\n    },\n    {\n      \"id\": \"run-simulation\",\n      \"command\": \"python\",\n      \"arguments\": [\"run_simulation.py\"],\n      \"precedents\": [\"download-scenario-data\"]\n    },\n    {\n      \"id\": \"send-results-to-adx\",\n      \"command\": \"csm-data\",\n      \"arguments\": [\n        \"adx-send-scenariodata\",\n        \"--send-datasets\",\n        \"--wait\"\n      ],\n      \"useSystemEnvironment\": true,\n      \"precedents\": [\"run-simulation\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/csm-data/#best-practices-and-tips","title":"Best Practices and Tips","text":"<p>Environment Variables</p> <p>Use environment variables for sensitive information and configuration that might change between environments: <pre><code># API connection\nexport CSM_ORGANIZATION_ID=\"o-organization\"\nexport CSM_WORKSPACE_ID=\"w-workspace\"\nexport CSM_SCENARIO_ID=\"s-scenario\"\n\n# Paths\nexport CSM_DATASET_ABSOLUTE_PATH=\"/path/to/dataset\"\nexport CSM_PARAMETERS_ABSOLUTE_PATH=\"/path/to/parameters\"\n\n# ADX connection\nexport AZURE_DATA_EXPLORER_RESOURCE_URI=\"https://adx.example.com\"\nexport AZURE_DATA_EXPLORER_RESOURCE_INGEST_URI=\"https://ingest-adx.example.com\"\nexport AZURE_DATA_EXPLORER_DATABASE_NAME=\"my-database\"\n</code></pre></p> <p>Error Handling</p> <p>Most commands will exit with a non-zero status code on failure, making them suitable for use in scripts and orchestration tools that check exit codes.</p> <p>Logging</p> <p>Control the verbosity of logging with the <code>--log-level</code> option: <pre><code>csm-data --log-level debug api run-load-data ...\n</code></pre></p>"},{"location":"tutorials/csm-data/#extending-csm-data","title":"Extending csm-data","text":"<p>If the existing commands don't exactly match your needs, you have several options:</p> <ol> <li>Use as a basis: Examine the code of similar commands and use it as a starting point for your own scripts</li> <li>Combine commands: Use shell scripting to combine multiple commands into a custom workflow</li> <li>Environment variables: Customize behavior through environment variables without modifying the code</li> <li>Contribute: Consider contributing enhancements back to the CoAL project</li> </ol>"},{"location":"tutorials/csm-data/#conclusion","title":"Conclusion","text":"<p>The <code>csm-data</code> CLI provides a powerful set of tools for managing data in CosmoTech platform environments. By leveraging these commands, you can:</p> <ul> <li>Streamline interactions with platform services</li> <li>Automate data processing workflows</li> <li>Integrate with orchestration tools</li> <li>Focus on your simulation logic rather than data handling</li> </ul> <p>Whether you're developing locally or deploying to production, <code>csm-data</code> offers a consistent interface for your data management needs.</p>"},{"location":"tutorials/datastore/","title":"Datastore","text":"<p>Objective</p> <ul> <li>Understand what the CoAL datastore is and its capabilities</li> <li>Learn how to store and retrieve data in various formats</li> <li>Master SQL querying capabilities for data analysis</li> <li>Build efficient data processing pipelines</li> </ul>"},{"location":"tutorials/datastore/#what-is-the-datastore","title":"What is the datastore?","text":"<p>The datastore is a powerful data management abstraction that provides a unified interface to a SQLite database. It allows you to store, retrieve, transform, and query tabular data in various formats through a consistent API.</p> <p>The core idea behind the datastore is to provide a robust, flexible system for data management that simplifies working with different data formats while offering persistence and advanced query capabilities.</p> <p>Key Features</p> <ul> <li>Format flexibility (Python dictionaries, CSV files, Pandas DataFrames, PyArrow Tables)</li> <li>Persistent storage in SQLite</li> <li>SQL query capabilities</li> <li>Simplified data pipeline management</li> </ul>"},{"location":"tutorials/datastore/#why-use-the-datastore","title":"Why use the datastore?","text":""},{"location":"tutorials/datastore/#format-flexibility","title":"Format Flexibility","text":"<p>The datastore works seamlessly with multiple data formats:</p> <ul> <li>Python dictionaries and lists</li> <li>CSV files</li> <li>Pandas DataFrames</li> <li>PyArrow Tables</li> </ul> <p>This flexibility eliminates the need for manual format conversions and allows you to work with data in your preferred format.</p>"},{"location":"tutorials/datastore/#persistence-and-performance","title":"Persistence and Performance","text":"<p>Instead of keeping all your data in memory or writing/reading from files repeatedly, the datastore:</p> <ul> <li>Persists data in a SQLite database</li> <li>Provides efficient storage and retrieval</li> <li>Handles large datasets that might not fit in memory</li> <li>Maintains data between application runs</li> </ul>"},{"location":"tutorials/datastore/#sql-query-capabilities","title":"SQL Query Capabilities","text":"<p>The datastore leverages the power of SQL:</p> <ul> <li>Filter, aggregate, join, and transform data using familiar SQL syntax</li> <li>Execute complex queries without writing custom data manipulation code</li> <li>Perform operations that would be cumbersome with file-based approaches</li> </ul>"},{"location":"tutorials/datastore/#simplified-data-pipeline","title":"Simplified Data Pipeline","text":"<p>The datastore serves as a central hub in your data processing pipeline:</p> <ul> <li>Import data from various sources</li> <li>Transform and clean data</li> <li>Query and analyze data</li> <li>Export results in different formats</li> </ul>"},{"location":"tutorials/datastore/#basic-example","title":"Basic example","text":"Basic use of the datastore<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\n# We initialize and reset the data store\nmy_datastore = Store(reset=True)\n\n# We create a simple list of dict data\nmy_data = [{\"foo\": \"bar\"}, {\"foo\": \"barbar\"}, {\"foo\": \"world\"}, {\"foo\": \"bar\"}]\n\n# We use a bundled method to send the py_list to the store\nstore_pylist(\"my_data\", my_data)\n\n# We can make a sql query over our data\n# Store.execute_query returns a pyarrow.Table object so we can make use of Table.to_pylist to get an equivalent format\nresults = my_datastore.execute_query(\"SELECT foo, count(*) as line_count FROM my_data GROUP BY foo\").to_pylist()\n\n# We can print our results now\nprint(results)\n# &gt; [{'foo': 'bar', 'line_count': 2}, {'foo': 'barbar', 'line_count': 1}, {'foo': 'world', 'line_count': 1}]\n</code></pre>"},{"location":"tutorials/datastore/#working-with-different-data-formats","title":"Working with different data formats","text":"<p>The datastore provides specialized adapters for working with various data formats:</p>"},{"location":"tutorials/datastore/#csv-files","title":"CSV Files","text":"Working with CSV files<pre><code>import pathlib\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.csv import store_csv_file, convert_store_table_to_csv\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Load data from a CSV file\ncsv_path = pathlib.Path(\"path/to/your/data.csv\")\nstore_csv_file(\"customers\", csv_path)\n\n# Query the data\nhigh_value_customers = store.execute_query(\n    \"\"\"\n    SELECT * FROM customers \n    WHERE annual_spend &gt; 10000\n    ORDER BY annual_spend DESC\n\"\"\"\n)\n\n# Export results to a new CSV file\noutput_path = pathlib.Path(\"path/to/output/high_value_customers.csv\")\nconvert_store_table_to_csv(\"high_value_customers\", output_path)\n</code></pre>"},{"location":"tutorials/datastore/#pandas-dataframes","title":"Pandas DataFrames","text":"Working with pandas DataFrames<pre><code>import pandas as pd\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.pandas import store_dataframe, convert_store_table_to_dataframe\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(\n    {\n        \"product_id\": [1, 2, 3, 4, 5],\n        \"product_name\": [\"Widget A\", \"Widget B\", \"Gadget X\", \"Tool Y\", \"Device Z\"],\n        \"price\": [19.99, 29.99, 99.99, 49.99, 199.99],\n        \"category\": [\"Widgets\", \"Widgets\", \"Gadgets\", \"Tools\", \"Devices\"],\n    }\n)\n\n# Store the DataFrame\nstore_dataframe(\"products\", df)\n\n# Query the data\nexpensive_products = store.execute_query(\n    \"\"\"\n    SELECT * FROM products\n    WHERE price &gt; 50\n    ORDER BY price DESC\n\"\"\"\n)\n\n# Convert results back to a pandas DataFrame for further analysis\nexpensive_df = convert_store_table_to_dataframe(\"expensive_products\", store)\n\n# Use pandas methods on the result\nprint(expensive_df.describe())\n</code></pre>"},{"location":"tutorials/datastore/#pyarrow-tables","title":"PyArrow Tables","text":"Working with PyArrow Tables directly<pre><code>import pyarrow as pa\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.pyarrow import store_table\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Create a PyArrow Table\ndata = {\n    \"date\": pa.array([\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"]),\n    \"value\": pa.array([100, 150, 200]),\n    \"category\": pa.array([\"A\", \"B\", \"A\"]),\n}\ntable = pa.Table.from_pydict(data)\n\n# Store the table\nstore_table(\"time_series\", table)\n\n# Query and retrieve data\nresult = store.execute_query(\n    \"\"\"\n    SELECT date, SUM(value) as total_value\n    FROM time_series\n    GROUP BY date\n\"\"\"\n)\n\nprint(result)\n</code></pre>"},{"location":"tutorials/datastore/#advanced-use-cases","title":"Advanced use cases","text":""},{"location":"tutorials/datastore/#joining-multiple-tables","title":"Joining multiple tables","text":"Joining tables in the datastore<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\nstore = Store(reset=True)\n\n# Store customer data\ncustomers = [\n    {\"customer_id\": 1, \"name\": \"Acme Corp\", \"segment\": \"Enterprise\"},\n    {\"customer_id\": 2, \"name\": \"Small Shop\", \"segment\": \"SMB\"},\n    {\"customer_id\": 3, \"name\": \"Tech Giant\", \"segment\": \"Enterprise\"},\n]\nstore_pylist(\"customers\", customers, store=store)\n\n# Store order data\norders = [\n    {\"order_id\": 101, \"customer_id\": 1, \"amount\": 5000},\n    {\"order_id\": 102, \"customer_id\": 2, \"amount\": 500},\n    {\"order_id\": 103, \"customer_id\": 1, \"amount\": 7500},\n    {\"order_id\": 104, \"customer_id\": 3, \"amount\": 10000},\n]\nstore_pylist(\"orders\", orders, store=store)\n\n# Join tables to analyze orders by customer segment\nresults = store.execute_query(\n    \"\"\"\n    SELECT c.segment, COUNT(o.order_id) as order_count, SUM(o.amount) as total_revenue\n    FROM customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n    GROUP BY c.segment\n\"\"\"\n).to_pylist()\n\nprint(results)\n# &gt; [{'segment': 'Enterprise', 'order_count': 3, 'total_revenue': 22500}, {'segment': 'SMB', 'order_count': 1, 'total_revenue': 500}]\n</code></pre>"},{"location":"tutorials/datastore/#data-transformation-pipeline","title":"Data transformation pipeline","text":"Complete pipelineStep-by-step Building a data transformation pipeline<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist, convert_table_as_pylist\nimport pathlib\nfrom cosmotech.coal.store.csv import store_csv_file, convert_store_table_to_csv\n\n# Initialize the store\nstore = Store(reset=True)\n\n# 1. Load raw data from CSV\nraw_data_path = pathlib.Path(\"path/to/raw_data.csv\")\nstore_csv_file(\"raw_data\", raw_data_path, store=store)\n\n# 2. Clean and transform the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE cleaned_data AS\n    SELECT \n        id,\n        TRIM(name) as name,\n        UPPER(category) as category,\n        CASE WHEN value &lt; 0 THEN 0 ELSE value END as value\n    FROM raw_data\n    WHERE id IS NOT NULL\n\"\"\"\n)\n\n# 3. Aggregate the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE summary_data AS\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value\n    FROM cleaned_data\n    GROUP BY category\n\"\"\"\n)\n\n# 4. Export the results\nsummary_data = convert_table_as_pylist(\"summary_data\", store=store)\nprint(summary_data)\n\n# 5. Save to CSV for reporting\noutput_path = pathlib.Path(\"path/to/output/summary.csv\")\nconvert_store_table_to_csv(\"summary_data\", output_path, store=store)\n</code></pre> Step 1: Load data<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.csv import store_csv_file\nimport pathlib\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Load raw data from CSV\nraw_data_path = pathlib.Path(\"path/to/raw_data.csv\")\nstore_csv_file(\"raw_data\", raw_data_path, store=store)\n</code></pre> Step 2: Clean data<pre><code># Clean and transform the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE cleaned_data AS\n    SELECT \n        id,\n        TRIM(name) as name,\n        UPPER(category) as category,\n        CASE WHEN value &lt; 0 THEN 0 ELSE value END as value\n    FROM raw_data\n    WHERE id IS NOT NULL\n\"\"\"\n)\n</code></pre> Step 3: Aggregate data<pre><code># Aggregate the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE summary_data AS\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value\n    FROM cleaned_data\n    GROUP BY category\n\"\"\"\n)\n</code></pre> Step 4: Export results<pre><code>from cosmotech.coal.store.native_python import convert_table_as_pylist\nfrom cosmotech.coal.store.csv import convert_store_table_to_csv\nimport pathlib\n\n# Export to Python list\nsummary_data = convert_table_as_pylist(\"summary_data\", store=store)\nprint(summary_data)\n\n# Save to CSV for reporting\noutput_path = pathlib.Path(\"path/to/output/summary.csv\")\nconvert_store_table_to_csv(\"summary_data\", output_path, store=store)\n</code></pre>"},{"location":"tutorials/datastore/#best-practices-and-tips","title":"Best practices and tips","text":"<p>Store initialization</p> <ul> <li>Use <code>reset=True</code> when you want to start with a fresh database</li> <li>Omit the reset parameter or set it to <code>False</code> when you want to maintain data between runs</li> <li>Specify a custom location with the <code>store_location</code> parameter if needed</li> </ul> Store initialization options<pre><code># Fresh store each time\nstore = Store(reset=True)\n\n# Persistent store at default location\nstore = Store()\n\n# Persistent store at custom location\nimport pathlib\n\ncustom_path = pathlib.Path(\"/path/to/custom/location\")\nstore = Store(store_location=custom_path)\n</code></pre> <p>Table management</p> <ul> <li>Use descriptive table names that reflect the data content</li> <li>Check if tables exist before attempting operations</li> <li>List available tables to explore the database</li> </ul> Table management<pre><code># Check if a table exists\nif store.table_exists(\"customers\"):\n    # Do something with the table\n    pass\n\n# List all tables\nfor table_name in store.list_tables():\n    print(f\"Table: {table_name}\")\n    # Get schema information\n    schema = store.get_table_schema(table_name)\n    print(f\"Schema: {schema}\")\n</code></pre> <p>Performance considerations</p> <ul> <li>For large datasets, consider chunking data when loading</li> <li>Use SQL to filter data early rather than loading everything into memory</li> <li>Index frequently queried columns for better performance</li> </ul> Handling large datasets<pre><code># Example of chunking data load\nchunk_size = 10000\nfor i in range(0, len(large_dataset), chunk_size):\n    chunk = large_dataset[i : i + chunk_size]\n    store_pylist(f\"data_chunk_{i//chunk_size}\", chunk, store=store)\n\n# Combine chunks with SQL\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE combined_data AS\n    SELECT * FROM data_chunk_0\n    UNION ALL\n    SELECT * FROM data_chunk_1\n    -- Add more chunks as needed\n\"\"\"\n)\n</code></pre>"},{"location":"tutorials/datastore/#integration-with-cosmotech-ecosystem","title":"Integration with CosmoTech ecosystem","text":"<p>The datastore is designed to work seamlessly with other components of the CosmoTech Acceleration Library:</p> <ul> <li>Data loading: Load data from various sources into the datastore</li> <li>Runner management: Store runner parameters and results</li> <li>API integration: Exchange data with CosmoTech APIs</li> <li>Reporting: Generate reports and visualizations from stored data</li> </ul> <p>This integration makes the datastore a central component in CosmoTech-based data processing workflows.</p>"},{"location":"tutorials/datastore/#conclusion","title":"Conclusion","text":"<p>The datastore provides a powerful, flexible foundation for data management in your CosmoTech applications. By leveraging its capabilities, you can:</p> <ul> <li>Simplify data handling across different formats</li> <li>Build robust data processing pipelines</li> <li>Perform complex data transformations and analyses</li> <li>Maintain data persistence between application runs</li> <li>Integrate seamlessly with other CosmoTech components</li> </ul> <p>Whether you're working with small datasets or large-scale data processing tasks, the datastore offers the tools you need to manage your data effectively.</p>"}]}