errors:
  validation:
    not_csv_file: "'{file_path}' is not a csv file"
    invalid_nodes_relations: "'{file_path}' does not contains valid nodes or relations"
    invalid_truth_value: '"{string} is not a recognized truth value'
    node_requirements: "Node files must have an '{id_column}' column"
    relationship_requirements: "Relationship files must have '{source_column}' and '{target_column}' columns, or '{id_column}'"
  environment:
    no_env_vars: "No set of environment variables found for a valid Cosmo Tech API connection"
    no_valid_connection: "No valid connection available to the Cosmo Tech API"
    missing_env_var: "Missing the following environment variable: {envvar}"
  file_system:
    file_not_found: "{source_folder} does not exists"
    file_exists: "File {csv_path} already exists"
    not_directory: "{target_dir} is a file and not a directory"
    file_not_exists: '"{file_path}" does not exists'
    not_single_file: '"{file_path}" is not a single file'
  data:
    no_table: "No table with name {table_name} exists"
    parameter_not_exists: "Parameter {parameter_name} does not exists"
    invalid_output_type: "{output_type} is not a valid type of output"
    no_workspace_files: "No workspace file were found with filter {file_prefix} in workspace {workspace_id}"
  workspace:
    not_found: "Workspace {workspace_id} was not found in Organization {organization_id}"
    target_is_folder: "Target {target_dir} is a folder"
    loading_file: "Loading file {file_name}"
    file_loaded: "File {file_path} loaded"
    sending_to_api: "Sending file to API"
    file_sent: "File sent to API"
  solution:
    invalid_file: "{file} is not a `.yaml` or `.json` file"

solution:
  loaded: "Loaded {path}"
  api_configured: "Configuration to the api set"
  loading_workspace: "Loading Workspace information to get Solution ID"

web:
  failed_open: "Failed to open: {url}"
  opened: "Opened {url} in your navigator"

logs:
  api:
    solution_debug: "Solution: {solution}"
  run_data:
    sending_to_table: "Sending data to table {table_name}"
  workspace:
    target_is_folder: "Target {target_dir} is a folder"
    loading_file: "Loading file {file_name}"
    file_loaded: "File {file} loaded"
    sending_to_api: "Sending file to API"
    file_sent: "File sent to API"
  connection:
    existing_sets: "Existing sets are:"
    azure_connection: "  Azure Entra Connection : {keys}"
    api_key_connection: "  Cosmo Tech API Key : {keys}"
    keycloak_connection: "  Keycloak connection : {keys}"
    found_keycloak: "Found Keycloack connection info"
    found_cert_authority: "Found Certificate Authority override for IDP connection, using it."
    found_api_key: "Found Api Key connection info"
    found_azure: "Found Azure Entra connection info"
    found_valid: "Found valid connection of type: {type}"
  data_transfer:
    sending_table: "Sending table {table_name} as {output_type}"
    sending_data: "  Sending {size} bytes of data"
    table_empty: "Table {table_name} is empty (skipping)"
    rows_inserted: "Inserted {rows} rows in table {table_name}"
    file_sent: "Sending {file_path} as {uploaded_name}"
  ingestion:
    creating_table: "Create table query: {query}"
    table_created: "Table {table} created successfully"
    table_creation_failed: "Issue creating table {table}"
    ingesting: "Ingesting {table}"
    waiting_results: "Waiting for ingestion results, retry in {duration}s ({count}/{limit})"
    max_retry: "Max number of retry, stop waiting"
    status: "Checking ingestion status"
    status_report: "{table} - {status}"
    no_wait: "No wait for ingestion result"
    exceptions: "Exceptions: {exceptions}"
  progress:
    loading_file: "Loading {file_name} from the API"
    file_loaded: "{file} successfully loaded from the API"
    operation_timing: "{operation} took {time:0.3}s"
  runner:
    starting_download: "Starting the Run data download"
    no_parameters: "no parameters found in the runner"
    loaded_data: "Loaded run data"
    loading_data: "Loading data from {source}"
    parameter_debug: "  - {param_id:<{max_name_size}} {var_type:<{max_type_size}} \"{value}\"{inherited}"
    not_single_dataset: "{runner_id} is not tied to a single dataset but {count}"
    dataset_state: "Dataset {dataset_id} is in state {status}"
    downloading_datasets: "Downloading {count} datasets"
    runner_info: "Runner info: {info}"
    dataset_info: "Dataset info: {info}"
    writing_parameters: "Writing parameters to files"
    generating_file: "Generating {file}"
    dataset_debug: "  - {folder} ({id})"
    no_dataset_write: "No dataset write asked, skipping"
    no_parameters_write: "No parameters write asked, skipping"
  database:
    creating_table: "creating table {table}"
    updating_metadata: "adding/updating runner metadata"
    metadata_updated: "Runner metadata table has been updated"
    sending_data: "Sending data to table {table}"
    no_rows: "  - No rows : skipping"
    column_list: "  - Column list: {columns}"
    row_count: "  - Sending {count} rows"
    query_results: "Query returned {count} rows"
    saved_results: "Results saved as {file}"
    no_results: "No results returned by the query"
    store_empty: "Data store is empty"
    store_tables: "Data store contains the following tables"
    table_entry: "  - {table}"
    table_schema: "Schema: {schema}"
    store_reset: "Data store in {folder} got reset"
    rows_fetched: "Rows fetched in {table} table: {count} in {time} seconds"
    tables_to_fetch: "Tables to fetched: {tables}"
    full_dataset: "Full dataset fetched and wrote in {time} seconds"
  storage:
    deleting_objects: "Deleting {objects}"
    no_objects: "No objects to delete"
    downloading: "Downloading {path} to {output}"
    sending_file: "Sending {file} as {name}"
    found_file: "Found {file}, storing it"
    clearing_content: "Clearing all dataset content"
    sending_content: "Sending content of '{file}'"
    row_batch: "Found row count of {count}, sending now"
    import_errors: "Found {count} errors while importing: "
    error_detail: "Error: {error}"
    all_data_sent: "Sent all data found"
    writing_lines: "Writing {count} lines in {file}"
    all_csv_written: "All CSV are written"
  orchestrator:
    searching_template: "Searching {template} in the solution"
    template_not_found: "Run template {template} was not found."
    generating_json: "Found {template} in the solution generating json file"
    no_parameters: "No parameters to write for {template}"
    creating_folders: "Creating folders for dataset parameters"
    folder_created: "- {folder}"
    step_found: "- {step} step found"
    steps_summary: "{count} step{plural} found, writing json file"
    loading_solution: "Loading Workspace information to get Solution ID"
    querying_handler: "Querying Handler {handler} for {template}"
    handler_not_found: "Handler {handler} was not found for Run Template {template} in Solution {solution}"
    extracting_handler: "Extracting handler to {path}"
    handler_not_zip: "Handler {handler} is not a zip file"
    run_issues: "Issues were met during run, please check the previous logs"
    error_details: "Error details: {details}"
  postgresql:
    getting_schema: "Getting schema for table {postgres_schema}.{target_table_name}"
    table_not_found: "Table {postgres_schema}.{target_table_name} not found"
    schema_adaptation_start: "Starting schema adaptation for table with {rows} rows"
    original_schema: "Original schema: {schema}"
    target_schema: "Target schema: {schema}"
    casting_column: "Attempting to cast column '{field_name}' from {original_type} to {target_type}"
    cast_failed: "Failed to cast column '{field_name}' from {original_type} to {target_type}. Filling with nulls. Error: {error}"
    adding_missing_column: "Adding missing column '{field_name}' with null values"
    dropping_columns: "Dropping extra columns not in target schema: {columns}"
    adaptation_summary: "Schema adaptation summary:"
    added_columns: "- Added columns (filled with nulls): {columns}"
    dropped_columns: "- Dropped columns: {columns}"
    successful_conversions: "- Successful type conversions: {conversions}"
    failed_conversions: "- Failed conversions (filled with nulls): {conversions}"
    final_schema: "Final adapted table schema: {schema}"
    preparing_send: "Preparing to send data to PostgreSQL table '{postgres_schema}.{target_table_name}'"
    input_rows: "Input table has {rows} rows"
    found_existing_table: "Found existing table with schema: {schema}"
    adapting_data: "Adapting incoming data to match existing schema"
    replace_mode: "Replace mode enabled - skipping schema adaptation"
    no_existing_table: "No existing table found - will create new table"
    connecting: "Connecting to PostgreSQL database"
    ingesting_data: "Ingesting data with mode: {mode}"
    ingestion_success: "Successfully ingested {rows} rows"
    runner:
      creating_table: "Creating table {table_name}"
      metadata: "Metadata: {metadata}"
      metadata_updated: "Metadata updated"
  postgreql:
    runner:
      creating_table: "Creating table {schema_table}"
      metadata: "Metadata: {metadata}"
      metadata_updated: "Metadata updated"
  adx:
    creating_kusto_client: "Creating Kusto client for cluster: {cluster_url}"
    creating_ingest_client: "Creating ingest client for URL: {ingest_url}"
    using_app_auth: "Using Azure AD application authentication"
    using_cli_auth: "Using Azure CLI authentication"
    generating_urls: "Generating URLs for cluster {cluster_name} in region {cluster_region}"
    running_query: "Running query on database {database}: {query}"
    running_command: "Running command on database {database}: {query}"
    query_complete: "Query complete, returned {rows} rows"
    command_complete: "Command execution complete"
    ingesting_dataframe: "Ingesting dataframe with {rows} rows to table {table_name}"
    ingestion_queued: "Ingestion queued with source ID: {source_id}"
    sending_to_adx: "Sending {items} items to ADX table {table_name}"
    empty_dict_list: "Empty dictionary list provided, nothing to send"
    table_creation_failed: "Error creating table {table_name}"
    checking_status: "Checking ingestion status for {count} operations"
    status_messages: "Found {success} success messages and {failure} failure messages"
    status_found: "Found status for {source_id}: {status}"
    ingestion_timeout: "Ingestion operation {source_id} timed out"
    clear_queues_no_confirmation: "Clear queues operation requires confirmation=True"
    clearing_queues: "DANGER: Clearing all ingestion status queues"
    queues_cleared: "All ingestion status queues have been cleared"
    waiting_ingestion: "Waiting for ingestion of data to finish"
    ingestion_failed: "Ingestion {ingestion_id} failed for table {table}"
    ingestion_completed: "All data ingestion attempts completed"
    failures_detected: "Failures detected during ingestion - dropping data with tag: {operation_tag}"
    checking_table_exists: "Checking if table exists"
    creating_nonexistent_table: "Table does not exist, creating it"
    dropping_data_by_tag: "Dropping data with tag: {tag}"
    drop_completed: "Drop by tag operation completed"
    drop_error: "Error during drop by tag operation: {error}"
    drop_details: "Drop by tag details"
    checking_table: "Checking if table {table_name} exists in database {database}"
    table_exists: "Table {table_name} exists"
    table_not_exists: "Table {table_name} does not exist"
    creating_table: "Creating table {table_name} in database {database}"
    create_query: "Create table query: {query}"
    table_created: "Table {table_name} created successfully"
    table_creation_error: "Error creating table {table_name}: {error}"
    mapping_type: "Mapping type for key {key} with value type {value_type}"
    runner:
      content_debug: "CSV content: {content}"
    store:
      sending_data: "Sending data to the table {table_name}"
      listing_tables: "Listing tables"
      working_on_table: "Working on table: {table_name}"
      table_empty: "Table {table_name} has no rows - skipping it"
      starting_ingestion: "Starting ingestion operation with tag: {operation_tag}"
      loading_datastore: "Loading datastore"
      data_sent: "Store data was sent for ADX ingestion"
      ingestion_error: "Error during ingestion process"
      dropping_data: "Dropping data with tag: {operation_tag}"
    auth:
      initializing_clients: "Initializing clients"
    utils:
      empty_column: "Column {column_name} has no content, defaulting it to string"
  dataset:
    # General
    download_started: "Starting download of {dataset_type} dataset"
    download_completed: "Successfully downloaded {dataset_type} dataset"
    operation_timing: "{operation} took {time} seconds"
    dataset_downloading: "Downloading dataset (organization: {organization_id}, dataset: {dataset_id})"
    dataset_info_retrieved: "Retrieved dataset info: {dataset_name} ({dataset_id})"
    dataset_type_detected: "Detected dataset type: {type}"
    parallel_download: "Downloading {count} datasets in parallel"
    sequential_download: "Downloading {count} datasets sequentially"

    # Processing
    processing_graph_data: "Processing graph data with {nodes_count} nodes and {relationships_count} relationships (restore_names={restore_names})"
    entity_count: "Found {count} entities of type {entity_type}"
    extracting_headers: "Extracting headers from {rows} rows"
    headers_extracted: "Extracted {count} fields: {fields}"

    # File operations
    converting_to_files: "Converting {dataset_type} dataset '{dataset_name}' to files"
    created_temp_folder: "Created temporary folder: {folder}"
    using_folder: "Using folder: {folder}"
    converting_graph_data: "Converting graph data with {entity_types} entity types to folder: {folder}"
    converting_file_data: "Converting {file_count} files of type {file_type} to folder: {folder}"
    skipping_empty_entity: "Skipping empty entity type: {entity_type}"
    writing_csv: "Writing CSV file with {count} records: {file_name}"
    writing_file: "Writing file: {file_name} (type: {file_type})"
    file_written: "File written: {file_path}"
    files_created: "Created {count} files in folder: {folder}"

    # ADT specific
    adt_connecting: "Connecting to ADT instance at {url}"
    adt_no_credentials: "No credentials available for ADT connection"
    adt_querying_twins: "Querying digital twins"
    adt_twins_found: "Found {count} digital twins"
    adt_querying_relations: "Querying relationships"
    adt_relations_found: "Found {count} relationships"

    # TwinGraph specific
    twingraph_downloading: "Downloading TwinGraph dataset (organization: {organization_id}, dataset: {dataset_id})"
    twingraph_querying_nodes: "Querying TwinGraph nodes for dataset {dataset_id}"
    twingraph_nodes_found: "Found {count} nodes in TwinGraph"
    twingraph_querying_edges: "Querying TwinGraph edges for dataset {dataset_id}"
    twingraph_edges_found: "Found {count} edges in TwinGraph"

    # Legacy TwinGraph specific
    legacy_twingraph_downloading: "Downloading legacy TwinGraph dataset (organization: {organization_id}, cache: {cache_name})"
    legacy_twingraph_querying_nodes: "Querying legacy TwinGraph nodes for cache {cache_name}"
    legacy_twingraph_nodes_found: "Found {count} nodes in legacy TwinGraph"
    legacy_twingraph_querying_relations: "Querying legacy TwinGraph relationships for cache {cache_name}"
    legacy_twingraph_relations_found: "Found {count} relationships in legacy TwinGraph"

    # File specific
    file_downloading: "Downloading file dataset (organization: {organization_id}, workspace: {workspace_id}, file: {file_name})"
    listing_workspace_files: "Listing workspace files"
    workspace_files_found: "Found {count} workspace files"
    no_files_found: "No files found matching: {file_name}"
    downloading_file: "Downloading file: {file_name}"
    file_downloaded: "Downloaded file: {file_name} to {path}"

    # File processing
    processing_excel: "Processing Excel file: {file_name}"
    sheet_processed: "Processed sheet {sheet_name} with {rows} rows"
    processing_csv: "Processing CSV file: {file_name}"
    csv_processed: "Processed CSV file {file_name} with {rows} rows"
    processing_json: "Processing JSON file: {file_name}"
    json_processed: "Processed JSON file {file_name} with {items} items"
    processing_text: "Processing text file: {file_name}"
    text_processed: "Processed text file {file_name} with {lines} lines"
