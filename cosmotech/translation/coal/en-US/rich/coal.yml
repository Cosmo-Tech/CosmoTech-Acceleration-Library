errors:
  validation:
    not_csv_file: "[important]'{file_path}'[/important] [red]not a csv file[/red]"
    invalid_nodes_relations: "[important]'{file_path}'[/important] [red]does not contains valid nodes or relations[/red]"
    invalid_truth_value: "[red]'{string}' is not a recognized truth value[/red]"
  environment:
    no_env_vars: "[red]No set of environment variables found for a valid Cosmo Tech API connection[/red]"
    no_valid_connection: "[red]No valid connection available to the Cosmo Tech API[/red]"
    missing_env_var: "[red]Missing the following environment variable:[/red] [important]{envvar}[/important]"
  file_system:
    file_not_found: "[red]{source_folder} does not exists[/red]"
    file_exists: "[yellow]File {csv_path} already exists[/yellow]"
    not_directory: "[red]{target_dir} is a file and not a directory[/red]"
    file_not_exists: "[red]'{file_path}' does not exists[/red]"
    not_single_file: "[red]'{file_path}' is not a single file[/red]"
  data:
    no_table: "[red]No table with name[/red] [important]{table_name}[/important] [red]exists[/red]"
    parameter_not_exists: "[red]Parameter[/red] [important]{parameter_name}[/important] [red]does not exists[/red]"
    invalid_output_type: "[red]{output_type} is not a valid type of output[/red]"
    no_workspace_files: "[red]No workspace file were found with filter[/red] [important]{file_prefix}[/important] [red]in workspace[/red] [important]{workspace_id}[/important]"
  workspace:
    not_found: "[red]Workspace[/red] [important]{workspace_id}[/important] [red]was not found in Organization[/red] [important]{organization_id}[/important]"

  solution:
    loaded: "[green]Loaded[/green] [important]{path}[/important]"
    api_configured: "[green]Configuration to the api set[/green]"
    loading_workspace: "[blue]Loading Workspace information to get Solution ID[/blue]"
  errors:
    solution:
      invalid_file: "[red]{file} is not a `.yaml` or `.json` file[/red]"
    environment:
      missing_var: "[red]Missing the following environment variable:[/red] [important]{envvar}[/important]"

web:
  failed_open: "[red]Failed to open:[/red] [important]{url}[/important]"
  opened: "[green]Opened[/green] [important]{url}[/important] [green]in your navigator[/green]"

logs:
  connection:
    existing_sets: "[blue]Existing sets are:[/blue]"
    azure_connection: "  [blue]Azure Entra Connection :[/blue] [important]{keys}[/important]"
    api_key_connection: "  [blue]Cosmo Tech API Key :[/blue] [important]{keys}[/important]"
    keycloak_connection: "  [blue]Keycloak connection :[/blue] [important]{keys}[/important]"
    found_keycloak: "[green]Found Keycloack connection info[/green]"
    found_cert_authority: "[green]Found Certificate Authority override for IDP connection, using it.[/green]"
    found_api_key: "[green]Found Api Key connection info[/green]"
    found_azure: "[green]Found Azure Entra connection info[/green]"
    found_valid: "[green]Found valid connection of type:[/green] [important]{type}[/important]"
  data_transfer:
    sending_table: "[blue]Sending table[/blue] [important]{table_name}[/important] [blue]as[/blue] [important]{output_type}[/important]"
    sending_data: "  [blue]Sending[/blue] [important]{size}[/important] [blue]bytes of data[/blue]"
    table_empty: "[yellow]Table[/yellow] [important]{table_name}[/important] [yellow]is empty (skipping)[/yellow]"
    rows_inserted: "[green]Inserted[/green] [important]{rows}[/important] [green]rows in table[/green] [important]{table_name}[/important]"
    file_sent: "[blue]Sending[/blue] [important]{file_path}[/important] [blue]as[/blue] [important]{uploaded_name}[/important]"
  ingestion:
    creating_table: "[blue]Create table query:[/blue] [important]{query}[/important]"
    table_created: "[green]Table[/green] [important]{table}[/important] [green]created successfully[/green]"
    table_creation_failed: "[red]Issue creating table[/red] [important]{table}[/important]"
    ingesting: "[blue]Ingesting[/blue] [important]{table}[/important]"
    waiting_results: "[blue]Waiting for ingestion results, retry in[/blue] [important]{duration}s[/important] [blue]([/blue][important]{count}[/important][blue]/[/blue][important]{limit}[/important][blue])[/blue]"
    max_retry: "[red]Max number of retry, stop waiting[/red]"
    status_report: "[important]{table}[/important] - [important]{status}[/important]"
    no_wait: "[yellow]No wait for ingestion result[/yellow]"
  progress:
    loading_file: "[blue]Loading[/blue] [important]{file_name}[/important] [blue]from the API[/blue]"
    file_loaded: "[important]{file}[/important] [green]successfully loaded from the API[/green]"
    operation_timing: "[important]{operation}[/important] [blue]took[/blue] [important]{time:0.3}s[/important]"

  runner:
    starting_download: "[blue]Starting the Run data download[/blue]"
    no_parameters: "[yellow]no parameters found in the runner[/yellow]"
    loaded_data: "[green]Loaded run data[/green]"
    parameter_debug: "  - [important]{param_id:<{max_name_size}}[/important] [important]{var_type:<{max_type_size}}[/important] '[important]{value}[/important]'[important]{inherited}[/important]"
    not_single_dataset: "[important]{runner_id}[/important] [red]is not tied to a single dataset but[/red] [important]{count}[/important]"
    dataset_state: "[blue]Dataset[/blue] [important]{dataset_id}[/important] [blue]is in state[/blue] [important]{status}[/important]"
    downloading_datasets: "[blue]Downloading[/blue] [important]{count}[/important] [blue]datasets[/blue]"
    writing_parameters: "[blue]Writing parameters to files[/blue]"
    generating_file: "[blue]Generating[/blue] [important]{file}[/important]"
    dataset_debug: "  - [important]{folder}[/important] ([important]{id}[/important])"
    no_dataset_write: "[yellow]No dataset write asked, skipping[/yellow]"
    no_parameters_write: "[yellow]No parameters write asked, skipping[/yellow]"

  database:
    creating_table: "[blue]creating table[/blue] [important]{table}[/important]"
    updating_metadata: "[blue]adding/updating runner metadata[/blue]"
    metadata_updated: "[green]Runner metadata table has been updated[/green]"
    sending_data: "[blue]Sending data to table[/blue] [important]{table}[/important]"
    no_rows: "  - [yellow]No rows : skipping[/yellow]"
    column_list: "  - [blue]Column list:[/blue] [important]{columns}[/important]"
    row_count: "  - [blue]Sending[/blue] [important]{count}[/important] [blue]rows[/blue]"
    query_results: "[blue]Query returned[/blue] [important]{count}[/important] [blue]rows[/blue]"
    saved_results: "[green]Results saved as[/green] [important]{file}[/important]"
    no_results: "[yellow]No results returned by the query[/yellow]"
    store_empty: "[yellow]Data store is empty[/yellow]"
    store_tables: "[blue]Data store contains the following tables[/blue]"
    table_entry: "  - [important]{table}[/important]"
    store_reset: "[green]Data store in[/green] [important]{folder}[/important] [green]got reset[/green]"
    rows_fetched: "[blue]Rows fetched in[/blue] [important]{table}[/important] [blue]table:[/blue] [important]{count}[/important] [blue]in[/blue] [important]{time}[/important] [blue]seconds[/blue]"
    tables_to_fetch: "[blue]Tables to fetched:[/blue] [important]{tables}[/important]"
    full_dataset: "[green]Full dataset fetched and wrote in[/green] [important]{time}[/important] [green]seconds[/green]"

  storage:
    deleting_objects: "[blue]Deleting[/blue] [important]{objects}[/important]"
    no_objects: "[yellow]No objects to delete[/yellow]"
    downloading: "[blue]Downloading[/blue] [important]{path}[/important] [blue]to[/blue] [important]{output}[/important]"
    sending_file: "[blue]Sending[/blue] [important]{file}[/important] [blue]as[/blue] [important]{name}[/important]"
    found_file: "[green]Found[/green] [important]{file}[/important][green], storing it[/green]"
    clearing_content: "[blue]Clearing all dataset content[/blue]"
    sending_content: "[blue]Sending content of[/blue] '[important]{file}[/important]'"
    row_batch: "[blue]Found row count of[/blue] [important]{count}[/important][blue], sending now[/blue]"
    import_errors: "[red]Found[/red] [important]{count}[/important] [red]errors while importing:[/red] "
    all_data_sent: "[green]Sent all data found[/green]"
    writing_lines: "[blue]Writing[/blue] [important]{count}[/important] [blue]lines in[/blue] [important]{file}[/important]"
    all_csv_written: "[green]All CSV are written[/green]"

  orchestrator:
    searching_template: "[blue]Searching[/blue] [important]{template}[/important] [blue]in the solution[/blue]"
    template_not_found: "[red]Run template[/red] [important]{template}[/important] [red]was not found.[/red]"
    generating_json: "[green]Found[/green] [important]{template}[/important] [green]in the solution generating json file[/green]"
    no_parameters: "[yellow]No parameters to write for[/yellow] [important]{template}[/important]"
    creating_folders: "[blue]Creating folders for dataset parameters[/blue]"
    folder_created: "- [important]{folder}[/important]"
    step_found: "- [important]{step}[/important] [green]step found[/green]"
    steps_summary: "[important]{count}[/important] [blue]step{plural} found, writing json file[/blue]"
    loading_solution: "[blue]Loading Workspace information to get Solution ID[/blue]"
    querying_handler: "[blue]Querying Handler[/blue] [important]{handler}[/important] [blue]for[/blue] [important]{template}[/important]"
    handler_not_found: "[red]Handler[/red] [important]{handler}[/important] [red]was not found for Run Template[/red] [important]{template}[/important] [red]in Solution[/red] [important]{solution}[/important]"
    extracting_handler: "[blue]Extracting handler to[/blue] [important]{path}[/important]"
    handler_not_zip: "[red]Handler[/red] [important]{handler}[/important] [red]is not a zip file[/red]"
    run_issues: "[red]Issues were met during run, please check the previous logs[/red]"

  postgresql:
    getting_schema: "[blue]Getting schema for table[/blue] [important]{postgres_schema}[/important].[important]{target_table_name}[/important]"
    table_not_found: "[yellow]Table[/yellow] [important]{postgres_schema}[/important].[important]{target_table_name}[/important] [yellow]not found[/yellow]"
    schema_adaptation_start: "[blue]Starting schema adaptation for table with[/blue] [important]{rows}[/important] [blue]rows[/blue]"
    original_schema: "[blue]Original schema:[/blue] [important]{schema}[/important]"
    target_schema: "[blue]Target schema:[/blue] [important]{schema}[/important]"
    casting_column: "[blue]Attempting to cast column[/blue] '[important]{field_name}[/important]' [blue]from[/blue] [important]{original_type}[/important] [blue]to[/blue] [important]{target_type}[/important]"
    cast_failed: "[red]Failed to cast column[/red] '[important]{field_name}[/important]' [red]from[/red] [important]{original_type}[/important] [red]to[/red] [important]{target_type}[/important][red]. Filling with nulls. Error:[/red] [important]{error}[/important]"
    adding_missing_column: "[blue]Adding missing column[/blue] '[important]{field_name}[/important]' [blue]with null values[/blue]"
    dropping_columns: "[blue]Dropping extra columns not in target schema:[/blue] [important]{columns}[/important]"
    adaptation_summary: "[blue]Schema adaptation summary:[/blue]"
    added_columns: "[blue]- Added columns (filled with nulls):[/blue] [important]{columns}[/important]"
    dropped_columns: "[blue]- Dropped columns:[/blue] [important]{columns}[/important]"
    successful_conversions: "[blue]- Successful type conversions:[/blue] [important]{conversions}[/important]"
    failed_conversions: "[blue]- Failed conversions (filled with nulls):[/blue] [important]{conversions}[/important]"
    final_schema: "[blue]Final adapted table schema:[/blue] [important]{schema}[/important]"
    preparing_send: "[blue]Preparing to send data to PostgreSQL table[/blue] '[important]{postgres_schema}[/important].[important]{target_table_name}[/important]'"
    input_rows: "[blue]Input table has[/blue] [important]{rows}[/important] [blue]rows[/blue]"
    found_existing_table: "[blue]Found existing table with schema:[/blue] [important]{schema}[/important]"
    adapting_data: "[blue]Adapting incoming data to match existing schema[/blue]"
    replace_mode: "[blue]Replace mode enabled - skipping schema adaptation[/blue]"
    no_existing_table: "[blue]No existing table found - will create new table[/blue]"
    connecting: "[blue]Connecting to PostgreSQL database[/blue]"
    ingesting_data: "[blue]Ingesting data with mode:[/blue] [important]{mode}[/important]"
    ingestion_success: "[green]Successfully ingested[/green] [important]{rows}[/important] [green]rows[/green]"
