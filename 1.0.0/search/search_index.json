{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cosmotech Acceleration library","text":"<p>Acceleration library for CosmoTech cloud-based solution development.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The CosmoTech Acceleration Library (CoAL) provides a comprehensive set of tools and utilities to accelerate the development of solutions based on the CosmoTech platform. It offers a unified interface for interacting with CosmoTech APIs, managing data, and integrating with various cloud services.</p>"},{"location":"#main-components","title":"Main Components","text":""},{"location":"#csm-data","title":"csm-data","text":"<p><code>csm-data</code> is a powerful CLI tool designed to help CosmoTech solution modelers and integrators interact with multiple systems. It provides ready-to-use commands to send and retrieve data from various systems where a CosmoTech API could be integrated.</p> <pre><code># Get help on available commands\ncsm-data --help\n\n# Get help on specific command groups\ncsm-data api --help\n</code></pre>"},{"location":"#datastore","title":"datastore","text":"<p>The datastore provides a way to maintain local data during simulations and comes with <code>csm-data</code> commands to easily send those data to target systems. It offers:</p> <ul> <li>Format flexibility (Python dictionaries, CSV files, Pandas DataFrames, PyArrow Tables)</li> <li>Persistent storage in SQLite</li> <li>SQL query capabilities</li> <li>Simplified data pipeline management</li> </ul> <pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\n# Initialize and reset the data store\nmy_datastore = Store(reset=True)\n\n# Create and store data\nmy_data = [{\"foo\": \"bar\"}, {\"foo\": \"barbar\"}, {\"foo\": \"world\"}, {\"foo\": \"bar\"}]\nstore_pylist(\"my_data\", my_data)\n\n# Query the data\nresults = my_datastore.execute_query(\"SELECT foo, count(*) as line_count FROM my_data GROUP BY foo\").to_pylist()\nprint(results)\n# &gt; [{'foo': 'bar', 'line_count': 2}, {'foo': 'barbar', 'line_count': 1}, {'foo': 'world', 'line_count': 1}]\n</code></pre>"},{"location":"#cosmotech-api-integration","title":"CosmoTech API Integration","text":"<p>CoAL provides comprehensive tools for interacting with the CosmoTech API, allowing you to:</p> <ul> <li>Authenticate with different identity providers (API Key, Azure Entra, Keycloak)</li> <li>Manage workspaces and files</li> <li>Work with the Twin Data Layer for graph data</li> <li>Handle runners and runs</li> <li>Process and transform data</li> <li>Build end-to-end workflows</li> </ul> <pre><code>import os\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Get the API client\napi_client, connection_type = get_api_client()\nprint(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\nfrom cosmotech_api.api.organization_api import OrganizationApi\norg_api = OrganizationApi(api_client)\n\n# List organizations\norganizations = org_api.find_all_organizations()\nfor org in organizations:\n    print(f\"Organization: {org.name} (ID: {org.id})\")\n\n# Don't forget to close the client when done\napi_client.close()\n</code></pre>"},{"location":"#other-components","title":"Other Components","text":"<ul> <li>coal: Core library with modules for API interaction, data management, etc.</li> <li>csm_data: CLI tool for data management and integration with various systems</li> <li>orchestrator_plugins: Plugins that integrate with external orchestration systems</li> <li>translation: Internationalization support for multiple languages</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install cosmotech-acceleration-library\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Check out the tutorials directory for comprehensive examples of how to use the library:</p> <ul> <li>CosmoTech API Integration</li> <li>Data Store Usage</li> <li>csm-data CLI</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#cloud-service-integration","title":"Cloud Service Integration","text":"<p>CoAL provides built-in support for various cloud services:</p> <ul> <li>Azure: Azure Data Explorer (ADX), Azure Storage, Azure Functions</li> <li>AWS: S3 buckets, and more</li> <li>Database Systems: PostgreSQL, SingleStore, and others</li> </ul>"},{"location":"#data-management","title":"Data Management","text":"<ul> <li>Load and transform data from various sources</li> <li>Store and query data locally</li> <li>Export data to different formats and destinations</li> <li>Manage datasets in the CosmoTech platform</li> </ul>"},{"location":"#orchestration-integration","title":"Orchestration Integration","text":"<ul> <li>Provides plugins that integrate with external orchestration systems</li> <li>Supports data transfer between orchestration steps</li> <li>Offers utilities for handling parameters and configurations</li> <li>Enables seamless integration with the CosmoTech platform during orchestrated workflows</li> </ul>"},{"location":"#documentation-and-tutorials","title":"Documentation and Tutorials","text":"<p>Comprehensive documentation is available at https://cosmo-tech.github.io/CosmoTech-Acceleration-Library/</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>CosmoTech API: Learn how to interact with the CosmoTech API directly: authentication, workspaces, Twin Data Layer, and more.</li> <li>Data Store: The datastore is your friend to keep data between orchestration steps. It comes with multiple ways to interact with it.</li> <li>csm-data: Make full use of <code>csm-data</code> commands to connect to services during your orchestration runs.</li> </ul>"},{"location":"#testing-and-code-coverage","title":"Testing and Code Coverage","text":"<p>The CosmoTech Acceleration Library maintains a comprehensive test suite to ensure reliability and stability. We use pytest for testing and pytest-cov for coverage reporting.</p>"},{"location":"#running-tests","title":"Running Tests","text":"<p>To run the test suite:</p> <pre><code># Install test dependencies\npip install -e \".[test]\"\n\n# Run tests with coverage reporting\npytest tests/unit/coal/ --cov=cosmotech.coal --cov-report=term-missing --cov-report=html\n</code></pre>"},{"location":"#coverage-reports","title":"Coverage Reports","text":"<p>After running tests with coverage, you can view detailed HTML reports:</p> <pre><code># Open the HTML coverage report\nopen coverage_html_report/index.html\n</code></pre> <p></p> <p>We maintain high test coverage to ensure code quality and reliability. All pull requests are expected to maintain or improve the current coverage levels.</p>"},{"location":"#test-generation-tools","title":"Test Generation Tools","text":"<p>To help maintain test coverage, we provide tools to identify untested functions and generate test files:</p> <pre><code># Find functions without tests\npython find_untested_functions.py\n\n# Generate test files for a specific module\npython generate_test_files.py --module cosmotech/coal/module/file.py\n\n# Generate test files for all untested functions\npython generate_test_files.py --all\n</code></pre> <p>These tools help ensure that every function has at least one test, which is a requirement for contributions to the project.</p>"},{"location":"#contact","title":"Contact","text":"<p>For support, feature requests, or contributions, please use the GitHub repository.</p>"},{"location":"dependencies/","title":"List of dependencies","text":"<p>Azure connection requirements </p> <p>Keycloak connection </p> <p>Modelops requirements </p> <p>Cosmotech specific requirements </p> <p>Commands requirements </p> <p>Orchestrator templates requirements </p> <p>Data store requirements </p> <p>CLI requirements </p> <p>Other requirements </p> <p>fix distutils missing from 3.12   Documentation generation   Extra requirements   Test requirements   Development requirements </p>"},{"location":"pull_request/","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, make sure you've completed all the necessary steps:</p> <ol> <li>Code Quality<ul> <li> Code follows the project's style guidelines (Black formatting)</li> <li> All linting checks pass</li> <li> Code is well-documented with docstrings</li> <li> Code is efficient and follows best practices</li> <li> No unnecessary dependencies are added</li> </ul> </li> <li>Testing<ul> <li> All unit tests pass</li> <li> Test coverage meets or exceeds 80%</li> <li> All functions have at least one test</li> <li> Edge cases and error conditions are tested</li> <li> Mocks are used for external services</li> </ul> </li> <li>Documentation<ul> <li> API documentation is updated</li> <li> Command help text is clear and comprehensive</li> <li> Translation strings are added for all user-facing text</li> <li> Usage examples are provided</li> <li> Any necessary tutorials are created or updated</li> </ul> </li> <li>Integration<ul> <li> New functionality integrates well with existing code</li> <li> No breaking changes to existing APIs</li> <li> Dependencies are properly specified in pyproject.toml</li> <li> Command is registered in the appropriate init.py file</li> </ul> </li> <li>Pull Request Description<ul> <li> Clear description of the changes</li> <li> Explanation of why the changes are needed</li> <li> Any potential issues or limitations</li> <li> References to related issues or discussions</li> </ul> </li> </ol>"},{"location":"csm-data/","title":"csm-data","text":"<p>Help command</p> <pre><code>&gt; csm-data --help\n\n Usage: csm-data [OPTIONS] COMMAND [ARGS]...                                    \n\n Cosmo Tech Data Interface                                                      \n Command toolkit providing quick implementation of data connections to use      \n inside the Cosmo Tech Platform                                                 \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --log-level    LVL  Either CRITICAL, ERROR, WARNING, INFO or DEBUG           \u2502\n\u2502                     ENV: LOG_LEVEL                                           \u2502\n\u2502 --version           Print version number and return.                         \u2502\n\u2502 --web-help          Open the web documentation                               \u2502\n\u2502 --help              Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 adx-send-data        csm_data.commands.storage.adx_send_data.description     \u2502\n\u2502 adx-send-runnerdata  Uses environment variables to send content of CSV files \u2502\n\u2502                      to ADX Requires a valid Azure connection either with:   \u2502\n\u2502                                                                              \u2502\n\u2502                       \u2022 The AZ cli command: az login                         \u2502\n\u2502                       \u2022 A triplet of env var AZURE_TENANT_ID,                \u2502\n\u2502                         AZURE_CLIENT_ID, AZURE_CLIENT_SECRET                 \u2502\n\u2502 api                  Cosmo Tech API helper command                           \u2502\n\u2502 az-storage-upload    Upload a folder to an Azure Storage Blob                \u2502\n\u2502 s3-bucket-delete     Delete S3 bucket content to a given folder              \u2502\n\u2502 s3-bucket-download   Download S3 bucket content to a given folder            \u2502\n\u2502 s3-bucket-upload     Upload a folder to a S3 Bucket                          \u2502\n\u2502 store                CoAL Data Store command group                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/adx-send-runnerdata/","title":"adx-send-runnerdata","text":"<p>Help command</p> <pre><code>&gt; csm-data adx-send-runnerdata --help\n\n Usage: csm-data adx-send-runnerdata [OPTIONS]                                  \n\n Uses environment variables to send content of CSV files to ADX Requires a      \n valid Azure connection either with:                                            \n\n  \u2022 The AZ cli command: az login                                                \n  \u2022 A triplet of env var AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET  \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --dataset-absolute-path            PATH  A local folder to store the main \u2502\n\u2502                                             dataset content                  \u2502\n\u2502                                             ENV: CSM_DATASET_ABSOLUTE_PATH   \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --parameters-absolute-path         PATH  A local folder to store the      \u2502\n\u2502                                             parameters content               \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_PARAMETERS_ABSOLUTE_PATH     \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --runner-id                        UUID  the Runner Id to add to records  \u2502\n\u2502                                             ENV: CSM_RUNNER_ID               \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --adx-uri                          URI   the ADX cluster path (URI info   \u2502\n\u2502                                             can be found into ADX cluster    \u2502\n\u2502                                             page)                            \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_RESOURCE_URI \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --adx-ingest-uri                   URI   The ADX cluster ingest path (URI \u2502\n\u2502                                             info can be found into ADX       \u2502\n\u2502                                             cluster page)                    \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_RESOURCE_IN\u2026 \u2502\n\u2502                                             [required]                       \u2502\n\u2502 *  --database-name                    NAME  The targeted database name       \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             AZURE_DATA_EXPLORER_DATABASE_NA\u2026 \u2502\n\u2502                                             [required]                       \u2502\n\u2502    --send-parameters/--no-send-pa\u2026          whether or not to send           \u2502\n\u2502                                             parameters (parameters path is   \u2502\n\u2502                                             mandatory then)                  \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_SEND_DATAWAREHOUSE_PARAMETE\u2026 \u2502\n\u2502                                             DEFAULT: no-send-parameters      \u2502\n\u2502    --send-datasets/--no-send-data\u2026          whether or not to send datasets  \u2502\n\u2502                                             (parameters path is mandatory    \u2502\n\u2502                                             then)                            \u2502\n\u2502                                             ENV:                             \u2502\n\u2502                                             CSM_SEND_DATAWAREHOUSE_DATASETS  \u2502\n\u2502                                             DEFAULT: no-send-datasets        \u2502\n\u2502    --wait/--no-wait                         Toggle waiting for the ingestion \u2502\n\u2502                                             results                          \u2502\n\u2502                                             ENV: WAIT_FOR_INGESTION          \u2502\n\u2502                                             DEFAULT: no-wait                 \u2502\n\u2502    --help                                   Show this message and exit.      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/az-storage-upload/","title":"az-storage-upload","text":"<p>Help command</p> <pre><code>&gt; csm-data az-storage-upload --help\n\n Usage: csm-data az-storage-upload [OPTIONS]                                    \n\n Upload a folder to an Azure Storage Blob                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder               PATH    The folder/file to upload to the    \u2502\n\u2502                                          target blob storage                 \u2502\n\u2502                                          ENV: CSM_DATASET_ABSOLUTE_PATH      \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --recursive/--no-recursive            Recursively send the content of     \u2502\n\u2502                                          every folder inside the starting    \u2502\n\u2502                                          folder to the blob storage          \u2502\n\u2502 *  --blob-name                   BUCKET  The blob name in the Azure Storage  \u2502\n\u2502                                          service to upload to                \u2502\n\u2502                                          ENV: AZURE_STORAGE_BLOB_NAME        \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --prefix                      PREFIX  A prefix by which all uploaded      \u2502\n\u2502                                          files should start with in the blob \u2502\n\u2502                                          storage                             \u2502\n\u2502                                          ENV: CSM_DATA_BLOB_PREFIX           \u2502\n\u2502    --az-storage-sas-url          URL     SAS url allowing access to the AZ   \u2502\n\u2502                                          storage container                   \u2502\n\u2502                                          ENV: AZURE_STORAGE_SAS_URL          \u2502\n\u2502    --web-help                            Open the web documentation          \u2502\n\u2502    --help                                Show this message and exit.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-delete/","title":"s3-bucket-delete","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-delete --help\n\n Usage: csm-data s3-bucket-delete [OPTIONS]                                     \n\n Delete S3 bucket content to a given folder                                     \n Will delete everything in the bucket unless a prefix is set, then only file    \n following the given prefix will be deleted                                     \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bucket-name         BUCKET  The bucket on S3 to delete                  \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_NAME                   \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --prefix-filter       PREFIX  A prefix by which all deleted files should  \u2502\n\u2502                                  start in the bucket                         \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_PREFIX                 \u2502\n\u2502    --use-ssl/--no-ssl            Use SSL to secure connection to S3          \u2502\n\u2502 *  --s3-url              URL     URL to connect to the S3 system             \u2502\n\u2502                                  ENV: AWS_ENDPOINT_URL                       \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --access-id           ID      Identity used to connect to the S3 system   \u2502\n\u2502                                  ENV: AWS_ACCESS_KEY_ID                      \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --secret-key          ID      Secret tied to the ID used to connect to    \u2502\n\u2502                                  the S3 system                               \u2502\n\u2502                                  ENV: AWS_SECRET_ACCESS_KEY                  \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --ssl-cert-bundle     PATH    Path to an alternate CA Bundle to validate  \u2502\n\u2502                                  SSL connections                             \u2502\n\u2502                                  ENV: CSM_S3_CA_BUNDLE                       \u2502\n\u2502    --web-help                    Open the web documentation                  \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-download/","title":"s3-bucket-download","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-download --help\n\n Usage: csm-data s3-bucket-download [OPTIONS]                                   \n\n Download S3 bucket content to a given folder                                   \n Will download everything in the bucket unless a prefix is set, then only file  \n following the given prefix will be downloaded                                  \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --target-folder       PATH    The folder in which to download the bucket  \u2502\n\u2502                                  content                                     \u2502\n\u2502                                  ENV: CSM_DATASET_ABSOLUTE_PATH              \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --bucket-name         BUCKET  The bucket on S3 to download                \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_NAME                   \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --prefix-filter       PREFIX  A prefix by which all downloaded files      \u2502\n\u2502                                  should start in the bucket                  \u2502\n\u2502                                  ENV: CSM_DATA_BUCKET_PREFIX                 \u2502\n\u2502    --use-ssl/--no-ssl            Use SSL to secure connection to S3          \u2502\n\u2502 *  --s3-url              URL     URL to connect to the S3 system             \u2502\n\u2502                                  ENV: AWS_ENDPOINT_URL                       \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --access-id           ID      Identity used to connect to the S3 system   \u2502\n\u2502                                  ENV: AWS_ACCESS_KEY_ID                      \u2502\n\u2502                                  [required]                                  \u2502\n\u2502 *  --secret-key          ID      Secret tied to the ID used to connect to    \u2502\n\u2502                                  the S3 system                               \u2502\n\u2502                                  ENV: AWS_SECRET_ACCESS_KEY                  \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --ssl-cert-bundle     PATH    Path to an alternate CA Bundle to validate  \u2502\n\u2502                                  SSL connections                             \u2502\n\u2502                                  ENV: CSM_S3_CA_BUNDLE                       \u2502\n\u2502    --web-help                    Open the web documentation                  \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/s3-bucket-upload/","title":"s3-bucket-upload","text":"<p>Help command</p> <pre><code>&gt; csm-data s3-bucket-upload --help\n\n Usage: csm-data s3-bucket-upload [OPTIONS]                                     \n\n Upload a folder to a S3 Bucket                                                 \n Will upload everything from a given folder to a S3 bucket. If a single file is \n passed only it will be uploaded, and recursive will be ignored                 \n\n Giving a prefix will add it to every upload (finishing the prefix with a \"/\"   \n will allow to upload in a folder inside the bucket)                            \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder               PATH    The folder/file to upload to the    \u2502\n\u2502                                          target bucket                       \u2502\n\u2502                                          ENV: CSM_DATASET_ABSOLUTE_PATH      \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --recursive/--no-recursive            Recursively send the content of     \u2502\n\u2502                                          every folder inside the starting    \u2502\n\u2502                                          folder to the bucket                \u2502\n\u2502 *  --bucket-name                 BUCKET  The bucket on S3 to upload to       \u2502\n\u2502                                          ENV: CSM_DATA_BUCKET_NAME           \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --prefix                      PREFIX  A prefix by which all uploaded      \u2502\n\u2502                                          files should start with in the      \u2502\n\u2502                                          bucket                              \u2502\n\u2502                                          ENV: CSM_DATA_BUCKET_PREFIX         \u2502\n\u2502    --use-ssl/--no-ssl                    Use SSL to secure connection to S3  \u2502\n\u2502 *  --s3-url                      URL     URL to connect to the S3 system     \u2502\n\u2502                                          ENV: AWS_ENDPOINT_URL               \u2502\n\u2502                                          [required]                          \u2502\n\u2502 *  --access-id                   ID      Identity used to connect to the S3  \u2502\n\u2502                                          system                              \u2502\n\u2502                                          ENV: AWS_ACCESS_KEY_ID              \u2502\n\u2502                                          [required]                          \u2502\n\u2502 *  --secret-key                  ID      Secret tied to the ID used to       \u2502\n\u2502                                          connect to the S3 system            \u2502\n\u2502                                          ENV: AWS_SECRET_ACCESS_KEY          \u2502\n\u2502                                          [required]                          \u2502\n\u2502    --ssl-cert-bundle             PATH    Path to an alternate CA Bundle to   \u2502\n\u2502                                          validate SSL connections            \u2502\n\u2502                                          ENV: CSM_S3_CA_BUNDLE               \u2502\n\u2502    --web-help                            Open the web documentation          \u2502\n\u2502    --help                                Show this message and exit.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/","title":"api","text":"<p>Help command</p> <pre><code>&gt; csm-data api --help\n\n Usage: csm-data api [OPTIONS] COMMAND [ARGS]...                                \n\n Cosmo Tech API helper command                                                  \n This command will inform you of which connection is available to use for the   \n Cosmo Tech API                                                                 \n\n If no connection is available, will list all possible set of parameters and    \n return an error code,                                                          \n\n You can use this command in a csm-orc template to make sure that API           \n connection is available.                                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --web-help      Open the web documentation                                   \u2502\n\u2502 --help          Show this message and exit.                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 postgres-send-runner-metadata  Send runner metadata to a PostgreSQL          \u2502\n\u2502                                database.                                     \u2502\n\u2502 rds-load-csv                   Load data from a runner's RDS database into a \u2502\n\u2502                                CSV file.                                     \u2502\n\u2502 rds-send-csv                   Send CSV files to a runner's RDS database.    \u2502\n\u2502 rds-send-store                 Send data from a store to a runner's RDS      \u2502\n\u2502                                database.                                     \u2502\n\u2502 run-load-data                  Download a runner data from the Cosmo Tech    \u2502\n\u2502                                API Requires a valid Azure connection either  \u2502\n\u2502                                with:                                         \u2502\n\u2502                                                                              \u2502\n\u2502                                 \u2022 The AZ cli command: az login               \u2502\n\u2502                                 \u2022 A triplet of env var AZURE_TENANT_ID,      \u2502\n\u2502                                   AZURE_CLIENT_ID, AZURE_CLIENT_SECRET       \u2502\n\u2502 runtemplate-load-handler       Uses environment variables to download cloud  \u2502\n\u2502                                based Template steps                          \u2502\n\u2502 tdl-load-files                 Query a twingraph and loads all the data from \u2502\n\u2502                                it                                            \u2502\n\u2502 tdl-send-files                 Reads a folder CSVs and send those to the     \u2502\n\u2502                                Cosmo Tech API as a Dataset                   \u2502\n\u2502 wsf-load-file                  Download files from a workspace.              \u2502\n\u2502 wsf-send-file                  Upload a file to a workspace.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-load-csv/","title":"rds-load-csv","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-load-csv --help\n\n Usage: csm-data api rds-load-csv [OPTIONS]                                     \n\n Load data from a runner's RDS database into a CSV file.                        \n Executes a SQL query against the runner's RDS database and saves the results   \n to a CSV file. By default, it will list all tables in the public schema if no  \n specific query is provided.                                                    \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --target-folder      PATH        The folder where the csv will be written \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --file-name          NAME        A file name to write the query results   \u2502\n\u2502                                     DEFAULT: results                         \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --query              SQL_QUERY   SQL query to execute (defaults to        \u2502\n\u2502                                     listing all tables in public schema)     \u2502\n\u2502                                     DEFAULT: SELECT table_name FROM          \u2502\n\u2502                                     information_schema.tables WHERE          \u2502\n\u2502                                     table_schema='public'                    \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-send-csv/","title":"rds-send-csv","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-send-csv --help\n\n Usage: csm-data api rds-send-csv [OPTIONS]                                     \n\n Send CSV files to a runner's RDS database.                                     \n Takes all CSV files from a source folder and sends their content to the        \n runner's RDS database. Each CSV file will be sent to a table named after the   \n file (without the .csv extension). The table name will be prefixed with \"CD_\"  \n in the database.                                                               \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --source-folder      PATH        The folder containing csvs to send       \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/rds-send-store/","title":"rds-send-store","text":"<p>Help command</p> <pre><code>&gt; csm-data api rds-send-store --help\n\n Usage: csm-data api rds-send-store [OPTIONS]                                   \n\n Send data from a store to a runner's RDS database.                             \n Takes all tables from a store and sends their content to the runner's RDS      \n database. Each table will be sent to a table with the same name, prefixed with \n \"CD_\" in the database. Null values in rows will be removed before sending.     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder       PATH        The folder containing the store files    \u2502\n\u2502                                     ENV: CSM_PARAMETERS_ABSOLUTE_PATH        \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/run-load-data/","title":"run-load-data","text":"<p>Help command</p> <pre><code>&gt; csm-data api run-load-data --help\n\n Usage: csm-data api run-load-data [OPTIONS]                                    \n\n Download a runner data from the Cosmo Tech API Requires a valid Azure          \n connection either with:                                                        \n\n  \u2022 The AZ cli command: az login                                                \n  \u2022 A triplet of env var AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET  \n Requires env var CSM_API_URL     The URL to a Cosmotech API                    \n Requires env var CSM_API_SCOPE   The identification scope of a Cosmotech API   \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id              o-##########  The id of an organization in \u2502\n\u2502                                                 the cosmotech api            \u2502\n\u2502                                                 ENV: CSM_ORGANIZATION_ID     \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --workspace-id                 w-##########  The id of a workspace in the \u2502\n\u2502                                                 cosmotech api                \u2502\n\u2502                                                 ENV: CSM_WORKSPACE_ID        \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --runner-id                    s-##########  The id of a runner in the    \u2502\n\u2502                                                 cosmotech api                \u2502\n\u2502                                                 ENV: CSM_RUNNER_ID           \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --dataset-absolute-path        PATH          csm_data.commands.api.run_l\u2026 \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 CSM_DATASET_ABSOLUTE_PATH    \u2502\n\u2502                                                 [required]                   \u2502\n\u2502 *  --parameters-absolute-path     PATH          A local folder to store the  \u2502\n\u2502                                                 parameters content           \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 CSM_PARAMETERS_ABSOLUTE_PATH \u2502\n\u2502                                                 [required]                   \u2502\n\u2502    --write-json/--no-write-js\u2026                  csm_data.commands.api.run_l\u2026 \u2502\n\u2502                                                 ENV: WRITE_JSON              \u2502\n\u2502                                                 DEFAULT: write-json          \u2502\n\u2502    --write-csv/--no-write-csv                   csm_data.commands.api.run_l\u2026 \u2502\n\u2502                                                 ENV: WRITE_CSV               \u2502\n\u2502                                                 DEFAULT: no-write-csv        \u2502\n\u2502    --fetch-dataset/--no-fetch\u2026                  csm_data.commands.api.run_l\u2026 \u2502\n\u2502                                                 ENV: FETCH_DATASET           \u2502\n\u2502                                                 DEFAULT: fetch-dataset       \u2502\n\u2502    --parallel/--no-parallel                     csm_data.commands.api.run_l\u2026 \u2502\n\u2502                                                 ENV:                         \u2502\n\u2502                                                 FETCH_DATASETS_IN_PARALLEL   \u2502\n\u2502                                                 DEFAULT: parallel            \u2502\n\u2502    --web-help                                   Open the web documentation   \u2502\n\u2502    --help                                       Show this message and exit.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/runtemplate-load-handler/","title":"runtemplate-load-handler","text":"<p>Help command</p> <pre><code>&gt; csm-data api runtemplate-load-handler --help\n\n Usage: csm-data api runtemplate-load-handler [OPTIONS]                         \n\n Uses environment variables to download cloud based Template steps              \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-##########         The id of an organization in    \u2502\n\u2502                                              the cosmotech api               \u2502\n\u2502                                              ENV: CSM_ORGANIZATION_ID        \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --workspace-id       w-##########         The id of a solution in the     \u2502\n\u2502                                              cosmotech api                   \u2502\n\u2502                                              ENV: CSM_WORKSPACE_ID           \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --run-template-id    NAME                 csm_data.commands.api.runtempl\u2026 \u2502\n\u2502                                              ENV: CSM_RUN_TEMPLATE_ID        \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --handler-list       HANDLER,...,HANDLER  A list of handlers to download  \u2502\n\u2502                                              (comma separated)               \u2502\n\u2502                                              ENV: CSM_CONTAINER_MODE         \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --help                                    Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/tdl-load-files/","title":"tdl-load-files","text":"<p>Help command</p> <pre><code>&gt; csm-data api tdl-load-files --help\n\n Usage: csm-data api tdl-load-files [OPTIONS]                                   \n\n Query a twingraph and loads all the data from it                               \n Will create 1 csv file per node type / relationship type                       \n\n The twingraph must have been populated using the \"tdl-send-files\" command for  \n this to work correctly                                                         \n\n Requires a valid connection to the API to send the data                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --scenario-id        s-XXXXXXXX  csm_data.commands.api.tdl_load_files.pa\u2026 \u2502\n\u2502                                     ENV: CSM_SCENARIO_ID                     \u2502\n\u2502    --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502 *  --dir                PATH        Path to the directory to write the       \u2502\n\u2502                                     results to                               \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/tdl-send-files/","title":"tdl-send-files","text":"<p>Help command</p> <pre><code>&gt; csm-data api tdl-send-files --help\n\n Usage: csm-data api tdl-send-files [OPTIONS]                                   \n\n Reads a folder CSVs and send those to the Cosmo Tech API as a Dataset          \n CSVs must follow a given format:                                               \n\n  \u2022 Nodes files must have an id column                                          \n  \u2022 Relationship files must have id, src and dest columns                       \n\n Non-existing relationship (aka dest or src does not point to existing node)    \n won't trigger an error, the relationship will not be created instead.          \n\n Requires a valid connection to the API to send the data                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --api-url            URI         The URI to a Cosmo Tech API instance     \u2502\n\u2502                                     ENV: CSM_API_URL                         \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --dir                PATH        Path to the directory containing csvs to \u2502\n\u2502                                     send                                     \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --clear/--keep                   Flag to clear the target dataset first   \u2502\n\u2502                                     (if set to True will clear the dataset   \u2502\n\u2502                                     before sending anything, irreversibly)   \u2502\n\u2502                                     DEFAULT: clear                           \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/wsf-load-file/","title":"wsf-load-file","text":"<p>Help command</p> <pre><code>&gt; csm-data api wsf-load-file --help\n\n Usage: csm-data api wsf-load-file [OPTIONS]                                    \n\n Download files from a workspace.                                               \n Downloads files from a specified path in a workspace to a local target folder. \n If the workspace path ends with '/', it will be treated as a folder and all    \n files within will be downloaded.                                               \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --workspace-path     PATH        Path inside the workspace to load (end   \u2502\n\u2502                                     with '/' for a folder)                   \u2502\n\u2502 *  --target-folder      PATH        Folder in which to send the downloaded   \u2502\n\u2502                                     file                                     \u2502\n\u2502                                     ENV: CSM_DATASET_ABSOLUTE_PATH           \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/api/wsf-send-file/","title":"wsf-send-file","text":"<p>Help command</p> <pre><code>&gt; csm-data api wsf-send-file --help\n\n Usage: csm-data api wsf-send-file [OPTIONS]                                    \n\n Upload a file to a workspace.                                                  \n Uploads a local file to a specified path in a workspace. If the workspace path \n ends with '/', the file will be uploaded to that folder with its original      \n name. Otherwise, the file will be uploaded with the name specified in the      \n workspace path.                                                                \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --organization-id     o-XXXXXXXX  An organization id for the Cosmo Tech   \u2502\n\u2502                                      API                                     \u2502\n\u2502                                      ENV: CSM_ORGANIZATION_ID                \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --workspace-id        w-XXXXXXXX  A workspace id for the Cosmo Tech API   \u2502\n\u2502                                      ENV: CSM_WORKSPACE_ID                   \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --file-path           PATH        Path to the file to send as a workspace \u2502\n\u2502                                      file                                    \u2502\n\u2502                                      [required]                              \u2502\n\u2502 *  --workspace-path      PATH        Path inside the workspace to store the  \u2502\n\u2502                                      file (end with '/' for a folder)        \u2502\n\u2502                                      [required]                              \u2502\n\u2502    --overwrite/--keep                Flag to overwrite the target file if it \u2502\n\u2502                                      exists                                  \u2502\n\u2502                                      DEFAULT: overwrite                      \u2502\n\u2502    --web-help                        Open the web documentation              \u2502\n\u2502    --help                            Show this message and exit.             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/legacy/","title":"legacy","text":"<p>Help command</p>"},{"location":"csm-data/legacy/generate-orchestrator/","title":"generate-orchestrator","text":"<p>Help command</p>"},{"location":"csm-data/legacy/generate-orchestrator/from-api/","title":"from-api","text":"<p>Help command</p>"},{"location":"csm-data/legacy/generate-orchestrator/from-file/","title":"from-file","text":"<p>Help command</p>"},{"location":"csm-data/legacy/init-local-parameter-folder/","title":"init-local-parameter-folder","text":"<p>Help command</p>"},{"location":"csm-data/legacy/init-local-parameter-folder/cloud/","title":"cloud","text":"<p>Help command</p>"},{"location":"csm-data/legacy/init-local-parameter-folder/solution/","title":"solution","text":"<p>Help command</p>"},{"location":"csm-data/store/","title":"store","text":"<p>Help command</p> <pre><code>&gt; csm-data store --help\n\n Usage: csm-data store [OPTIONS] COMMAND [ARGS]...                              \n\n CoAL Data Store command group                                                  \n This group of commands will give you helper commands to interact with the      \n datastore                                                                      \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --web-help      Open the web documentation                                   \u2502\n\u2502 --help          Show this message and exit.                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 dump-to-azure          Dump a datastore to a Azure storage account.          \u2502\n\u2502 dump-to-postgresql     Running this command will dump your store to a given  \u2502\n\u2502                        postgresql database                                   \u2502\n\u2502 dump-to-s3             Dump a datastore to a S3                              \u2502\n\u2502 list-tables            Running this command will list the existing tables in \u2502\n\u2502                        your datastore                                        \u2502\n\u2502 load-csv-folder        Running this command will find all csvs in the given  \u2502\n\u2502                        folder and put them in the store                      \u2502\n\u2502 load-from-singlestore  Load data from SingleStore tables into the store.     \u2502\n\u2502                        Will download everything from a given SingleStore     \u2502\n\u2502                        database following some configuration into the store. \u2502\n\u2502 rds-send-store         Send data from a store to a runner's RDS database.    \u2502\n\u2502 reset                  Running this command will reset the state of your     \u2502\n\u2502                        store                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-azure/","title":"dump-to-azure","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-azure --help\n\n Usage: csm-data store dump-to-azure [OPTIONS]                                  \n\n Dump a datastore to a Azure storage account.                                   \n Will upload everything from a given data store to a Azure storage container.   \n\n 3 modes currently exists:                                                      \n\n  \u2022 sqlite: will dump the data store underlying database as is                  \n  \u2022 csv: will convert every table of the datastore to csv and send them as      \n    separate files                                                              \n  \u2022 parquet: will convert every table of the datastore to parquet and send them \n    as separate files                                                           \n\n Make use of the azure.storage.blob library to access the container             \n\n More information is available on this page:                                    \n [https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blob \n s-python?tabs=managed-identity%2Croles-azure-portal%2Csign-in-azure-cli&amp;pivots \n =blob-storage-quickstart-scratch]                                              \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder      PATH                  The folder containing the store \u2502\n\u2502                                              files                           \u2502\n\u2502                                              ENV:                            \u2502\n\u2502                                              CSM_PARAMETERS_ABSOLUTE_PATH    \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --output-type       [sqlite|csv|parquet]  Choose the type of file output  \u2502\n\u2502                                              to use (sqlite, csv, parquet)   \u2502\n\u2502 *  --account-name      TEXT                  The account name on Azure to    \u2502\n\u2502                                              upload to                       \u2502\n\u2502                                              ENV: AZURE_ACCOUNT_NAME         \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --container-name    TEXT                  The container name on Azure to  \u2502\n\u2502                                              upload to                       \u2502\n\u2502                                              ENV: AZURE_CONTAINER_NAME       \u2502\n\u2502    --prefix            PREFIX                A prefix by which all uploaded  \u2502\n\u2502                                              files should start with in the  \u2502\n\u2502                                              container                       \u2502\n\u2502                                              ENV: CSM_DATA_PREFIX            \u2502\n\u2502 *  --tenant-id         ID                    Tenant Identity used to connect \u2502\n\u2502                                              to Azure storage system         \u2502\n\u2502                                              ENV: AZURE_TENANT_ID            \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --client-id         ID                    Client Identity used to connect \u2502\n\u2502                                              to Azure storage system         \u2502\n\u2502                                              ENV: AZURE_CLIENT_ID            \u2502\n\u2502                                              [required]                      \u2502\n\u2502 *  --client-secret     ID                    Client Secret tied to the ID    \u2502\n\u2502                                              used to connect to Azure        \u2502\n\u2502                                              storage system                  \u2502\n\u2502                                              ENV: AZURE_CLIENT_SECRET        \u2502\n\u2502                                              [required]                      \u2502\n\u2502    --web-help                                Open the web documentation      \u2502\n\u2502    --help                                    Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-postgresql/","title":"dump-to-postgresql","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-postgresql --help\n\n Usage: csm-data store dump-to-postgresql [OPTIONS]                             \n\n Running this command will dump your store to a given postgresql database       \n Tables names from the store will be prepended with table-prefix in target      \n database                                                                       \n\n The postgresql user must have USAGE granted on the schema for this script to   \n work due to the use of the command COPY FROM STDIN                             \n\n\n\n You can simply give him that grant by running the command: GRANT USAGE ON      \n SCHEMA  TO                                                                     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder         PATH     The folder containing the store files     \u2502\n\u2502                                    ENV: CSM_PARAMETERS_ABSOLUTE_PATH         \u2502\n\u2502                                    [required]                                \u2502\n\u2502    --table-prefix         PREFIX   Prefix to add to the table name           \u2502\n\u2502 *  --postgres-host        TEXT     PostgreSQL host URI                       \u2502\n\u2502                                    ENV: POSTGRES_HOST_URI                    \u2502\n\u2502                                    [required]                                \u2502\n\u2502    --postgres-port        INTEGER  PostgreSQL database port                  \u2502\n\u2502                                    ENV: POSTGRES_HOST_PORT                   \u2502\n\u2502 *  --postgres-db          TEXT     PostgreSQL database name                  \u2502\n\u2502                                    ENV: POSTGRES_DB_NAME                     \u2502\n\u2502                                    [required]                                \u2502\n\u2502 *  --postgres-schema      TEXT     PostgreSQL schema name                    \u2502\n\u2502                                    ENV: POSTGRES_DB_SCHEMA                   \u2502\n\u2502                                    [required]                                \u2502\n\u2502 *  --postgres-user        TEXT     PostgreSQL connection user name           \u2502\n\u2502                                    ENV: POSTGRES_USER_NAME                   \u2502\n\u2502                                    [required]                                \u2502\n\u2502 *  --postgres-password    TEXT     PostgreSQL connection password            \u2502\n\u2502                                    ENV: POSTGRES_USER_PASSWORD               \u2502\n\u2502                                    [required]                                \u2502\n\u2502    --replace/--append              Append data on existing tables            \u2502\n\u2502                                    DEFAULT: replace                          \u2502\n\u2502    --help                          Show this message and exit.               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/dump-to-s3/","title":"dump-to-s3","text":"<p>Help command</p> <pre><code>&gt; csm-data store dump-to-s3 --help\n\n Usage: csm-data store dump-to-s3 [OPTIONS]                                     \n\n Dump a datastore to a S3                                                       \n Will upload everything from a given data store to a S3 bucket.                 \n\n 3 modes currently exists:                                                      \n\n  \u2022 sqlite: will dump the data store underlying database as is                  \n  \u2022 csv: will convert every table of the datastore to csv and send them as      \n    separate files                                                              \n  \u2022 parquet: will convert every table of the datastore to parquet and send them \n    as separate files                                                           \n\n Giving a prefix will add it to every upload (finishing the prefix with a \"/\"   \n will allow to upload in a folder inside the bucket)                            \n\n Make use of the boto3 library to access the bucket                             \n\n More information is available on this page:                                    \n [https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.h \n tml]                                                                           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder        PATH                  The folder containing the     \u2502\n\u2502                                                store files                   \u2502\n\u2502                                                ENV:                          \u2502\n\u2502                                                CSM_PARAMETERS_ABSOLUTE_PATH  \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --output-type         [sqlite|csv|parquet]  Choose the type of file       \u2502\n\u2502                                                output to use (sqlite, csv,   \u2502\n\u2502                                                parquet)                      \u2502\n\u2502 *  --bucket-name         BUCKET                The bucket on S3 to upload to \u2502\n\u2502                                                ENV: CSM_DATA_BUCKET_NAME     \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --prefix              PREFIX                A prefix by which all         \u2502\n\u2502                                                uploaded files should start   \u2502\n\u2502                                                with in the bucket            \u2502\n\u2502                                                ENV: CSM_DATA_BUCKET_PREFIX   \u2502\n\u2502    --use-ssl/--no-ssl                          Use SSL to secure connection  \u2502\n\u2502                                                to S3                         \u2502\n\u2502 *  --s3-url              URL                   URL to connect to the S3      \u2502\n\u2502                                                system                        \u2502\n\u2502                                                ENV: AWS_ENDPOINT_URL         \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --access-id           ID                    Identity used to connect to   \u2502\n\u2502                                                the S3 system                 \u2502\n\u2502                                                ENV: AWS_ACCESS_KEY_ID        \u2502\n\u2502                                                [required]                    \u2502\n\u2502 *  --secret-key          ID                    Secret tied to the ID used to \u2502\n\u2502                                                connect to the S3 system      \u2502\n\u2502                                                ENV: AWS_SECRET_ACCESS_KEY    \u2502\n\u2502                                                [required]                    \u2502\n\u2502    --ssl-cert-bundle     PATH                  Path to an alternate CA       \u2502\n\u2502                                                Bundle to validate SSL        \u2502\n\u2502                                                connections                   \u2502\n\u2502                                                ENV: CSM_S3_CA_BUNDLE         \u2502\n\u2502    --web-help                                  Open the web documentation    \u2502\n\u2502    --help                                      Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/list-tables/","title":"list-tables","text":"<p>Help command</p> <pre><code>&gt; csm-data store list-tables --help\n\n Usage: csm-data store list-tables [OPTIONS]                                    \n\n Running this command will list the existing tables in your datastore           \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder          PATH  The folder containing the store files       \u2502\n\u2502                                  ENV: CSM_PARAMETERS_ABSOLUTE_PATH           \u2502\n\u2502                                  [required]                                  \u2502\n\u2502    --schema/--no-schema          Display the schema of the tables            \u2502\n\u2502    --help                        Show this message and exit.                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/load-csv-folder/","title":"load-csv-folder","text":"<p>Help command</p> <pre><code>&gt; csm-data store load-csv-folder --help\n\n Usage: csm-data store load-csv-folder [OPTIONS]                                \n\n Running this command will find all csvs in the given folder and put them in    \n the store                                                                      \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder    PATH  The folder containing the store files             \u2502\n\u2502                            ENV: CSM_PARAMETERS_ABSOLUTE_PATH                 \u2502\n\u2502                            [required]                                        \u2502\n\u2502 *  --csv-folder      PATH  The folder containing the csv files to store      \u2502\n\u2502                            ENV: CSM_DATASET_ABSOLUTE_PATH                    \u2502\n\u2502                            [required]                                        \u2502\n\u2502    --help                  Show this message and exit.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/load-from-singlestore/","title":"load-from-singlestore","text":"<p>Help command</p> <pre><code>&gt; csm-data store load-from-singlestore --help\n\n Usage: csm-data store load-from-singlestore [OPTIONS]                          \n\n Load data from SingleStore tables into the store. Will download everything     \n from a given SingleStore database following some configuration into the store. \n Make use of the singlestoredb to access to SingleStore                         \n\n More information is available on this page:                                    \n [https://docs.singlestore.com/cloud/developer-resources/connect-with-applicati \n on-development-tools/connect-with-python/connect-using-the-singlestore-python- \n client/]                                                                       \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --singlestore-host        TEXT     SingleStore instance URI               \u2502\n\u2502                                       ENV: SINGLE_STORE_HOST                 \u2502\n\u2502                                       [required]                             \u2502\n\u2502    --singlestore-port        INTEGER  SingleStore port                       \u2502\n\u2502                                       ENV: SINGLE_STORE_PORT                 \u2502\n\u2502 *  --singlestore-db          TEXT     SingleStore database name              \u2502\n\u2502                                       ENV: SINGLE_STORE_DB                   \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-user        TEXT     SingleStore connection user name       \u2502\n\u2502                                       ENV: SINGLE_STORE_USERNAME             \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-password    TEXT     SingleStore connection password        \u2502\n\u2502                                       ENV: SINGLE_STORE_PASSWORD             \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --singlestore-tables      TEXT     SingleStore table names to fetched     \u2502\n\u2502                                       (separated by comma)                   \u2502\n\u2502                                       ENV: SINGLE_STORE_TABLES               \u2502\n\u2502                                       [required]                             \u2502\n\u2502 *  --store-folder            PATH     The folder containing the store files  \u2502\n\u2502                                       ENV: CSM_PARAMETERS_ABSOLUTE_PATH      \u2502\n\u2502                                       [required]                             \u2502\n\u2502    --help                             Show this message and exit.            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/rds-send-store/","title":"rds-send-store","text":"<p>Help command</p> <pre><code>&gt; csm-data store rds-send-store --help\n\n Usage: csm-data store rds-send-store [OPTIONS]                                 \n\n Send data from a store to a runner's RDS database.                             \n Takes all tables from a store and sends their content to the runner's RDS      \n database. Each table will be sent to a table with the same name, prefixed with \n \"CD_\" in the database. Null values in rows will be removed before sending.     \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder       PATH        The folder containing the store files    \u2502\n\u2502                                     ENV: CSM_PARAMETERS_ABSOLUTE_PATH        \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --organization-id    o-XXXXXXXX  An organization id for the Cosmo Tech    \u2502\n\u2502                                     API                                      \u2502\n\u2502                                     ENV: CSM_ORGANIZATION_ID                 \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --workspace-id       w-XXXXXXXX  A workspace id for the Cosmo Tech API    \u2502\n\u2502                                     ENV: CSM_WORKSPACE_ID                    \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --runner-id          r-XXXXXXXX  A runner id for the Cosmo Tech API       \u2502\n\u2502                                     ENV: CSM_RUNNER_ID                       \u2502\n\u2502                                     [required]                               \u2502\n\u2502 *  --run-id             run-XXXXXX  A run id for the Cosmo Tech API          \u2502\n\u2502                                     ENV: CSM_RUN_ID                          \u2502\n\u2502                                     [required]                               \u2502\n\u2502    --web-help                       Open the web documentation               \u2502\n\u2502    --help                           Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"csm-data/store/reset/","title":"reset","text":"<p>Help command</p> <pre><code>&gt; csm-data store reset --help\n\n Usage: csm-data store reset [OPTIONS]                                          \n\n Running this command will reset the state of your store                        \n\n\u256d\u2500 OPTIONS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --store-folder    PATH  The folder containing the store files             \u2502\n\u2502                            ENV: CSM_PARAMETERS_ABSOLUTE_PATH                 \u2502\n\u2502                            [required]                                        \u2502\n\u2502    --help                  Show this message and exit.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"references/SUMMARY/","title":"SUMMARY","text":"<ul> <li>References<ul> <li>coal<ul> <li>azure<ul> <li>storage</li> <li>blob</li> <li>adx<ul> <li>query</li> <li>tables</li> <li>auth</li> <li>utils</li> <li>ingestion</li> </ul> </li> </ul> </li> <li>utils<ul> <li>postgresql</li> </ul> </li> <li>aws<ul> <li>s3</li> </ul> </li> <li>csm<ul> <li>engine</li> </ul> </li> <li>cosmotech_api<ul> <li>parameters</li> <li>run</li> <li>connection</li> <li>workspace</li> <li>twin_data_layer</li> <li>runner<ul> <li>metadata</li> <li>parameters</li> </ul> </li> <li>dataset<ul> <li>utils</li> </ul> </li> </ul> </li> <li>store<ul> <li>native_python</li> <li>csv</li> <li>store</li> <li>pandas</li> <li>pyarrow</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"references/coal/utils/","title":"cosmotech.coal.utils","text":""},{"location":"references/coal/utils/#cosmotech.coal.utils","title":"<code>utils</code>","text":""},{"location":"references/coal/aws/s3/","title":"cosmotech.coal.aws.s3","text":""},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3","title":"<code>s3</code>","text":"<p>S3 bucket operations module.</p> <p>This module provides functions for interacting with S3 buckets, including uploading, downloading, and deleting files.</p>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.create_s3_client","title":"<code>create_s3_client(endpoint_url, access_id, secret_key, use_ssl=True, ssl_cert_bundle=None)</code>","text":"<p>Create an S3 client with the given credentials and configuration.</p> <p>Args:     endpoint_url: The S3 endpoint URL     access_id: The AWS access key ID     secret_key: The AWS secret access key     use_ssl: Whether to use SSL for the connection     ssl_cert_bundle: Path to the SSL certificate bundle</p> <p>Returns:     An S3 client object</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def create_s3_client(\n    endpoint_url: str,\n    access_id: str,\n    secret_key: str,\n    use_ssl: bool = True,\n    ssl_cert_bundle: Optional[str] = None,\n) -&gt; boto3.client:\n    \"\"\"\n    Create an S3 client with the given credentials and configuration.\n\n    Args:\n        endpoint_url: The S3 endpoint URL\n        access_id: The AWS access key ID\n        secret_key: The AWS secret access key\n        use_ssl: Whether to use SSL for the connection\n        ssl_cert_bundle: Path to the SSL certificate bundle\n\n    Returns:\n        An S3 client object\n    \"\"\"\n    boto3_parameters = {\n        \"use_ssl\": use_ssl,\n        \"endpoint_url\": endpoint_url,\n        \"aws_access_key_id\": access_id,\n        \"aws_secret_access_key\": secret_key,\n    }\n    if ssl_cert_bundle:\n        boto3_parameters[\"verify\"] = ssl_cert_bundle\n\n    return boto3.client(\"s3\", **boto3_parameters)\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.create_s3_resource","title":"<code>create_s3_resource(endpoint_url, access_id, secret_key, use_ssl=True, ssl_cert_bundle=None)</code>","text":"<p>Create an S3 resource with the given credentials and configuration.</p> <p>Args:     endpoint_url: The S3 endpoint URL     access_id: The AWS access key ID     secret_key: The AWS secret access key     use_ssl: Whether to use SSL for the connection     ssl_cert_bundle: Path to the SSL certificate bundle</p> <p>Returns:     An S3 resource object</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def create_s3_resource(\n    endpoint_url: str,\n    access_id: str,\n    secret_key: str,\n    use_ssl: bool = True,\n    ssl_cert_bundle: Optional[str] = None,\n) -&gt; boto3.resource:\n    \"\"\"\n    Create an S3 resource with the given credentials and configuration.\n\n    Args:\n        endpoint_url: The S3 endpoint URL\n        access_id: The AWS access key ID\n        secret_key: The AWS secret access key\n        use_ssl: Whether to use SSL for the connection\n        ssl_cert_bundle: Path to the SSL certificate bundle\n\n    Returns:\n        An S3 resource object\n    \"\"\"\n    boto3_parameters = {\n        \"use_ssl\": use_ssl,\n        \"endpoint_url\": endpoint_url,\n        \"aws_access_key_id\": access_id,\n        \"aws_secret_access_key\": secret_key,\n    }\n    if ssl_cert_bundle:\n        boto3_parameters[\"verify\"] = ssl_cert_bundle\n\n    return boto3.resource(\"s3\", **boto3_parameters)\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.delete_objects","title":"<code>delete_objects(bucket_name, s3_resource, file_prefix=None)</code>","text":"<p>Delete objects from an S3 bucket, optionally filtered by prefix.</p> <p>Args:     bucket_name: Name of the S3 bucket     s3_resource: S3 resource object     file_prefix: Optional prefix to filter objects to delete</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def delete_objects(\n    bucket_name: str,\n    s3_resource: boto3.resource,\n    file_prefix: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Delete objects from an S3 bucket, optionally filtered by prefix.\n\n    Args:\n        bucket_name: Name of the S3 bucket\n        s3_resource: S3 resource object\n        file_prefix: Optional prefix to filter objects to delete\n    \"\"\"\n    bucket = s3_resource.Bucket(bucket_name)\n\n    if file_prefix:\n        bucket_files = bucket.objects.filter(Prefix=file_prefix)\n    else:\n        bucket_files = bucket.objects.all()\n\n    boto_objects = [{\"Key\": _file.key} for _file in bucket_files if _file.key != file_prefix]\n    if boto_objects:\n        LOGGER.info(T(\"coal.services.azure_storage.deleting_objects\").format(objects=boto_objects))\n        boto_delete_request = {\"Objects\": boto_objects}\n        bucket.delete_objects(Delete=boto_delete_request)\n    else:\n        LOGGER.info(T(\"coal.services.azure_storage.no_objects\"))\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.download_files","title":"<code>download_files(target_folder, bucket_name, s3_resource, file_prefix=None)</code>","text":"<p>Download files from an S3 bucket to a local folder.</p> <p>Args:     target_folder: Local folder to download files to     bucket_name: Name of the S3 bucket     s3_resource: S3 resource object     file_prefix: Optional prefix to filter objects to download</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def download_files(\n    target_folder: str,\n    bucket_name: str,\n    s3_resource: boto3.resource,\n    file_prefix: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Download files from an S3 bucket to a local folder.\n\n    Args:\n        target_folder: Local folder to download files to\n        bucket_name: Name of the S3 bucket\n        s3_resource: S3 resource object\n        file_prefix: Optional prefix to filter objects to download\n    \"\"\"\n    bucket = s3_resource.Bucket(bucket_name)\n\n    pathlib.Path(target_folder).mkdir(parents=True, exist_ok=True)\n    remove_prefix = False\n    if file_prefix:\n        bucket_files = bucket.objects.filter(Prefix=file_prefix)\n        if file_prefix.endswith(\"/\"):\n            remove_prefix = True\n    else:\n        bucket_files = bucket.objects.all()\n    for _file in bucket_files:\n        if not (path_name := str(_file.key)).endswith(\"/\"):\n            target_file = path_name\n            if remove_prefix:\n                target_file = target_file.removeprefix(file_prefix)\n            output_file = f\"{target_folder}/{target_file}\"\n            pathlib.Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n            LOGGER.info(T(\"coal.services.azure_storage.downloading\").format(path=path_name, output=output_file))\n            bucket.download_file(_file.key, output_file)\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.upload_data_stream","title":"<code>upload_data_stream(data_stream, bucket_name, s3_client, file_name, file_prefix='')</code>","text":"<p>Upload a data stream to an S3 bucket.</p> <p>Args:     data_stream: BytesIO stream containing the data to upload     bucket_name: Name of the S3 bucket     s3_client: S3 client object     file_name: Name of the file to create in the bucket     file_prefix: Prefix to add to the file name in the bucket</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def upload_data_stream(\n    data_stream: BytesIO,\n    bucket_name: str,\n    s3_client: boto3.client,\n    file_name: str,\n    file_prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Upload a data stream to an S3 bucket.\n\n    Args:\n        data_stream: BytesIO stream containing the data to upload\n        bucket_name: Name of the S3 bucket\n        s3_client: S3 client object\n        file_name: Name of the file to create in the bucket\n        file_prefix: Prefix to add to the file name in the bucket\n    \"\"\"\n    uploaded_file_name = file_prefix + file_name\n    data_stream.seek(0)\n    size = len(data_stream.read())\n    data_stream.seek(0)\n\n    LOGGER.info(T(\"coal.common.data_transfer.sending_data\").format(size=size))\n    s3_client.upload_fileobj(data_stream, bucket_name, uploaded_file_name)\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.upload_file","title":"<code>upload_file(file_path, bucket_name, s3_resource, file_prefix='')</code>","text":"<p>Upload a single file to an S3 bucket.</p> <p>Args:     file_path: Path to the file to upload     bucket_name: Name of the S3 bucket     s3_resource: S3 resource object     file_prefix: Prefix to add to the file name in the bucket</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def upload_file(\n    file_path: pathlib.Path,\n    bucket_name: str,\n    s3_resource: boto3.resource,\n    file_prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Upload a single file to an S3 bucket.\n\n    Args:\n        file_path: Path to the file to upload\n        bucket_name: Name of the S3 bucket\n        s3_resource: S3 resource object\n        file_prefix: Prefix to add to the file name in the bucket\n    \"\"\"\n    uploaded_file_name = file_prefix + file_path.name\n    LOGGER.info(T(\"coal.common.data_transfer.file_sent\").format(file_path=file_path, uploaded_name=uploaded_file_name))\n    s3_resource.Bucket(bucket_name).upload_file(str(file_path), uploaded_file_name)\n</code></pre>"},{"location":"references/coal/aws/s3/#cosmotech.coal.aws.s3.upload_folder","title":"<code>upload_folder(source_folder, bucket_name, s3_resource, file_prefix='', recursive=False)</code>","text":"<p>Upload files from a folder to an S3 bucket.</p> <p>Args:     source_folder: Path to the folder containing files to upload     bucket_name: Name of the S3 bucket     s3_resource: S3 resource object     file_prefix: Prefix to add to the file names in the bucket     recursive: Whether to recursively upload files from subdirectories</p> Source code in <code>cosmotech/coal/aws/s3.py</code> <pre><code>def upload_folder(\n    source_folder: str,\n    bucket_name: str,\n    s3_resource: boto3.resource,\n    file_prefix: str = \"\",\n    recursive: bool = False,\n) -&gt; None:\n    \"\"\"\n    Upload files from a folder to an S3 bucket.\n\n    Args:\n        source_folder: Path to the folder containing files to upload\n        bucket_name: Name of the S3 bucket\n        s3_resource: S3 resource object\n        file_prefix: Prefix to add to the file names in the bucket\n        recursive: Whether to recursively upload files from subdirectories\n    \"\"\"\n    source_path = pathlib.Path(source_folder)\n    if not source_path.exists():\n        LOGGER.error(T(\"coal.common.file_operations.not_found\").format(source_folder=source_folder))\n        raise FileNotFoundError(T(\"coal.common.file_operations.not_found\").format(source_folder=source_folder))\n\n    if source_path.is_dir():\n        _source_name = str(source_path)\n        for _file_path in source_path.glob(\"**/*\" if recursive else \"*\"):\n            if _file_path.is_file():\n                _file_name = str(_file_path).removeprefix(_source_name).removeprefix(\"/\")\n                uploaded_file_name = file_prefix + _file_name\n                LOGGER.info(\n                    T(\"coal.common.data_transfer.file_sent\").format(\n                        file_path=_file_path, uploaded_name=uploaded_file_name\n                    )\n                )\n                s3_resource.Bucket(bucket_name).upload_file(str(_file_path), uploaded_file_name)\n    else:\n        upload_file(source_path, bucket_name, s3_resource, file_prefix)\n</code></pre>"},{"location":"references/coal/azure/blob/","title":"cosmotech.coal.azure.blob","text":""},{"location":"references/coal/azure/blob/#cosmotech.coal.azure.blob","title":"<code>blob</code>","text":"<p>Azure Blob Storage operations module.</p> <p>This module provides functions for interacting with Azure Blob Storage, including uploading data from the Store.</p>"},{"location":"references/coal/azure/blob/#cosmotech.coal.azure.blob.dump_store_to_azure","title":"<code>dump_store_to_azure(store_folder, account_name, container_name, tenant_id, client_id, client_secret, output_type='sqlite', file_prefix='')</code>","text":"<p>Dump Store data to Azure Blob Storage.</p> <p>Args:     store_folder: Folder containing the Store     account_name: Azure Storage account name     container_name: Azure Storage container name     tenant_id: Azure tenant ID     client_id: Azure client ID     client_secret: Azure client secret     output_type: Output file type (sqlite, csv, or parquet)     file_prefix: Prefix for uploaded files</p> <p>Raises:     ValueError: If the output type is invalid</p> Source code in <code>cosmotech/coal/azure/blob.py</code> <pre><code>def dump_store_to_azure(\n    store_folder: str,\n    account_name: str,\n    container_name: str,\n    tenant_id: str,\n    client_id: str,\n    client_secret: str,\n    output_type: str = \"sqlite\",\n    file_prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Dump Store data to Azure Blob Storage.\n\n    Args:\n        store_folder: Folder containing the Store\n        account_name: Azure Storage account name\n        container_name: Azure Storage container name\n        tenant_id: Azure tenant ID\n        client_id: Azure client ID\n        client_secret: Azure client secret\n        output_type: Output file type (sqlite, csv, or parquet)\n        file_prefix: Prefix for uploaded files\n\n    Raises:\n        ValueError: If the output type is invalid\n    \"\"\"\n    _s = Store(store_location=store_folder)\n\n    if output_type not in VALID_TYPES:\n        LOGGER.error(T(\"coal.common.validation.invalid_output_type\").format(output_type=output_type))\n        raise ValueError(T(\"coal.common.validation.invalid_output_type\").format(output_type=output_type))\n\n    container_client = BlobServiceClient(\n        account_url=f\"https://{account_name}.blob.core.windows.net/\",\n        credential=ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret),\n    ).get_container_client(container_name)\n\n    def data_upload(data_stream: BytesIO, file_name: str):\n        uploaded_file_name = file_prefix + file_name\n        data_stream.seek(0)\n        size = len(data_stream.read())\n        data_stream.seek(0)\n\n        LOGGER.info(T(\"coal.common.data_transfer.sending_data\").format(size=size))\n        container_client.upload_blob(name=uploaded_file_name, data=data_stream, length=size, overwrite=True)\n\n    if output_type == \"sqlite\":\n        _file_path = _s._database_path\n        _file_name = \"db.sqlite\"\n        _uploaded_file_name = file_prefix + _file_name\n        LOGGER.info(\n            T(\"coal.common.data_transfer.file_sent\").format(file_path=_file_path, uploaded_name=_uploaded_file_name)\n        )\n        with open(_file_path, \"rb\") as data:\n            container_client.upload_blob(name=_uploaded_file_name, data=data, overwrite=True)\n    else:\n        tables = list(_s.list_tables())\n        for table_name in tables:\n            _data_stream = BytesIO()\n            _file_name = None\n            _data = _s.get_table(table_name)\n            if not len(_data):\n                LOGGER.info(T(\"coal.common.data_transfer.table_empty\").format(table_name=table_name))\n                continue\n            if output_type == \"csv\":\n                _file_name = table_name + \".csv\"\n                pc.write_csv(_data, _data_stream)\n            elif output_type == \"parquet\":\n                _file_name = table_name + \".parquet\"\n                pq.write_table(_data, _data_stream)\n            LOGGER.info(\n                T(\"coal.common.data_transfer.sending_table\").format(table_name=table_name, output_type=output_type)\n            )\n            data_upload(_data_stream, _file_name)\n</code></pre>"},{"location":"references/coal/azure/storage/","title":"cosmotech.coal.azure.storage","text":""},{"location":"references/coal/azure/storage/#cosmotech.coal.azure.storage","title":"<code>storage</code>","text":"<p>Azure Storage operations module.</p> <p>This module provides functions for interacting with Azure Storage, including uploading files to blob storage.</p>"},{"location":"references/coal/azure/storage/#cosmotech.coal.azure.storage.upload_file","title":"<code>upload_file(file_path, blob_name, az_storage_sas_url, file_prefix='')</code>","text":"<p>Upload a single file to Azure Blob Storage.</p> <p>Args:     file_path: Path to the file to upload     blob_name: Name of the blob container     az_storage_sas_url: SAS URL for the Azure Storage account     file_prefix: Prefix to add to the file name in the blob</p> Source code in <code>cosmotech/coal/azure/storage.py</code> <pre><code>def upload_file(\n    file_path: pathlib.Path,\n    blob_name: str,\n    az_storage_sas_url: str,\n    file_prefix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Upload a single file to Azure Blob Storage.\n\n    Args:\n        file_path: Path to the file to upload\n        blob_name: Name of the blob container\n        az_storage_sas_url: SAS URL for the Azure Storage account\n        file_prefix: Prefix to add to the file name in the blob\n    \"\"\"\n    uploaded_file_name = blob_name + \"/\" + file_prefix + file_path.name\n    LOGGER.info(T(\"coal.common.data_transfer.file_sent\").format(file_path=file_path, uploaded_name=uploaded_file_name))\n    ContainerClient.from_container_url(az_storage_sas_url).upload_blob(\n        uploaded_file_name, file_path.open(\"rb\"), overwrite=True\n    )\n</code></pre>"},{"location":"references/coal/azure/storage/#cosmotech.coal.azure.storage.upload_folder","title":"<code>upload_folder(source_folder, blob_name, az_storage_sas_url, file_prefix='', recursive=False)</code>","text":"<p>Upload files from a folder to Azure Blob Storage.</p> <p>Args:     source_folder: Path to the folder containing files to upload     blob_name: Name of the blob container     az_storage_sas_url: SAS URL for the Azure Storage account     file_prefix: Prefix to add to the file names in the blob     recursive: Whether to recursively upload files from subdirectories</p> Source code in <code>cosmotech/coal/azure/storage.py</code> <pre><code>def upload_folder(\n    source_folder: str,\n    blob_name: str,\n    az_storage_sas_url: str,\n    file_prefix: str = \"\",\n    recursive: bool = False,\n) -&gt; None:\n    \"\"\"\n    Upload files from a folder to Azure Blob Storage.\n\n    Args:\n        source_folder: Path to the folder containing files to upload\n        blob_name: Name of the blob container\n        az_storage_sas_url: SAS URL for the Azure Storage account\n        file_prefix: Prefix to add to the file names in the blob\n        recursive: Whether to recursively upload files from subdirectories\n    \"\"\"\n    source_path = pathlib.Path(source_folder)\n    if not source_path.exists():\n        LOGGER.error(T(\"coal.common.file_operations.not_found\").format(source_folder=source_folder))\n        raise FileNotFoundError(T(\"coal.common.file_operations.not_found\").format(source_folder=source_folder))\n\n    if source_path.is_dir():\n        _source_name = str(source_path)\n        for _file_path in source_path.glob(\"**/*\" if recursive else \"*\"):\n            if _file_path.is_file():\n                _file_name = str(_file_path).removeprefix(_source_name).removeprefix(\"/\")\n                upload_file(_file_path, blob_name, az_storage_sas_url, file_prefix)\n    else:\n        upload_file(source_path, blob_name, az_storage_sas_url, file_prefix)\n</code></pre>"},{"location":"references/coal/azure/adx/auth/","title":"cosmotech.coal.azure.adx.auth","text":""},{"location":"references/coal/azure/adx/auth/#cosmotech.coal.azure.adx.auth","title":"<code>auth</code>","text":""},{"location":"references/coal/azure/adx/auth/#cosmotech.coal.azure.adx.auth.create_ingest_client","title":"<code>create_ingest_client(ingest_url, client_id=None, client_secret=None, tenant_id=None)</code>","text":"<p>Create a QueuedIngestClient for ingesting data to ADX.</p> <p>Args:     ingest_url: The ingestion URL of the ADX cluster     client_id: Azure client ID (optional, will use environment variable if not provided)     client_secret: Azure client secret (optional, will use environment variable if not provided)     tenant_id: Azure tenant ID (optional, will use environment variable if not provided)</p> <p>Returns:     QueuedIngestClient: A client for ingesting data to ADX</p> Source code in <code>cosmotech/coal/azure/adx/auth.py</code> <pre><code>def create_ingest_client(\n    ingest_url: str,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    tenant_id: Optional[str] = None,\n) -&gt; QueuedIngestClient:\n    \"\"\"\n    Create a QueuedIngestClient for ingesting data to ADX.\n\n    Args:\n        ingest_url: The ingestion URL of the ADX cluster\n        client_id: Azure client ID (optional, will use environment variable if not provided)\n        client_secret: Azure client secret (optional, will use environment variable if not provided)\n        tenant_id: Azure tenant ID (optional, will use environment variable if not provided)\n\n    Returns:\n        QueuedIngestClient: A client for ingesting data to ADX\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.creating_ingest_client\").format(ingest_url=ingest_url))\n\n    try:\n        az_client_id = client_id or os.environ[\"AZURE_CLIENT_ID\"]\n        az_client_secret = client_secret or os.environ[\"AZURE_CLIENT_SECRET\"]\n        az_tenant_id = tenant_id or os.environ[\"AZURE_TENANT_ID\"]\n\n        kcsb = KustoConnectionStringBuilder.with_aad_application_key_authentication(\n            ingest_url, az_client_id, az_client_secret, az_tenant_id\n        )\n        LOGGER.debug(T(\"coal.services.adx.using_app_auth\"))\n    except KeyError:\n        LOGGER.debug(T(\"coal.services.adx.using_cli_auth\"))\n        kcsb = KustoConnectionStringBuilder.with_az_cli_authentication(ingest_url)\n\n    return QueuedIngestClient(kcsb)\n</code></pre>"},{"location":"references/coal/azure/adx/auth/#cosmotech.coal.azure.adx.auth.create_kusto_client","title":"<code>create_kusto_client(cluster_url, client_id=None, client_secret=None, tenant_id=None)</code>","text":"<p>Create a KustoClient for querying ADX.</p> <p>Args:     cluster_url: The URL of the ADX cluster     client_id: Azure client ID (optional, will use environment variable if not provided)     client_secret: Azure client secret (optional, will use environment variable if not provided)     tenant_id: Azure tenant ID (optional, will use environment variable if not provided)</p> <p>Returns:     KustoClient: A client for querying ADX</p> Source code in <code>cosmotech/coal/azure/adx/auth.py</code> <pre><code>def create_kusto_client(\n    cluster_url: str,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    tenant_id: Optional[str] = None,\n) -&gt; KustoClient:\n    \"\"\"\n    Create a KustoClient for querying ADX.\n\n    Args:\n        cluster_url: The URL of the ADX cluster\n        client_id: Azure client ID (optional, will use environment variable if not provided)\n        client_secret: Azure client secret (optional, will use environment variable if not provided)\n        tenant_id: Azure tenant ID (optional, will use environment variable if not provided)\n\n    Returns:\n        KustoClient: A client for querying ADX\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.creating_kusto_client\").format(cluster_url=cluster_url))\n\n    try:\n        az_client_id = client_id or os.environ[\"AZURE_CLIENT_ID\"]\n        az_client_secret = client_secret or os.environ[\"AZURE_CLIENT_SECRET\"]\n        az_tenant_id = tenant_id or os.environ[\"AZURE_TENANT_ID\"]\n\n        kcsb = KustoConnectionStringBuilder.with_aad_application_key_authentication(\n            cluster_url, az_client_id, az_client_secret, az_tenant_id\n        )\n        LOGGER.debug(T(\"coal.services.adx.using_app_auth\"))\n    except KeyError:\n        LOGGER.debug(T(\"coal.services.adx.using_cli_auth\"))\n        kcsb = KustoConnectionStringBuilder.with_az_cli_authentication(cluster_url)\n\n    return KustoClient(kcsb)\n</code></pre>"},{"location":"references/coal/azure/adx/auth/#cosmotech.coal.azure.adx.auth.get_cluster_urls","title":"<code>get_cluster_urls(cluster_name, cluster_region)</code>","text":"<p>Generate cluster and ingest URLs from cluster name and region.</p> <p>Args:     cluster_name: The name of the ADX cluster     cluster_region: The region of the ADX cluster</p> <p>Returns:     tuple: (cluster_url, ingest_url)</p> Source code in <code>cosmotech/coal/azure/adx/auth.py</code> <pre><code>def get_cluster_urls(cluster_name: str, cluster_region: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Generate cluster and ingest URLs from cluster name and region.\n\n    Args:\n        cluster_name: The name of the ADX cluster\n        cluster_region: The region of the ADX cluster\n\n    Returns:\n        tuple: (cluster_url, ingest_url)\n    \"\"\"\n    LOGGER.debug(\n        T(\"coal.services.adx.generating_urls\").format(cluster_name=cluster_name, cluster_region=cluster_region)\n    )\n\n    cluster_url = f\"https://{cluster_name}.{cluster_region}.kusto.windows.net\"\n    ingest_url = f\"https://ingest-{cluster_name}.{cluster_region}.kusto.windows.net\"\n\n    return cluster_url, ingest_url\n</code></pre>"},{"location":"references/coal/azure/adx/auth/#cosmotech.coal.azure.adx.auth.initialize_clients","title":"<code>initialize_clients(adx_uri, adx_ingest_uri)</code>","text":"<p>Initialize and return the Kusto and ingest clients.</p> <p>Args:     adx_uri: The Azure Data Explorer resource URI     adx_ingest_uri: The Azure Data Explorer resource ingest URI</p> <p>Returns:     tuple: (kusto_client, ingest_client)</p> Source code in <code>cosmotech/coal/azure/adx/auth.py</code> <pre><code>def initialize_clients(adx_uri: str, adx_ingest_uri: str) -&gt; Tuple[KustoClient, QueuedIngestClient]:\n    \"\"\"\n    Initialize and return the Kusto and ingest clients.\n\n    Args:\n        adx_uri: The Azure Data Explorer resource URI\n        adx_ingest_uri: The Azure Data Explorer resource ingest URI\n\n    Returns:\n        tuple: (kusto_client, ingest_client)\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.initializing_clients\"))\n    kusto_client = create_kusto_client(adx_uri)\n    ingest_client = create_ingest_client(adx_ingest_uri)\n    return kusto_client, ingest_client\n</code></pre>"},{"location":"references/coal/azure/adx/ingestion/","title":"cosmotech.coal.azure.adx.ingestion","text":""},{"location":"references/coal/azure/adx/ingestion/#cosmotech.coal.azure.adx.ingestion.IngestionStatus","title":"<code>IngestionStatus</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>cosmotech/coal/azure/adx/ingestion.py</code> <pre><code>class IngestionStatus(Enum):\n    QUEUED = \"QUEUED\"\n    SUCCESS = \"SUCCESS\"\n    FAILURE = \"FAILURE\"\n    UNKNOWN = \"UNKNOWN\"\n    TIMEOUT = \"TIMED OUT\"\n</code></pre>"},{"location":"references/coal/azure/adx/query/","title":"cosmotech.coal.azure.adx.query","text":""},{"location":"references/coal/azure/adx/query/#cosmotech.coal.azure.adx.query","title":"<code>query</code>","text":""},{"location":"references/coal/azure/adx/query/#cosmotech.coal.azure.adx.query.run_command_query","title":"<code>run_command_query(client, database, query)</code>","text":"<p>Execute a command query on the database.</p> <p>Args:     client: The KustoClient to use     database: The name of the database     query: The query to execute</p> <p>Returns:     KustoResponseDataSet: The results of the query</p> Source code in <code>cosmotech/coal/azure/adx/query.py</code> <pre><code>def run_command_query(client: KustoClient, database: str, query: str) -&gt; KustoResponseDataSet:\n    \"\"\"\n    Execute a command query on the database.\n\n    Args:\n        client: The KustoClient to use\n        database: The name of the database\n        query: The query to execute\n\n    Returns:\n        KustoResponseDataSet: The results of the query\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.running_command\").format(database=database, query=query))\n\n    result = client.execute_mgmt(database, query)\n    LOGGER.debug(T(\"coal.services.adx.command_complete\"))\n\n    return result\n</code></pre>"},{"location":"references/coal/azure/adx/query/#cosmotech.coal.azure.adx.query.run_query","title":"<code>run_query(client, database, query)</code>","text":"<p>Execute a simple query on the database.</p> <p>Args:     client: The KustoClient to use     database: The name of the database     query: The query to execute</p> <p>Returns:     KustoResponseDataSet: The results of the query</p> Source code in <code>cosmotech/coal/azure/adx/query.py</code> <pre><code>def run_query(client: KustoClient, database: str, query: str) -&gt; KustoResponseDataSet:\n    \"\"\"\n    Execute a simple query on the database.\n\n    Args:\n        client: The KustoClient to use\n        database: The name of the database\n        query: The query to execute\n\n    Returns:\n        KustoResponseDataSet: The results of the query\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.running_query\").format(database=database, query=query))\n\n    result = client.execute(database, query)\n    LOGGER.debug(\n        T(\"coal.services.adx.query_complete\").format(\n            rows=len(result.primary_results[0]) if result.primary_results else 0\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"references/coal/azure/adx/tables/","title":"cosmotech.coal.azure.adx.tables","text":""},{"location":"references/coal/azure/adx/tables/#cosmotech.coal.azure.adx.tables","title":"<code>tables</code>","text":""},{"location":"references/coal/azure/adx/tables/#cosmotech.coal.azure.adx.tables.check_and_create_table","title":"<code>check_and_create_table(kusto_client, database, table_name, data)</code>","text":"<p>Check if a table exists and create it if it doesn't.</p> <p>Args:     kusto_client: The Kusto client     database: The database name     table_name: The table name     data: The PyArrow table data</p> <p>Returns:     bool: True if the table was created, False if it already existed</p> Source code in <code>cosmotech/coal/azure/adx/tables.py</code> <pre><code>def check_and_create_table(kusto_client: KustoClient, database: str, table_name: str, data: pyarrow.Table) -&gt; bool:\n    \"\"\"\n    Check if a table exists and create it if it doesn't.\n\n    Args:\n        kusto_client: The Kusto client\n        database: The database name\n        table_name: The table name\n        data: The PyArrow table data\n\n    Returns:\n        bool: True if the table was created, False if it already existed\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.checking_table_exists\"))\n    if not table_exists(kusto_client, database, table_name):\n        from cosmotech.coal.azure.adx.utils import create_column_mapping\n\n        mapping = create_column_mapping(data)\n        LOGGER.debug(T(\"coal.services.adx.creating_nonexistent_table\"))\n        create_table(kusto_client, database, table_name, mapping)\n        return True\n    return False\n</code></pre>"},{"location":"references/coal/azure/adx/tables/#cosmotech.coal.azure.adx.tables.create_table","title":"<code>create_table(client, database, table_name, schema)</code>","text":"<p>Create a table in the database.</p> <p>Args:     client: The KustoClient to use     database: The name of the database     table_name: The name of the table to create     schema: Dictionary mapping column names to ADX types</p> <p>Returns:     bool: True if the table was created successfully, False otherwise</p> Source code in <code>cosmotech/coal/azure/adx/tables.py</code> <pre><code>def create_table(client: KustoClient, database: str, table_name: str, schema: Dict[str, str]) -&gt; bool:\n    \"\"\"\n    Create a table in the database.\n\n    Args:\n        client: The KustoClient to use\n        database: The name of the database\n        table_name: The name of the table to create\n        schema: Dictionary mapping column names to ADX types\n\n    Returns:\n        bool: True if the table was created successfully, False otherwise\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.creating_table\").format(database=database, table_name=table_name))\n\n    create_query = f\".create-merge table {table_name}(\"\n\n    for column_name, column_type in schema.items():\n        create_query += f\"{column_name}:{column_type},\"\n\n    create_query = create_query[:-1] + \")\"\n\n    LOGGER.debug(T(\"coal.services.adx.create_query\").format(query=create_query))\n\n    try:\n        client.execute(database, create_query)\n        LOGGER.info(T(\"coal.services.adx.table_created\").format(table_name=table_name))\n        return True\n    except Exception as e:\n        LOGGER.error(T(\"coal.services.adx.table_creation_error\").format(table_name=table_name, error=str(e)))\n        return False\n</code></pre>"},{"location":"references/coal/azure/adx/tables/#cosmotech.coal.azure.adx.tables.table_exists","title":"<code>table_exists(client, database, table_name)</code>","text":"<p>Check if a table exists in the database.</p> <p>Args:     client: The KustoClient to use     database: The name of the database     table_name: The name of the table to check</p> <p>Returns:     bool: True if the table exists, False otherwise</p> Source code in <code>cosmotech/coal/azure/adx/tables.py</code> <pre><code>def table_exists(client: KustoClient, database: str, table_name: str) -&gt; bool:\n    \"\"\"\n    Check if a table exists in the database.\n\n    Args:\n        client: The KustoClient to use\n        database: The name of the database\n        table_name: The name of the table to check\n\n    Returns:\n        bool: True if the table exists, False otherwise\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.checking_table\").format(database=database, table_name=table_name))\n\n    get_tables_query = f\".show database ['{database}'] schema| distinct TableName\"\n    tables = client.execute(database, get_tables_query)\n\n    for r in tables.primary_results[0]:\n        if table_name == r[0]:\n            LOGGER.debug(T(\"coal.services.adx.table_exists\").format(table_name=table_name))\n            return True\n\n    LOGGER.debug(T(\"coal.services.adx.table_not_exists\").format(table_name=table_name))\n    return False\n</code></pre>"},{"location":"references/coal/azure/adx/utils/","title":"cosmotech.coal.azure.adx.utils","text":""},{"location":"references/coal/azure/adx/utils/#cosmotech.coal.azure.adx.utils","title":"<code>utils</code>","text":""},{"location":"references/coal/azure/adx/utils/#cosmotech.coal.azure.adx.utils.create_column_mapping","title":"<code>create_column_mapping(data)</code>","text":"<p>Create a column mapping for a PyArrow table.</p> <p>Args:     data: The PyArrow table data</p> <p>Returns:     dict: A mapping of column names to their ADX types</p> Source code in <code>cosmotech/coal/azure/adx/utils.py</code> <pre><code>def create_column_mapping(data: pyarrow.Table) -&gt; Dict[str, str]:\n    \"\"\"\n    Create a column mapping for a PyArrow table.\n\n    Args:\n        data: The PyArrow table data\n\n    Returns:\n        dict: A mapping of column names to their ADX types\n    \"\"\"\n    mapping = dict()\n    for column_name in data.column_names:\n        column = data.column(column_name)\n        try:\n            ex = next(v for v in column.to_pylist() if v is not None)\n        except StopIteration:\n            LOGGER.error(T(\"coal.services.adx.empty_column\").format(column_name=column_name))\n            mapping[column_name] = type_mapping(column_name, \"string\")\n            continue\n        else:\n            mapping[column_name] = type_mapping(column_name, ex)\n    return mapping\n</code></pre>"},{"location":"references/coal/azure/adx/utils/#cosmotech.coal.azure.adx.utils.type_mapping","title":"<code>type_mapping(key, key_example_value)</code>","text":"<p>Map Python types to ADX types.</p> <p>Args:     key: The name of the key     key_example_value: A possible value of the key</p> <p>Returns:     str: The name of the type used in ADX</p> Source code in <code>cosmotech/coal/azure/adx/utils.py</code> <pre><code>def type_mapping(key: str, key_example_value: Any) -&gt; str:\n    \"\"\"\n    Map Python types to ADX types.\n\n    Args:\n        key: The name of the key\n        key_example_value: A possible value of the key\n\n    Returns:\n        str: The name of the type used in ADX\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.adx.mapping_type\").format(key=key, value_type=type(key_example_value).__name__))\n\n    if key == \"SimulationRun\":\n        return \"guid\"\n\n    try:\n        # Use dateutil parser to test if the value could be a date, in case of error it is not\n        dateutil.parser.parse(key_example_value, fuzzy=False)\n        return \"datetime\"\n    except (ValueError, TypeError):\n        pass\n\n    if isinstance(key_example_value, float):\n        return \"real\"\n\n    if isinstance(key_example_value, int):\n        return \"long\"\n\n    # Default case to string\n    return \"string\"\n</code></pre>"},{"location":"references/coal/cosmotech_api/connection/","title":"cosmotech.coal.cosmotech_api.connection","text":""},{"location":"references/coal/cosmotech_api/connection/#cosmotech.coal.cosmotech_api.connection","title":"<code>connection</code>","text":""},{"location":"references/coal/cosmotech_api/parameters/","title":"cosmotech.coal.cosmotech_api.parameters","text":""},{"location":"references/coal/cosmotech_api/parameters/#cosmotech.coal.cosmotech_api.parameters","title":"<code>parameters</code>","text":"<p>Parameter handling functions.</p> <p>This module provides functions for handling parameters in solution templates.</p>"},{"location":"references/coal/cosmotech_api/parameters/#cosmotech.coal.cosmotech_api.parameters.write_parameters","title":"<code>write_parameters(parameter_folder, parameters, write_csv, write_json)</code>","text":"<p>Write parameters to CSV and/or JSON files.</p> <p>Args:     parameter_folder: The folder to write the parameters to     parameters: The parameters to write     write_csv: Whether to write the parameters to a CSV file     write_json: Whether to write the parameters to a JSON file</p> Source code in <code>cosmotech/coal/cosmotech_api/parameters.py</code> <pre><code>def write_parameters(\n    parameter_folder: str, parameters: List[Dict[str, Any]], write_csv: bool, write_json: bool\n) -&gt; None:\n    \"\"\"\n    Write parameters to CSV and/or JSON files.\n\n    Args:\n        parameter_folder: The folder to write the parameters to\n        parameters: The parameters to write\n        write_csv: Whether to write the parameters to a CSV file\n        write_json: Whether to write the parameters to a JSON file\n    \"\"\"\n    if write_csv:\n        tmp_parameter_file = os.path.join(parameter_folder, \"parameters.csv\")\n        LOGGER.info(T(\"coal.cosmotech_api.runner.generating_file\").format(file=tmp_parameter_file))\n        with open(tmp_parameter_file, \"w\") as _file:\n            _w = DictWriter(_file, fieldnames=[\"parameterId\", \"value\", \"varType\", \"isInherited\"])\n            _w.writeheader()\n            _w.writerows(parameters)\n\n    if write_json:\n        tmp_parameter_file = os.path.join(parameter_folder, \"parameters.json\")\n        LOGGER.info(T(\"coal.cosmotech_api.runner.generating_file\").format(file=tmp_parameter_file))\n        with open(tmp_parameter_file, \"w\") as _file:\n            json.dump(parameters, _file, indent=2)\n</code></pre>"},{"location":"references/coal/cosmotech_api/run/","title":"cosmotech.coal.cosmotech_api.run","text":""},{"location":"references/coal/cosmotech_api/run/#cosmotech.coal.cosmotech_api.run","title":"<code>run</code>","text":""},{"location":"references/coal/cosmotech_api/twin_data_layer/","title":"cosmotech.coal.cosmotech_api.twin_data_layer","text":""},{"location":"references/coal/cosmotech_api/twin_data_layer/#cosmotech.coal.cosmotech_api.twin_data_layer.CSVSourceFile","title":"<code>CSVSourceFile</code>","text":"Source code in <code>cosmotech/coal/cosmotech_api/twin_data_layer.py</code> <pre><code>class CSVSourceFile:\n    def __init__(self, file_path: pathlib.Path):\n        self.file_path = file_path\n        if not file_path.name.endswith(\".csv\"):\n            raise ValueError(T(\"coal.common.validation.not_csv_file\").format(file_path=file_path))\n        with open(file_path) as _file:\n            dr = DictReader(_file)\n            self.fields = list(dr.fieldnames)\n        self.object_type = file_path.name[:-4]\n\n        self.id_column = None\n        self.source_column = None\n        self.target_column = None\n\n        for _c in self.fields:\n            if _c.lower() == ID_COLUMN:\n                self.id_column = _c\n            if _c.lower() == SOURCE_COLUMN:\n                self.source_column = _c\n            if _c.lower() == TARGET_COLUMN:\n                self.target_column = _c\n\n        has_id = self.id_column is not None\n        has_source = self.source_column is not None\n        has_target = self.target_column is not None\n\n        is_relation = all([has_source, has_target])\n\n        if not has_id and not is_relation:\n            LOGGER.error(T(\"coal.common.validation.invalid_nodes_relations\").format(file_path=file_path))\n            LOGGER.error(T(\"coal.common.validation.node_requirements\").format(id_column=ID_COLUMN))\n            LOGGER.error(\n                T(\"coal.common.validation.relationship_requirements\").format(\n                    id_column=ID_COLUMN,\n                    source_column=SOURCE_COLUMN,\n                    target_column=TARGET_COLUMN,\n                )\n            )\n            raise ValueError(T(\"coal.common.validation.invalid_nodes_relations\").format(file_path=file_path))\n\n        self.is_node = has_id and not is_relation\n\n        self.content_fields = {\n            _f: _f for _f in self.fields if _f not in [self.id_column, self.source_column, self.target_column]\n        }\n        if has_id:\n            self.content_fields[ID_COLUMN] = self.id_column\n        if is_relation:\n            self.content_fields[SOURCE_COLUMN] = self.source_column\n            self.content_fields[TARGET_COLUMN] = self.target_column\n\n    def reload(self, inplace: bool = False) -&gt; \"CSVSourceFile\":\n        if inplace:\n            self.__init__(self.file_path)\n            return self\n        return CSVSourceFile(self.file_path)\n\n    def generate_query_insert(self) -&gt; str:\n        \"\"\"\n        Read a CSV file headers and generate a CREATE cypher query\n        :return: the Cypher query for CREATE\n        \"\"\"\n\n        field_names = sorted(self.content_fields.keys(), key=len, reverse=True)\n\n        if self.is_node:\n            query = (\n                \"CREATE (:\"\n                + self.object_type\n                + \", \".join(f\"{property_name}: ${self.content_fields[property_name]}\" for property_name in field_names)\n                + \"})\"\n            )\n            # query = (\"UNWIND $params AS params \" +\n            #          f\"MERGE (n:{self.object_type}) \" +\n            #          \"SET n += params\")\n        else:\n            query = (\n                \"MATCH \"\n                + \"(source {\"\n                + ID_COLUMN\n                + \":$\"\n                + self.source_column\n                + \"}),\\n\"\n                + \"(target {\"\n                + ID_COLUMN\n                + \":$\"\n                + self.target_column\n                + \"})\\n\"\n                + \"CREATE (source)-[rel:\"\n                + self.object_type\n                + \" {\"\n                + \", \".join(f\"{property_name}: ${self.content_fields[property_name]}\" for property_name in field_names)\n                + \"}\"\n                + \"]-&gt;(target)\\n\"\n            )\n            # query = (\"UNWIND $params AS params \" +\n            #          \"MATCH (source {\" + ID_COLUMN + \":params.\" + self.source_column + \"})\\n\" +\n            #          \"MATCH (target {\" + ID_COLUMN + \":params.\" + self.target_column + \"})\\n\" +\n            #          f\"CREATE (from) - [rel:{self.object_type}]-&gt;(to)\" +\n            #          \"SET rel += params\")\n        return query\n</code></pre>"},{"location":"references/coal/cosmotech_api/twin_data_layer/#cosmotech.coal.cosmotech_api.twin_data_layer.CSVSourceFile.generate_query_insert","title":"<code>generate_query_insert()</code>","text":"<p>Read a CSV file headers and generate a CREATE cypher query</p> <p>Returns:</p> Type Description <code>str</code> <p>the Cypher query for CREATE</p> Source code in <code>cosmotech/coal/cosmotech_api/twin_data_layer.py</code> <pre><code>def generate_query_insert(self) -&gt; str:\n    \"\"\"\n    Read a CSV file headers and generate a CREATE cypher query\n    :return: the Cypher query for CREATE\n    \"\"\"\n\n    field_names = sorted(self.content_fields.keys(), key=len, reverse=True)\n\n    if self.is_node:\n        query = (\n            \"CREATE (:\"\n            + self.object_type\n            + \", \".join(f\"{property_name}: ${self.content_fields[property_name]}\" for property_name in field_names)\n            + \"})\"\n        )\n        # query = (\"UNWIND $params AS params \" +\n        #          f\"MERGE (n:{self.object_type}) \" +\n        #          \"SET n += params\")\n    else:\n        query = (\n            \"MATCH \"\n            + \"(source {\"\n            + ID_COLUMN\n            + \":$\"\n            + self.source_column\n            + \"}),\\n\"\n            + \"(target {\"\n            + ID_COLUMN\n            + \":$\"\n            + self.target_column\n            + \"})\\n\"\n            + \"CREATE (source)-[rel:\"\n            + self.object_type\n            + \" {\"\n            + \", \".join(f\"{property_name}: ${self.content_fields[property_name]}\" for property_name in field_names)\n            + \"}\"\n            + \"]-&gt;(target)\\n\"\n        )\n        # query = (\"UNWIND $params AS params \" +\n        #          \"MATCH (source {\" + ID_COLUMN + \":params.\" + self.source_column + \"})\\n\" +\n        #          \"MATCH (target {\" + ID_COLUMN + \":params.\" + self.target_column + \"})\\n\" +\n        #          f\"CREATE (from) - [rel:{self.object_type}]-&gt;(to)\" +\n        #          \"SET rel += params\")\n    return query\n</code></pre>"},{"location":"references/coal/cosmotech_api/workspace/","title":"cosmotech.coal.cosmotech_api.workspace","text":""},{"location":"references/coal/cosmotech_api/workspace/#cosmotech.coal.cosmotech_api.workspace","title":"<code>workspace</code>","text":""},{"location":"references/coal/cosmotech_api/workspace/#cosmotech.coal.cosmotech_api.workspace.download_workspace_file","title":"<code>download_workspace_file(api_client, organization_id, workspace_id, file_name, target_dir)</code>","text":"<p>Downloads a given file from a workspace to a given directory If the file is inside a directory in the workspace, sub-directories will be created.</p> <p>Parameters:</p> Name Type Description Default <code>api_client</code> <code>ApiClient</code> <p>An api client used to connect to the Cosmo Tech API</p> required <code>organization_id</code> <code>str</code> <p>An ID of an Organization in the Cosmo Tech API</p> required <code>workspace_id</code> <code>str</code> <p>An ID of a Workspace in the Cosmo Tech API</p> required <code>file_name</code> <code>str</code> <p>The file to download to the workspace</p> required <code>target_dir</code> <code>Path</code> <p>The directory in which to write the file</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The path to the created file</p> Source code in <code>cosmotech/coal/cosmotech_api/workspace.py</code> <pre><code>def download_workspace_file(\n    api_client: cosmotech_api.api_client.ApiClient,\n    organization_id: str,\n    workspace_id: str,\n    file_name: str,\n    target_dir: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"\n    Downloads a given file from a workspace to a given directory\n    If the file is inside a directory in the workspace, sub-directories will be created.\n    :param api_client: An api client used to connect to the Cosmo Tech API\n    :param organization_id: An ID of an Organization in the Cosmo Tech API\n    :param workspace_id: An ID of a Workspace in the Cosmo Tech API\n    :param file_name: The file to download to the workspace\n    :param target_dir: The directory in which to write the file\n    :return: The path to the created file\n    \"\"\"\n    if target_dir.is_file():\n        raise ValueError(T(\"coal.common.file_operations.not_directory\").format(target_dir=target_dir))\n    api_ws = cosmotech_api.api.workspace_api.WorkspaceApi(api_client)\n\n    LOGGER.info(T(\"coal.cosmotech_api.workspace.loading_file\").format(file_name=file_name))\n\n    _file_content = api_ws.download_workspace_file(organization_id, workspace_id, file_name)\n\n    local_target_file = target_dir / file_name\n    local_target_file.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(local_target_file, \"wb\") as _file:\n        _file.write(_file_content)\n\n    LOGGER.info(T(\"coal.cosmotech_api.workspace.file_loaded\").format(file=local_target_file))\n\n    return local_target_file\n</code></pre>"},{"location":"references/coal/cosmotech_api/workspace/#cosmotech.coal.cosmotech_api.workspace.list_workspace_files","title":"<code>list_workspace_files(api_client, organization_id, workspace_id, file_prefix)</code>","text":"<p>Helper function to list all workspace files using a pre-given file prefix</p> <p>Parameters:</p> Name Type Description Default <code>api_client</code> <code>ApiClient</code> <p>An api client used to connect to the Cosmo Tech API</p> required <code>organization_id</code> <code>str</code> <p>An ID of an Organization in the Cosmo Tech API</p> required <code>workspace_id</code> <code>str</code> <p>An ID of a Workspace in the Cosmo Tech API</p> required <code>file_prefix</code> <code>str</code> <p>The prefix of the files to find in the Workspace</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of existing files inside the workspace</p> Source code in <code>cosmotech/coal/cosmotech_api/workspace.py</code> <pre><code>def list_workspace_files(\n    api_client: cosmotech_api.api_client.ApiClient,\n    organization_id: str,\n    workspace_id: str,\n    file_prefix: str,\n) -&gt; list[str]:\n    \"\"\"\n    Helper function to list all workspace files using a pre-given file prefix\n    :param api_client: An api client used to connect to the Cosmo Tech API\n    :param organization_id: An ID of an Organization in the Cosmo Tech API\n    :param workspace_id: An ID of a Workspace in the Cosmo Tech API\n    :param file_prefix: The prefix of the files to find in the Workspace\n    :return: A list of existing files inside the workspace\n    \"\"\"\n    target_list = []\n    api_ws = cosmotech_api.api.workspace_api.WorkspaceApi(api_client)\n    LOGGER.info(T(\"coal.cosmotech_api.workspace.target_is_folder\"))\n    wsf = api_ws.find_all_workspace_files(organization_id, workspace_id)\n    for workspace_file in wsf:\n        if workspace_file.file_name.startswith(file_prefix):\n            target_list.append(workspace_file.file_name)\n\n    if not target_list:\n        LOGGER.error(\n            T(\"coal.common.errors.data_no_workspace_files\").format(file_prefix=file_prefix, workspace_id=workspace_id)\n        )\n        raise ValueError(\n            T(\"coal.common.errors.data_no_workspace_files\").format(file_prefix=file_prefix, workspace_id=workspace_id)\n        )\n\n    return target_list\n</code></pre>"},{"location":"references/coal/cosmotech_api/workspace/#cosmotech.coal.cosmotech_api.workspace.upload_workspace_file","title":"<code>upload_workspace_file(api_client, organization_id, workspace_id, file_path, workspace_path, overwrite=True)</code>","text":"<p>Upload a local file to a given workspace</p> <p>If workspace_path ends with a \"/\" it will be considered as a folder inside the workspace and the file will keep its current name</p> <p>Parameters:</p> Name Type Description Default <code>api_client</code> <code>ApiClient</code> <p>An api client used to connect to the Cosmo Tech API</p> required <code>organization_id</code> <code>str</code> <p>An ID of an Organization in the Cosmo Tech API</p> required <code>workspace_id</code> <code>str</code> <p>An ID of a Workspace in the Cosmo Tech API</p> required <code>file_path</code> <code>str</code> <p>Path to the file to upload in the workspace</p> required <code>workspace_path</code> <code>str</code> <p>The path inside the workspace to upload the file to</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite existing file in the workspace</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The final name of the file uploaded to the workspace</p> Source code in <code>cosmotech/coal/cosmotech_api/workspace.py</code> <pre><code>def upload_workspace_file(\n    api_client: cosmotech_api.api_client.ApiClient,\n    organization_id: str,\n    workspace_id: str,\n    file_path: str,\n    workspace_path: str,\n    overwrite: bool = True,\n) -&gt; str:\n    \"\"\"\n    Upload a local file to a given workspace\n\n    If workspace_path ends with a \"/\" it will be considered as a folder inside the workspace\n    and the file will keep its current name\n\n    :param api_client: An api client used to connect to the Cosmo Tech API\n    :param organization_id: An ID of an Organization in the Cosmo Tech API\n    :param workspace_id: An ID of a Workspace in the Cosmo Tech API\n    :param file_path: Path to the file to upload in the workspace\n    :param workspace_path: The path inside the workspace to upload the file to\n    :param overwrite: Overwrite existing file in the workspace\n    :return: The final name of the file uploaded to the workspace\n    \"\"\"\n    target_file = pathlib.Path(file_path)\n    if not target_file.exists():\n        LOGGER.error(T(\"coal.common.file_operations.not_exists\").format(file_path=file_path))\n        raise ValueError(T(\"coal.common.file_operations.not_exists\").format(file_path=file_path))\n    if not target_file.is_file():\n        LOGGER.error(T(\"coal.common.file_operations.not_single_file\").format(file_path=file_path))\n        raise ValueError(T(\"coal.common.file_operations.not_single_file\").format(file_path=file_path))\n\n    api_ws = cosmotech_api.api.workspace_api.WorkspaceApi(api_client)\n    destination = workspace_path + target_file.name if workspace_path.endswith(\"/\") else workspace_path\n\n    LOGGER.info(T(\"coal.cosmotech_api.workspace.sending_to_api\").format(destination=destination))\n    try:\n        _file = api_ws.upload_workspace_file(\n            organization_id, workspace_id, file_path, overwrite, destination=destination\n        )\n    except cosmotech_api.exceptions.ApiException as e:\n        LOGGER.error(T(\"coal.common.file_operations.already_exists\").format(csv_path=destination))\n        raise e\n\n    LOGGER.info(T(\"coal.cosmotech_api.workspace.file_sent\").format(file=_file.file_name))\n    return _file.file_name\n</code></pre>"},{"location":"references/coal/cosmotech_api/dataset/utils/","title":"cosmotech.coal.cosmotech_api.dataset.utils","text":""},{"location":"references/coal/cosmotech_api/dataset/utils/#cosmotech.coal.cosmotech_api.dataset.utils","title":"<code>utils</code>","text":""},{"location":"references/coal/cosmotech_api/dataset/utils/#cosmotech.coal.cosmotech_api.dataset.utils.get_content_from_twin_graph_data","title":"<code>get_content_from_twin_graph_data(nodes, relationships, restore_names=False)</code>","text":"<p>Extract content from twin graph data.</p> <p>When restore_names is True, the \"id\" value inside the \"properties\" field in the cypher query response is used instead of the numerical id found in the \"id\" field. When restore_names is set to False, this function keeps the previous behavior implemented when adding support for twingraph in v2 (default: False)</p> <p>Example with a sample of cypher response: [{   n: {     id: \"50\"  &lt;-- this id is used if restore_names is False     label: \"Customer\"     properties: {       Satisfaction: 0       SurroundingSatisfaction: 0       Thirsty: false       id: \"Lars_Coret\"  &lt;-- this id is used if restore_names is True     }     type: \"NODE\"   } }]</p> <p>Args:     nodes: List of node data from cypher query     relationships: List of relationship data from cypher query     restore_names: Whether to use property ID instead of node ID</p> <p>Returns:     Dict mapping entity types to lists of entities</p> Source code in <code>cosmotech/coal/cosmotech_api/dataset/utils.py</code> <pre><code>def get_content_from_twin_graph_data(\n    nodes: List[Dict], relationships: List[Dict], restore_names: bool = False\n) -&gt; Dict[str, List[Dict]]:\n    \"\"\"\n    Extract content from twin graph data.\n\n    When restore_names is True, the \"id\" value inside the \"properties\" field in the cypher query response is used\n    instead of the numerical id found in the \"id\" field. When restore_names is set to False, this function\n    keeps the previous behavior implemented when adding support for twingraph in v2 (default: False)\n\n    Example with a sample of cypher response:\n    [{\n      n: {\n        id: \"50\"  &lt;-- this id is used if restore_names is False\n        label: \"Customer\"\n        properties: {\n          Satisfaction: 0\n          SurroundingSatisfaction: 0\n          Thirsty: false\n          id: \"Lars_Coret\"  &lt;-- this id is used if restore_names is True\n        }\n        type: \"NODE\"\n      }\n    }]\n\n    Args:\n        nodes: List of node data from cypher query\n        relationships: List of relationship data from cypher query\n        restore_names: Whether to use property ID instead of node ID\n\n    Returns:\n        Dict mapping entity types to lists of entities\n    \"\"\"\n    LOGGER.debug(\n        T(\"coal.services.dataset.processing_graph_data\").format(\n            nodes_count=len(nodes),\n            relationships_count=len(relationships),\n            restore_names=restore_names,\n        )\n    )\n\n    content = dict()\n    # build keys\n    for item in relationships:\n        content[item[\"src\"][\"label\"]] = list()\n        content[item[\"dest\"][\"label\"]] = list()\n        content[item[\"rel\"][\"label\"]] = list()\n\n    # Process nodes\n    for item in nodes:\n        label = item[\"n\"][\"label\"]\n        props = item[\"n\"][\"properties\"].copy()  # Create a copy to avoid modifying the original\n        if not restore_names:\n            props.update({\"id\": item[\"n\"][\"id\"]})\n        content.setdefault(label, list())\n        content[label].append(props)\n\n    # Process relationships\n    for item in relationships:\n        src = item[\"src\"]\n        dest = item[\"dest\"]\n        rel = item[\"rel\"]\n        props = rel[\"properties\"].copy()  # Create a copy to avoid modifying the original\n        content[rel[\"label\"]].append(\n            {\n                \"id\": rel[\"id\"],\n                \"source\": src[\"properties\"][\"id\"] if restore_names else src[\"id\"],\n                \"target\": dest[\"properties\"][\"id\"] if restore_names else dest[\"id\"],\n                **props,\n            }\n        )\n\n    # Log the number of entities by type\n    for entity_type, entities in content.items():\n        LOGGER.debug(T(\"coal.services.dataset.entity_count\").format(entity_type=entity_type, count=len(entities)))\n\n    return content\n</code></pre>"},{"location":"references/coal/cosmotech_api/dataset/utils/#cosmotech.coal.cosmotech_api.dataset.utils.sheet_to_header","title":"<code>sheet_to_header(sheet_content)</code>","text":"<p>Extract header fields from sheet content.</p> <p>Args:     sheet_content: List of dictionaries representing sheet rows</p> <p>Returns:     List of field names with id, source, and target fields first if present</p> Source code in <code>cosmotech/coal/cosmotech_api/dataset/utils.py</code> <pre><code>def sheet_to_header(sheet_content: List[Dict]) -&gt; List[str]:\n    \"\"\"\n    Extract header fields from sheet content.\n\n    Args:\n        sheet_content: List of dictionaries representing sheet rows\n\n    Returns:\n        List of field names with id, source, and target fields first if present\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.dataset.extracting_headers\").format(rows=len(sheet_content)))\n\n    fieldnames = []\n    has_src = False\n    has_id = False\n\n    for r in sheet_content:\n        for k in r.keys():\n            if k not in fieldnames:\n                if k in [\"source\", \"target\"]:\n                    has_src = True\n                elif k == \"id\":\n                    has_id = True\n                else:\n                    fieldnames.append(k)\n\n    # Ensure source/target and id fields come first\n    if has_src:\n        fieldnames = [\"source\", \"target\"] + fieldnames\n    if has_id:\n        fieldnames = [\"id\"] + fieldnames\n\n    LOGGER.debug(\n        T(\"coal.services.dataset.headers_extracted\").format(\n            count=len(fieldnames),\n            fields=\", \".join(fieldnames[:5]) + (\"...\" if len(fieldnames) &gt; 5 else \"\"),\n        )\n    )\n\n    return fieldnames\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/metadata/","title":"cosmotech.coal.cosmotech_api.runner.metadata","text":""},{"location":"references/coal/cosmotech_api/runner/metadata/#cosmotech.coal.cosmotech_api.runner.metadata","title":"<code>metadata</code>","text":"<p>Runner metadata retrieval functions.</p>"},{"location":"references/coal/cosmotech_api/runner/metadata/#cosmotech.coal.cosmotech_api.runner.metadata.get_runner_metadata","title":"<code>get_runner_metadata(api_client, organization_id, workspace_id, runner_id, include=None, exclude=None)</code>","text":"<p>Get runner metadata from the API.</p> <p>Args:     api_client: The API client to use     organization_id: The ID of the organization     workspace_id: The ID of the workspace     runner_id: The ID of the runner     include: Optional list of fields to include     exclude: Optional list of fields to exclude</p> <p>Returns:     Dictionary with runner metadata</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/metadata.py</code> <pre><code>def get_runner_metadata(\n    api_client: cosmotech_api.api_client.ApiClient,\n    organization_id: str,\n    workspace_id: str,\n    runner_id: str,\n    include: Optional[list[str]] = None,\n    exclude: Optional[list[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Get runner metadata from the API.\n\n    Args:\n        api_client: The API client to use\n        organization_id: The ID of the organization\n        workspace_id: The ID of the workspace\n        runner_id: The ID of the runner\n        include: Optional list of fields to include\n        exclude: Optional list of fields to exclude\n\n    Returns:\n        Dictionary with runner metadata\n    \"\"\"\n    runner_api = cosmotech_api.RunnerApi(api_client)\n    runner: cosmotech_api.Runner = runner_api.get_runner(organization_id, workspace_id, runner_id)\n\n    return runner.model_dump(by_alias=True, exclude_none=True, include=include, exclude=exclude, mode=\"json\")\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/parameters/","title":"cosmotech.coal.cosmotech_api.runner.parameters","text":""},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters","title":"<code>parameters</code>","text":"<p>Parameter handling functions.</p>"},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters.format_parameters_list","title":"<code>format_parameters_list(runner_data)</code>","text":"<p>Format parameters from runner data as a list of dictionaries.</p> <p>Args:     runner_data: Runner data object</p> <p>Returns:     List of parameter dictionaries</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/parameters.py</code> <pre><code>def format_parameters_list(runner_data: Any) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Format parameters from runner data as a list of dictionaries.\n\n    Args:\n        runner_data: Runner data object\n\n    Returns:\n        List of parameter dictionaries\n    \"\"\"\n    parameters = []\n\n    if not runner_data.parameters_values:\n        return parameters\n\n    max_name_size = max(map(lambda r: len(r.parameter_id), runner_data.parameters_values))\n    max_type_size = max(map(lambda r: len(r.var_type), runner_data.parameters_values))\n\n    for parameter_data in runner_data.parameters_values:\n        parameter_name = parameter_data.parameter_id\n        value = parameter_data.value\n        var_type = parameter_data.var_type\n        is_inherited = parameter_data.is_inherited\n\n        parameters.append(\n            {\n                \"parameterId\": parameter_name,\n                \"value\": value,\n                \"varType\": var_type,\n                \"isInherited\": is_inherited,\n            }\n        )\n\n        LOGGER.debug(\n            T(\"coal.cosmotech_api.runner.parameter_debug\").format(\n                param_id=parameter_name,\n                max_name_size=max_name_size,\n                var_type=var_type,\n                max_type_size=max_type_size,\n                value=value,\n                inherited=\" inherited\" if is_inherited else \"\",\n            )\n        )\n\n    return parameters\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters.get_runner_parameters","title":"<code>get_runner_parameters(runner_data)</code>","text":"<p>Extract parameters from runner data.</p> <p>Args:     runner_data: Runner data object</p> <p>Returns:     Dictionary mapping parameter IDs to values</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/parameters.py</code> <pre><code>def get_runner_parameters(runner_data: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract parameters from runner data.\n\n    Args:\n        runner_data: Runner data object\n\n    Returns:\n        Dictionary mapping parameter IDs to values\n    \"\"\"\n    content = dict()\n    for parameter in runner_data.parameters_values:\n        content[parameter.parameter_id] = parameter.value\n    return content\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters.write_parameters","title":"<code>write_parameters(parameter_folder, parameters, write_csv=True, write_json=False)</code>","text":"<p>Write parameters to files based on specified formats.</p> <p>Args:     parameter_folder: Folder to write the files to     parameters: List of parameter dictionaries     write_csv: Whether to write a CSV file     write_json: Whether to write a JSON file</p> <p>Returns:     Dictionary mapping file types to file paths</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/parameters.py</code> <pre><code>def write_parameters(\n    parameter_folder: str,\n    parameters: List[Dict[str, Any]],\n    write_csv: bool = True,\n    write_json: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Write parameters to files based on specified formats.\n\n    Args:\n        parameter_folder: Folder to write the files to\n        parameters: List of parameter dictionaries\n        write_csv: Whether to write a CSV file\n        write_json: Whether to write a JSON file\n\n    Returns:\n        Dictionary mapping file types to file paths\n    \"\"\"\n    result = {}\n\n    if write_csv:\n        result[\"csv\"] = write_parameters_to_csv(parameter_folder, parameters)\n\n    if write_json:\n        result[\"json\"] = write_parameters_to_json(parameter_folder, parameters)\n\n    return result\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters.write_parameters_to_csv","title":"<code>write_parameters_to_csv(parameter_folder, parameters)</code>","text":"<p>Write parameters to a CSV file.</p> <p>Args:     parameter_folder: Folder to write the file to     parameters: List of parameter dictionaries</p> <p>Returns:     Path to the created file</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/parameters.py</code> <pre><code>def write_parameters_to_csv(parameter_folder: str, parameters: List[Dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Write parameters to a CSV file.\n\n    Args:\n        parameter_folder: Folder to write the file to\n        parameters: List of parameter dictionaries\n\n    Returns:\n        Path to the created file\n    \"\"\"\n    pathlib.Path(parameter_folder).mkdir(exist_ok=True, parents=True)\n    tmp_parameter_file = os.path.join(parameter_folder, \"parameters.csv\")\n\n    LOGGER.info(T(\"coal.cosmotech_api.runner.generating_file\").format(file=tmp_parameter_file))\n\n    with open(tmp_parameter_file, \"w\") as _file:\n        _w = DictWriter(_file, fieldnames=[\"parameterId\", \"value\", \"varType\", \"isInherited\"])\n        _w.writeheader()\n        _w.writerows(parameters)\n\n    return tmp_parameter_file\n</code></pre>"},{"location":"references/coal/cosmotech_api/runner/parameters/#cosmotech.coal.cosmotech_api.runner.parameters.write_parameters_to_json","title":"<code>write_parameters_to_json(parameter_folder, parameters)</code>","text":"<p>Write parameters to a JSON file.</p> <p>Args:     parameter_folder: Folder to write the file to     parameters: List of parameter dictionaries</p> <p>Returns:     Path to the created file</p> Source code in <code>cosmotech/coal/cosmotech_api/runner/parameters.py</code> <pre><code>def write_parameters_to_json(parameter_folder: str, parameters: List[Dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Write parameters to a JSON file.\n\n    Args:\n        parameter_folder: Folder to write the file to\n        parameters: List of parameter dictionaries\n\n    Returns:\n        Path to the created file\n    \"\"\"\n    pathlib.Path(parameter_folder).mkdir(exist_ok=True, parents=True)\n    tmp_parameter_file = os.path.join(parameter_folder, \"parameters.json\")\n\n    LOGGER.info(T(\"coal.cosmotech_api.runner.generating_file\").format(file=tmp_parameter_file))\n\n    with open(tmp_parameter_file, \"w\") as _file:\n        json.dump(parameters, _file, indent=2)\n\n    return tmp_parameter_file\n</code></pre>"},{"location":"references/coal/csm/engine/","title":"cosmotech.coal.csm.engine","text":""},{"location":"references/coal/csm/engine/#cosmotech.coal.csm.engine","title":"<code>engine</code>","text":""},{"location":"references/coal/csm/engine/#cosmotech.coal.csm.engine.apply_simple_csv_parameter_to_simulator","title":"<code>apply_simple_csv_parameter_to_simulator(simulator, parameter_name, target_attribute_name, csv_id_column='id', csv_value_column='value')</code>","text":"<p>Accelerator used to apply CSV parameters directly to a simulator Will raise a ValueError if the parameter does not exist If an entity is not found, will skip the row in the CSV</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <p>The simulator object to which the parameter will be applied</p> required <code>parameter_name</code> <code>str</code> <p>The name of the parameter fetched from the API</p> required <code>target_attribute_name</code> <code>str</code> <p>Target attribute of the entities listed in the CSV</p> required <code>csv_id_column</code> <code>str</code> <p>Column in the CSV file used for the entity ID</p> <code>'id'</code> <code>csv_value_column</code> <code>str</code> <p>Column in the CSV file used for the attribute value to change</p> <code>'value'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>cosmotech/coal/csm/engine/__init__.py</code> <pre><code>def apply_simple_csv_parameter_to_simulator(\n    simulator,\n    parameter_name: str,\n    target_attribute_name: str,\n    csv_id_column: str = \"id\",\n    csv_value_column: str = \"value\",\n):\n    \"\"\"\n    Accelerator used to apply CSV parameters directly to a simulator\n    Will raise a ValueError if the parameter does not exist\n    If an entity is not found, will skip the row in the CSV\n    :param simulator: The simulator object to which the parameter will be applied\n    :param parameter_name: The name of the parameter fetched from the API\n    :param target_attribute_name: Target attribute of the entities listed in the CSV\n    :param csv_id_column: Column in the CSV file used for the entity ID\n    :param csv_value_column: Column in the CSV file used for the attribute value to change\n    :return: None\n    \"\"\"\n    parameter_path = os.path.join(os.environ.get(\"CSM_PARAMETERS_ABSOLUTE_PATH\"), parameter_name)\n    if os.path.exists(parameter_path):\n        csv_files = glob.glob(os.path.join(parameter_path, \"*.csv\"))\n        for csv_filename in csv_files:\n            model = simulator.GetModel()\n            with open(csv_filename, \"r\") as csv_file:\n                for row in csv.DictReader(csv_file):\n                    entity_name = row.get(csv_id_column)\n                    value = json.loads(row.get(csv_value_column))\n                    entity = model.FindEntityByName(entity_name)\n                    if entity:\n                        entity.SetAttributeAsString(target_attribute_name, json.dumps(value))\n    else:\n        raise ValueError(f\"Parameter {parameter_name} does not exists.\")\n</code></pre>"},{"location":"references/coal/store/csv/","title":"cosmotech.coal.store.csv","text":""},{"location":"references/coal/store/csv/#cosmotech.coal.store.csv","title":"<code>csv</code>","text":""},{"location":"references/coal/store/native_python/","title":"cosmotech.coal.store.native_python","text":""},{"location":"references/coal/store/native_python/#cosmotech.coal.store.native_python","title":"<code>native_python</code>","text":""},{"location":"references/coal/store/pandas/","title":"cosmotech.coal.store.pandas","text":""},{"location":"references/coal/store/pandas/#cosmotech.coal.store.pandas","title":"<code>pandas</code>","text":""},{"location":"references/coal/store/pyarrow/","title":"cosmotech.coal.store.pyarrow","text":""},{"location":"references/coal/store/pyarrow/#cosmotech.coal.store.pyarrow","title":"<code>pyarrow</code>","text":""},{"location":"references/coal/store/store/","title":"cosmotech.coal.store.store","text":""},{"location":"references/coal/store/store/#cosmotech.coal.store.store.Store","title":"<code>Store</code>","text":"Source code in <code>cosmotech/coal/store/store.py</code> <pre><code>class Store:\n    @staticmethod\n    def sanitize_column(column_name: str) -&gt; str:\n        return column_name.replace(\" \", \"_\")\n\n    def __init__(\n        self,\n        reset=False,\n        store_location: pathlib.Path = pathlib.Path(os.environ.get(\"CSM_PARAMETERS_ABSOLUTE_PATH\", \".\")),\n    ):\n        self.store_location = pathlib.Path(store_location) / \".coal/store\"\n        self.store_location.mkdir(parents=True, exist_ok=True)\n        self._tables = dict()\n        self._database_path = self.store_location / \"db.sqlite\"\n        if reset:\n            self.reset()\n        self._database = str(self._database_path)\n\n    def reset(self):\n        if self._database_path.exists():\n            self._database_path.unlink()\n\n    def get_table(self, table_name: str) -&gt; pyarrow.Table:\n        if not self.table_exists(table_name):\n            raise ValueError(T(\"coal.errors.data.no_table\").format(table_name=table_name))\n        return self.execute_query(f\"select * from {table_name}\")\n\n    def table_exists(self, table_name) -&gt; bool:\n        return table_name in self.list_tables()\n\n    def get_table_schema(self, table_name: str) -&gt; pyarrow.Schema:\n        if not self.table_exists(table_name):\n            raise ValueError(T(\"coal.errors.data.no_table\").format(table_name=table_name))\n        with dbapi.connect(self._database) as conn:\n            return conn.adbc_get_table_schema(table_name)\n\n    def add_table(self, table_name: str, data=pyarrow.Table, replace: bool = False):\n        with dbapi.connect(self._database, autocommit=True) as conn:\n            with conn.cursor() as curs:\n                rows = curs.adbc_ingest(table_name, data, \"replace\" if replace else \"create_append\")\n                LOGGER.debug(T(\"coal.common.data_transfer.rows_inserted\").format(rows=rows, table_name=table_name))\n\n    def execute_query(self, sql_query: str) -&gt; pyarrow.Table:\n        batch_size = 1024\n        batch_size_increment = 1024\n        while True:\n            try:\n                with dbapi.connect(self._database, autocommit=True) as conn:\n                    with conn.cursor() as curs:\n                        curs.adbc_statement.set_options(**{\"adbc.sqlite.query.batch_rows\": str(batch_size)})\n                        curs.execute(sql_query)\n                        return curs.fetch_arrow_table()\n            except OSError:\n                batch_size += batch_size_increment\n\n    def list_tables(self) -&gt; list[str]:\n        with dbapi.connect(self._database) as conn:\n            objects = conn.adbc_get_objects(depth=\"all\").read_all()\n            tables = objects[\"catalog_db_schemas\"][0][0][\"db_schema_tables\"]\n        for table in tables:\n            table_name: pyarrow.StringScalar = table[\"table_name\"]\n            yield table_name.as_py()\n</code></pre>"},{"location":"references/coal/utils/postgresql/","title":"cosmotech.coal.utils.postgresql","text":""},{"location":"references/coal/utils/postgresql/#cosmotech.coal.utils.postgresql","title":"<code>postgresql</code>","text":""},{"location":"references/coal/utils/postgresql/#cosmotech.coal.utils.postgresql.adapt_table_to_schema","title":"<code>adapt_table_to_schema(data, target_schema)</code>","text":"<p>Adapt a PyArrow table to match a target schema with detailed logging.</p> Source code in <code>cosmotech/coal/utils/postgresql.py</code> <pre><code>def adapt_table_to_schema(data: pa.Table, target_schema: pa.Schema) -&gt; pa.Table:\n    \"\"\"\n    Adapt a PyArrow table to match a target schema with detailed logging.\n    \"\"\"\n    LOGGER.debug(T(\"coal.services.postgresql.schema_adaptation_start\").format(rows=len(data)))\n    LOGGER.debug(T(\"coal.services.postgresql.original_schema\").format(schema=data.schema))\n    LOGGER.debug(T(\"coal.services.postgresql.target_schema\").format(schema=target_schema))\n\n    target_fields = {field.name: field.type for field in target_schema}\n    new_columns = []\n\n    # Track adaptations for summary\n    added_columns = []\n    dropped_columns = []\n    type_conversions = []\n    failed_conversions = []\n\n    # Process each field in target schema\n    for field_name, target_type in target_fields.items():\n        if field_name in data.column_names:\n            # Column exists - try to cast to target type\n            col = data[field_name]\n            original_type = col.type\n\n            if original_type != target_type:\n                LOGGER.debug(\n                    T(\"coal.services.postgresql.casting_column\").format(\n                        field_name=field_name,\n                        original_type=original_type,\n                        target_type=target_type,\n                    )\n                )\n                try:\n                    new_col = pa.compute.cast(col, target_type)\n                    new_columns.append(new_col)\n                    type_conversions.append(f\"{field_name}: {original_type} -&gt; {target_type}\")\n                except pa.ArrowInvalid as e:\n                    LOGGER.warning(\n                        T(\"coal.services.postgresql.cast_failed\").format(\n                            field_name=field_name,\n                            original_type=original_type,\n                            target_type=target_type,\n                            error=str(e),\n                        )\n                    )\n                    new_columns.append(pa.nulls(len(data), type=target_type))\n                    failed_conversions.append(f\"{field_name}: {original_type} -&gt; {target_type}\")\n            else:\n                new_columns.append(col)\n        else:\n            # Column doesn't exist - add nulls\n            LOGGER.debug(T(\"coal.services.postgresql.adding_missing_column\").format(field_name=field_name))\n            new_columns.append(pa.nulls(len(data), type=target_type))\n            added_columns.append(field_name)\n\n    # Log columns that will be dropped\n    dropped_columns = [name for name in data.column_names if name not in target_fields]\n    if dropped_columns:\n        LOGGER.debug(T(\"coal.services.postgresql.dropping_columns\").format(columns=dropped_columns))\n\n    # Create new table\n    adapted_table = pa.Table.from_arrays(new_columns, schema=target_schema)\n\n    # Log summary of adaptations\n    LOGGER.debug(T(\"coal.services.postgresql.adaptation_summary\"))\n    if added_columns:\n        LOGGER.debug(T(\"coal.services.postgresql.added_columns\").format(columns=added_columns))\n    if dropped_columns:\n        LOGGER.debug(T(\"coal.services.postgresql.dropped_columns\").format(columns=dropped_columns))\n    if type_conversions:\n        LOGGER.debug(T(\"coal.services.postgresql.successful_conversions\").format(conversions=type_conversions))\n    if failed_conversions:\n        LOGGER.debug(T(\"coal.services.postgresql.failed_conversions\").format(conversions=failed_conversions))\n\n    LOGGER.debug(T(\"coal.services.postgresql.final_schema\").format(schema=adapted_table.schema))\n    return adapted_table\n</code></pre>"},{"location":"references/coal/utils/postgresql/#cosmotech.coal.utils.postgresql.get_postgresql_table_schema","title":"<code>get_postgresql_table_schema(target_table_name, postgres_host, postgres_port, postgres_db, postgres_schema, postgres_user, postgres_password, force_encode=False)</code>","text":"<p>Get the schema of an existing PostgreSQL table using SQL queries.</p> <p>Args:     target_table_name: Name of the table     postgres_host: PostgreSQL host     postgres_port: PostgreSQL port     postgres_db: PostgreSQL database name     postgres_schema: PostgreSQL schema name     postgres_user: PostgreSQL username     postgres_password: PostgreSQL password</p> <p>Returns:     PyArrow Schema if table exists, None otherwise</p> Source code in <code>cosmotech/coal/utils/postgresql.py</code> <pre><code>def get_postgresql_table_schema(\n    target_table_name: str,\n    postgres_host: str,\n    postgres_port: str,\n    postgres_db: str,\n    postgres_schema: str,\n    postgres_user: str,\n    postgres_password: str,\n    force_encode: bool = False,\n) -&gt; Optional[pa.Schema]:\n    \"\"\"\n    Get the schema of an existing PostgreSQL table using SQL queries.\n\n    Args:\n        target_table_name: Name of the table\n        postgres_host: PostgreSQL host\n        postgres_port: PostgreSQL port\n        postgres_db: PostgreSQL database name\n        postgres_schema: PostgreSQL schema name\n        postgres_user: PostgreSQL username\n        postgres_password: PostgreSQL password\n\n    Returns:\n        PyArrow Schema if table exists, None otherwise\n    \"\"\"\n    LOGGER.debug(\n        T(\"coal.services.postgresql.getting_schema\").format(\n            postgres_schema=postgres_schema, target_table_name=target_table_name\n        )\n    )\n\n    postgresql_full_uri = generate_postgresql_full_uri(\n        postgres_host,\n        postgres_port,\n        postgres_db,\n        postgres_user,\n        postgres_password,\n        force_encode,\n    )\n\n    with dbapi.connect(postgresql_full_uri) as conn:\n        try:\n            return conn.adbc_get_table_schema(\n                target_table_name,\n                db_schema_filter=postgres_schema,\n            )\n        except adbc_driver_manager.ProgrammingError:\n            LOGGER.warning(\n                T(\"coal.services.postgresql.table_not_found\").format(\n                    postgres_schema=postgres_schema, target_table_name=target_table_name\n                )\n            )\n        return None\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>A list of tutorials for concepts added to CoAL</p> <p> csm-data</p> <p>Make full use of <code>csm-data</code> commands to connect to services during your orchestration runs</p> <p> csm-data</p> <p> Data store</p> <p>The datastore is your friend to keep data between orchestration steps. It comes with multiple ways to interact with it.</p> <p> Datastore</p> <p> CosmoTech API</p> <p>Learn how to interact with the CosmoTech API directly: authentication, workspaces, Twin Data Layer, and more.</p> <p> CosmoTech API</p> <p> Contributing to CoAL</p> <p>Learn how to contribute to CoAL: from setting up your development environment to implementing new features and submitting pull requests.</p> <p> Contributing</p>"},{"location":"tutorials/contributing/","title":"Contributing to CoAL","text":"<p>Objective</p> <ul> <li>Set up your development environment with Black and pre-commit hooks</li> <li>Understand the CoAL architecture and contribution workflow</li> <li>Learn how to implement a new feature with a practical example</li> <li>Master the process of writing unit tests and documentation</li> <li>Successfully submit a pull request</li> </ul>"},{"location":"tutorials/contributing/#introduction","title":"Introduction","text":"<p>Contributing to the CosmoTech Acceleration Library (CoAL) is a great way to enhance the platform's capabilities and share your expertise with the community. This tutorial will guide you through the entire process of contributing a new feature to CoAL, from setting up your development environment to submitting a pull request.</p> <p>We'll use a practical example throughout this tutorial: implementing a new store write functionality for MongoDB and creating a corresponding csm-data command. This example will demonstrate all the key aspects of the contribution process, including:</p> <ul> <li>Setting up your development environment</li> <li>Understanding the CoAL architecture</li> <li>Implementing new functionality</li> <li>Creating CLI commands</li> <li>Writing unit tests</li> <li>Documenting your work</li> <li>Submitting a pull request</li> </ul> <p>By the end of this tutorial, you'll have a solid understanding of how to contribute to CoAL and be ready to implement your own features.</p>"},{"location":"tutorials/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Before you start contributing, you need to set up your development environment. This includes forking and cloning the repository, installing dependencies, and configuring code formatting tools.</p>"},{"location":"tutorials/contributing/#forking-and-cloning-the-repository","title":"Forking and Cloning the Repository","text":"<ol> <li>Fork the CosmoTech-Acceleration-Library repository on GitHub</li> <li> <p>Clone your fork locally:</p> <pre><code>git clone https://github.com/your-username/CosmoTech-Acceleration-Library.git\ncd CosmoTech-Acceleration-Library\n</code></pre> </li> <li> <p>Add the upstream repository as a remote:</p> <pre><code>git remote add upstream https://github.com/Cosmo-Tech/CosmoTech-Acceleration-Library.git\n</code></pre> </li> </ol>"},{"location":"tutorials/contributing/#installing-dependencies","title":"Installing Dependencies","text":"<p>Install the package in development mode along with all development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This will install the package in editable mode, allowing you to make changes to the code without reinstalling it. It will also install all the development dependencies specified in the <code>pyproject.toml</code> file.</p>"},{"location":"tutorials/contributing/#setting-up-black-for-code-formatting","title":"Setting Up Black for Code Formatting","text":"<p>CoAL uses Black for code formatting to ensure consistent code style across the codebase. Black is configured in the <code>pyproject.toml</code> file with specific settings for line length, target Python version, and file exclusions.</p> <p>To manually run Black on your codebase:</p> <pre><code># Format all Python files in the project\npython -m black .\n\n# Format a specific directory\npython -m black cosmotech/coal/\n\n# Check if files would be reformatted without actually changing them\npython -m black --check .\n\n# Show diff of changes without writing files\npython -m black --diff .\n</code></pre>"},{"location":"tutorials/contributing/#configuring-pre-commit-hooks","title":"Configuring Pre-commit Hooks","text":"<p>CoAL uses pre-commit hooks to automatically run checks before each commit, including Black formatting, trailing whitespace removal, and test coverage verification.</p> <p>To install pre-commit:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now, when you commit changes, the pre-commit hooks will automatically run and check your code. If any issues are found, the commit will be aborted, and you'll need to fix the issues before committing again.</p> <p>The pre-commit configuration includes:</p> <ul> <li>Trailing whitespace removal</li> <li>End-of-file fixer</li> <li>YAML syntax checking</li> <li>Black code formatting</li> <li>Pytest checks with coverage requirements</li> <li>Verification that all functions have tests</li> </ul>"},{"location":"tutorials/contributing/#understanding-the-coal-architecture","title":"Understanding the CoAL Architecture","text":"<p>Before implementing a new feature, it's important to understand the architecture of CoAL and how its components interact.</p>"},{"location":"tutorials/contributing/#core-modules","title":"Core Modules","text":"<p>CoAL is organized into several key modules:</p> <ul> <li>coal: The core library functionality<ul> <li>store: Data storage and retrieval</li> <li>cosmotech_api: Interaction with the CosmoTech API</li> <li>aws: AWS integration</li> <li>azure: Azure integration</li> <li>postgresql: PostgreSQL integration</li> <li>utils: Utility functions</li> </ul> </li> <li>csm_data: CLI commands for data operations</li> <li>orchestrator_plugins: Plugins for csm-orc</li> <li>translation: Translation resources</li> </ul>"},{"location":"tutorials/contributing/#store-module-architecture","title":"Store Module Architecture","text":"<p>The store module provides a unified interface for data storage and retrieval. It's built around the <code>Store</code> class, which provides methods for:</p> <ul> <li>Adding and retrieving tables</li> <li>Executing SQL queries</li> <li>Listing available tables</li> <li>Resetting the store</li> </ul> <p>The store module also includes adapters for different data formats:</p> <ul> <li>native_python: Python dictionaries and lists</li> <li>csv: CSV files</li> <li>pandas: Pandas DataFrames</li> <li>pyarrow: PyArrow Tables</li> </ul> <p>External storage systems are implemented as separate modules that interact with the core <code>Store</code> class:</p> <ul> <li>postgresql: PostgreSQL integration</li> <li>singlestore: SingleStore integration</li> </ul>"},{"location":"tutorials/contributing/#cli-command-structure","title":"CLI Command Structure","text":"<p>The <code>csm_data</code> CLI is organized into command groups, each focused on specific types of operations:</p> <ul> <li>api: Commands for interacting with the CosmoTech API</li> <li>store: Commands for working with the CoAL datastore</li> <li>s3-bucket-*: Commands for S3 bucket operations</li> <li>adx-send-scenariodata: Command for sending scenario data to Azure Data Explorer</li> <li>az-storage-upload: Command for uploading to Azure Storage</li> </ul> <p>Each command is implemented as a separate Python file in the appropriate directory, using the Click library for command-line interface creation.</p>"},{"location":"tutorials/contributing/#implementing-a-new-store-feature","title":"Implementing a New Store Feature","text":"<p>Now that we understand the architecture, let's implement a new store feature: MongoDB integration. This will allow users to write data from the CoAL datastore to MongoDB.</p>"},{"location":"tutorials/contributing/#creating-the-module-structure","title":"Creating the Module Structure","text":"<p>First, we'll create a new module for MongoDB integration:</p> <pre><code>mkdir -p cosmotech/coal/mongodb\ntouch cosmotech/coal/mongodb/__init__.py\ntouch cosmotech/coal/mongodb/store.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-the-core-functionality","title":"Implementing the Core Functionality","text":"<p>Now, let's implement the core functionality in <code>cosmotech/coal/mongodb/store.py</code>:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\n\"\"\"\nMongoDB store operations module.\n\nThis module provides functions for interacting with MongoDB databases\nfor store operations.\n\"\"\"\n\nfrom time import perf_counter\nimport pyarrow\nimport pymongo\n\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.utils.logger import LOGGER\nfrom cosmotech.orchestrator.utils.translate import T\n\n\ndef send_pyarrow_table_to_mongodb(\n    data: pyarrow.Table,\n    collection_name: str,\n    mongodb_uri: str,\n    mongodb_db: str,\n    replace: bool = True,\n) -&gt; int:\n    \"\"\"\n    Send a PyArrow table to MongoDB.\n\n    Args:\n        data: PyArrow table to send\n        collection_name: MongoDB collection name\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        replace: Whether to replace existing collection\n\n    Returns:\n        Number of documents inserted\n    \"\"\"\n    # Convert PyArrow table to list of dictionaries\n    records = data.to_pylist()\n\n    # Connect to MongoDB\n    client = pymongo.MongoClient(mongodb_uri)\n    db = client[mongodb_db]\n\n    # Drop collection if replace is True and collection exists\n    if replace and collection_name in db.list_collection_names():\n        db[collection_name].drop()\n\n    # Insert records\n    if records:\n        result = db[collection_name].insert_many(records)\n        return len(result.inserted_ids)\n\n    return 0\n\n\ndef dump_store_to_mongodb(\n    store_folder: str,\n    mongodb_uri: str,\n    mongodb_db: str,\n    collection_prefix: str = \"Cosmotech_\",\n    replace: bool = True,\n) -&gt; None:\n    \"\"\"\n    Dump Store data to a MongoDB database.\n\n    Args:\n        store_folder: Folder containing the Store\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        collection_prefix: Collection prefix\n        replace: Whether to replace existing collections\n    \"\"\"\n    _s = Store(store_location=store_folder)\n\n    tables = list(_s.list_tables())\n    if len(tables):\n        LOGGER.info(T(\"coal.logs.database.sending_data\").format(table=mongodb_db))\n        total_rows = 0\n        _process_start = perf_counter()\n        for table_name in tables:\n            _s_time = perf_counter()\n            target_collection_name = f\"{collection_prefix}{table_name}\"\n            LOGGER.info(T(\"coal.logs.database.table_entry\").format(table=target_collection_name))\n            data = _s.get_table(table_name)\n            if not len(data):\n                LOGGER.info(T(\"coal.logs.database.no_rows\"))\n                continue\n            _dl_time = perf_counter()\n            rows = send_pyarrow_table_to_mongodb(\n                data,\n                target_collection_name,\n                mongodb_uri,\n                mongodb_db,\n                replace,\n            )\n            total_rows += rows\n            _up_time = perf_counter()\n            LOGGER.info(T(\"coal.logs.database.row_count\").format(count=rows))\n            LOGGER.debug(\n                T(\"coal.logs.progress.operation_timing\").format(\n                    operation=\"Load from datastore\", time=f\"{_dl_time - _s_time:0.3}\"\n                )\n            )\n            LOGGER.debug(\n                T(\"coal.logs.progress.operation_timing\").format(\n                    operation=\"Send to MongoDB\", time=f\"{_up_time - _dl_time:0.3}\"\n                )\n            )\n        _process_end = perf_counter()\n        LOGGER.info(\n            T(\"coal.logs.database.rows_fetched\").format(\n                table=\"all tables\",\n                count=total_rows,\n                time=f\"{_process_end - _process_start:0.3}\",\n            )\n        )\n    else:\n        LOGGER.info(T(\"coal.logs.database.store_empty\"))\n</code></pre>"},{"location":"tutorials/contributing/#updating-the-package-initialization","title":"Updating the Package Initialization","text":"<p>Next, we need to update the <code>__init__.py</code> file to expose our new function:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.coal.mongodb.store import dump_store_to_mongodb\n\n__all__ = [\"dump_store_to_mongodb\"]\n</code></pre>"},{"location":"tutorials/contributing/#adding-dependencies","title":"Adding Dependencies","text":"<p>We need to add pymongo as a dependency. Update the <code>pyproject.toml</code> file to include pymongo in the optional dependencies:</p> <pre><code>[project.optional-dependencies]\nmongodb = [\"pymongo&gt;=4.3.3\"]\n</code></pre>"},{"location":"tutorials/contributing/#creating-a-new-csm-data-command","title":"Creating a new CSM-DATA Command","text":"<p>Now that we have implemented the core functionality, let's create a new csm-data command to expose this functionality to users.</p>"},{"location":"tutorials/contributing/#creating-the-command-file","title":"Creating the Command File","text":"<p>Create a new file for the command:</p> <pre><code>touch cosmotech/csm_data/commands/store/dump_to_mongodb.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-the-command","title":"Implementing the Command","text":"<p>Now, let's implement the command:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.csm_data.utils.click import click\nfrom cosmotech.csm_data.utils.decorators import web_help, translate_help\nfrom cosmotech.orchestrator.utils.translate import T\n\n\n@click.command()\n@web_help(\"csm-data/store/dump-to-mongodb\")\n@translate_help(\"csm_data.commands.store.dump_to_mongodb.description\")\n@click.option(\n    \"--store-folder\",\n    envvar=\"CSM_PARAMETERS_ABSOLUTE_PATH\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.store_folder\"),\n    metavar=\"PATH\",\n    type=str,\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--collection-prefix\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.collection_prefix\"),\n    metavar=\"PREFIX\",\n    type=str,\n    default=\"Cosmotech_\",\n)\n@click.option(\n    \"--mongodb-uri\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.mongodb_uri\"),\n    envvar=\"MONGODB_URI\",\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--mongodb-db\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.mongodb_db\"),\n    envvar=\"MONGODB_DB_NAME\",\n    show_envvar=True,\n    required=True,\n)\n@click.option(\n    \"--replace/--append\",\n    \"replace\",\n    help=T(\"csm_data.commands.store.dump_to_mongodb.parameters.replace\"),\n    default=True,\n    is_flag=True,\n    show_default=True,\n)\ndef dump_to_mongodb(\n    store_folder,\n    collection_prefix: str,\n    mongodb_uri,\n    mongodb_db,\n    replace: bool,\n):\n    # Import the function at the start of the command\n    from cosmotech.coal.mongodb import dump_store_to_mongodb\n\n    dump_store_to_mongodb(\n        store_folder=store_folder,\n        collection_prefix=collection_prefix,\n        mongodb_uri=mongodb_uri,\n        mongodb_db=mongodb_db,\n        replace=replace,\n    )\n</code></pre>"},{"location":"tutorials/contributing/#registering-the-command","title":"Registering the Command","text":"<p>Update the <code>cosmotech/csm_data/commands/store/__init__.py</code> file to register the new command:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nfrom cosmotech.csm_data.commands.store.dump_to_azure import dump_to_azure\nfrom cosmotech.csm_data.commands.store.dump_to_postgresql import dump_to_postgresql\nfrom cosmotech.csm_data.commands.store.dump_to_s3 import dump_to_s3\nfrom cosmotech.csm_data.commands.store.dump_to_mongodb import dump_to_mongodb  # Add this line\nfrom cosmotech.csm_data.commands.store.list_tables import list_tables\nfrom cosmotech.csm_data.commands.store.load_csv_folder import load_csv_folder\nfrom cosmotech.csm_data.commands.store.load_from_singlestore import load_from_singlestore\nfrom cosmotech.csm_data.commands.store.reset import reset\nfrom cosmotech.csm_data.commands.store.store import store\n\n__all__ = [\n    \"dump_to_azure\",\n    \"dump_to_postgresql\",\n    \"dump_to_s3\",\n    \"dump_to_mongodb\",  # Add this line\n    \"list_tables\",\n    \"load_csv_folder\",\n    \"load_from_singlestore\",\n    \"reset\",\n    \"store\",\n]\n</code></pre>"},{"location":"tutorials/contributing/#adding-translation-strings","title":"Adding Translation Strings","text":"<p>Create translation files for the new command:</p> <ol> <li>For English (en-US):</li> </ol> <pre><code>touch cosmotech/translation/csm_data/en-US/commands/store/dump_to_mongodb.yml\n</code></pre> <pre><code>commands:\n  store:\n    dump_to_mongodb:\n      description: |\n        Dump store data to MongoDB.\n      parameters:\n        store_folder: Folder containing the store\n        collection_prefix: Prefix for MongoDB collections\n        mongodb_uri: MongoDB connection URI\n        mongodb_db: MongoDB database name\n        replace: Replace existing collections\n</code></pre> <ol> <li>For French (fr-FR):</li> </ol> <pre><code>touch cosmotech/translation/csm_data/fr-FR/commands/store/dump_to_mongodb.yml\n</code></pre> <pre><code>commands:\n  store:\n    dump_to_mongodb:\n      description: |\n        Exporter les donn\u00e9es du store vers MongoDB.\n      parameters:\n        store_folder: Dossier contenant le store\n        collection_prefix: Pr\u00e9fixe pour les collections MongoDB\n        mongodb_uri: URI de connexion MongoDB\n        mongodb_db: Nom de la base de donn\u00e9es MongoDB\n        replace: Remplacer les collections existantes\n</code></pre>"},{"location":"tutorials/contributing/#writing-unit-tests","title":"Writing Unit Tests","text":"<p>Testing is a critical part of the contribution process. All new functionality must be thoroughly tested to ensure it works as expected and to prevent regressions.</p>"},{"location":"tutorials/contributing/#creating-test-files","title":"Creating Test Files","text":"<p>Create test files for the new functionality:</p> <pre><code>mkdir -p tests/unit/coal/mongodb\ntouch tests/unit/coal/mongodb/__init__.py\ntouch tests/unit/coal/mongodb/test_store.py\n</code></pre>"},{"location":"tutorials/contributing/#implementing-unit-tests","title":"Implementing Unit Tests","text":"<p>Now, let's implement the unit tests for the MongoDB store functionality:</p> <pre><code># Copyright (C) - 2023 - 2025 - Cosmo Tech\n# This document and all information contained herein is the exclusive property -\n# including all intellectual property rights pertaining thereto - of Cosmo Tech.\n# Any use, reproduction, translation, broadcasting, transmission, distribution,\n# etc., to any person is prohibited unless it has been previously and\n# specifically authorized by written means by Cosmo Tech.\n\nimport os\nimport tempfile\nfrom unittest.mock import patch, MagicMock\n\nimport pyarrow\nimport pytest\n\nfrom cosmotech.coal.mongodb.store import send_pyarrow_table_to_mongodb, dump_store_to_mongodb\nfrom cosmotech.coal.store.store import Store\n\n\n@pytest.fixture\ndef sample_table():\n    \"\"\"Create a sample PyArrow table for testing.\"\"\"\n    data = {\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [30, 25, 35],\n    }\n    return pyarrow.Table.from_pydict(data)\n\n\n@pytest.fixture\ndef temp_store():\n    \"\"\"Create a temporary store for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        store = Store(store_location=temp_dir)\n        yield store, temp_dir\n\n\nclass TestSendPyarrowTableToMongoDB:\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = []\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_replace(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = [\"test_collection\"]\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.drop.assert_called_once()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_append(self, mock_client, sample_table):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = [\"test_collection\"]\n        mock_collection.insert_many.return_value.inserted_ids = [\"id1\", \"id2\", \"id3\"]\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            sample_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            False,\n        )\n\n        # Verify the result\n        assert result == 3\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.drop.assert_not_called()\n        mock_collection.insert_many.assert_called_once()\n\n    @patch(\"pymongo.MongoClient\")\n    def test_send_pyarrow_table_to_mongodb_empty(self, mock_client):\n        # Set up mocks\n        mock_db = MagicMock()\n        mock_collection = MagicMock()\n        mock_client.return_value.__getitem__.return_value = mock_db\n        mock_db.__getitem__.return_value = mock_collection\n        mock_db.list_collection_names.return_value = []\n\n        # Create an empty table\n        empty_table = pyarrow.Table.from_pydict({})\n\n        # Call the function\n        result = send_pyarrow_table_to_mongodb(\n            empty_table,\n            \"test_collection\",\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            True,\n        )\n\n        # Verify the result\n        assert result == 0\n        mock_client.assert_called_once_with(\"mongodb://localhost:27017\")\n        mock_client.return_value.__getitem__.assert_called_once_with(\"test_db\")\n        mock_db.list_collection_names.assert_called_once()\n        mock_collection.insert_many.assert_not_called()\n\n\nclass TestDumpStoreToMongoDB:\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb(self, mock_send, temp_store):\n        store, temp_dir = temp_store\n\n        # Add a table to the store\n        sample_data = pyarrow.Table.from_pydict(\n            {\n                \"id\": [1, 2, 3],\n                \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n                \"age\": [30, 25, 35],\n            }\n        )\n        store.add_table(\"test_table\", sample_data)\n\n        # Set up mock\n        mock_send.return_value = 3\n\n        # Call the function\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was called correctly\n        mock_send.assert_called_once()\n        args, kwargs = mock_send.call_args\n        assert kwargs[\"collection_name\"] == \"Cosmotech_test_table\"\n        assert kwargs[\"mongodb_uri\"] == \"mongodb://localhost:27017\"\n        assert kwargs[\"mongodb_db\"] == \"test_db\"\n        assert kwargs[\"replace\"] is True\n\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb_empty(self, mock_send, temp_store):\n        _, temp_dir = temp_store\n\n        # Call the function with an empty store\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was not called\n        mock_send.assert_not_called()\n\n    @patch(\"cosmotech.coal.mongodb.store.send_pyarrow_table_to_mongodb\")\n    def test_dump_store_to_mongodb_multiple_tables(self, mock_send, temp_store):\n        store, temp_dir = temp_store\n\n        # Add multiple tables to the store\n        table1 = pyarrow.Table.from_pydict(\n            {\n                \"id\": [1, 2, 3],\n                \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            }\n        )\n        table2 = pyarrow.Table.from_pydict(\n            {\n                \"id\": [4, 5],\n                \"name\": [\"Dave\", \"Eve\"],\n            }\n        )\n        store.add_table(\"table1\", table1)\n        store.add_table(\"table2\", table2)\n\n        # Set up mock\n        mock_send.side_effect = [3, 2]\n\n        # Call the function\n        dump_store_to_mongodb(\n            temp_dir,\n            \"mongodb://localhost:27017\",\n            \"test_db\",\n            \"Cosmotech_\",\n            True,\n        )\n\n        # Verify the mock was called correctly for each table\n        assert mock_send.call_count == 2\n        call_args_list = mock_send.call_args_list\n\n        # Check first call\n        args, kwargs = call_args_list[0]\n        assert kwargs[\"collection_name\"] in [\"Cosmotech_table1\", \"Cosmotech_table2\"]\n\n        # Check second call\n        args, kwargs = call_args_list[1]\n        assert kwargs[\"collection_name\"] in [\"Cosmotech_table1\", \"Cosmotech_table2\"]\n\n        # Ensure both tables were processed\n        collection_names = [\n            call_args_list[0][1][\"collection_name\"],\n            call_args_list[1][1][\"collection_name\"],\n        ]\n        assert \"Cosmotech_table1\" in collection_names\n        assert \"Cosmotech_table2\" in collection_names\n</code></pre>"},{"location":"tutorials/contributing/#running-the-tests","title":"Running the Tests","text":"<p>To run the tests, use pytest:</p> <pre><code># Run all tests\npytest tests/unit/coal/mongodb/\n\n# Run with coverage\npytest tests/unit/coal/mongodb/ --cov=cosmotech.coal.mongodb --cov-report=term-missing\n</code></pre> <p>Make sure all tests pass and that you have adequate code coverage (at least 80%).</p>"},{"location":"tutorials/contributing/#documentation","title":"Documentation","text":"<p>Documentation is a critical part of the contribution process. All new features must be documented to ensure users can understand and use them effectively.</p>"},{"location":"tutorials/contributing/#updating-cli-documentation","title":"Updating CLI Documentation","text":"<p>Let's add csm-data documentation for our new functionality. Create a new file:</p> <pre><code>touch docs/csm-data/store/dump-to-mongodb.md\n</code></pre> <p>Add the following content:</p> <pre><code>---\nhide:\n  - toc\ndescription: \"Command help: `csm-data store dump-to-mongodb`\"\n---\n# dump-to-mongodb\n\n!!! info \"Help command\"\n    ```text\n    ```\n</code></pre> <p>The documentation build system will generate the content that will be inserted in the file to add a minimal documentation. You can then add more elements as necessary.</p>"},{"location":"tutorials/contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, make sure you've completed all the necessary steps:</p> <ol> <li>Code Quality<ul> <li> Code follows the project's style guidelines (Black formatting)</li> <li> All linting checks pass</li> <li> Code is well-documented with docstrings</li> <li> Code is efficient and follows best practices</li> <li> No unnecessary dependencies are added</li> </ul> </li> <li>Testing<ul> <li> All unit tests pass</li> <li> Test coverage meets or exceeds 80%</li> <li> All functions have at least one test</li> <li> Edge cases and error conditions are tested</li> <li> Mocks are used for external services</li> </ul> </li> <li>Documentation<ul> <li> API documentation is updated</li> <li> Command help text is clear and comprehensive</li> <li> Translation strings are added for all user-facing text</li> <li> Usage examples are provided</li> <li> Any necessary tutorials are created or updated</li> </ul> </li> <li>Integration<ul> <li> New functionality integrates well with existing code</li> <li> No breaking changes to existing APIs</li> <li> Dependencies are properly specified in pyproject.toml</li> <li> Command is registered in the appropriate init.py file</li> </ul> </li> <li>Pull Request Description<ul> <li> Clear description of the changes</li> <li> Explanation of why the changes are needed</li> <li> Any potential issues or limitations</li> <li> References to related issues or discussions</li> </ul> </li> </ol>"},{"location":"tutorials/contributing/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now learned how to contribute to CoAL by implementing a new feature, creating a new csm-data command, writing unit tests, and documenting your work.</p> <p>By following this tutorial, you've gained practical experience with:</p> <ul> <li>Setting up your development environment with Black and pre-commit hooks</li> <li>Understanding the CoAL architecture</li> <li>Implementing new functionality</li> <li>Creating CLI commands</li> <li>Writing unit tests</li> <li>Documenting your work</li> <li>Preparing for a pull request</li> </ul> <p>You're now ready to contribute your own features to CoAL and help improve the platform for everyone.</p> <p>Remember that the CoAL community is here to help. If you have any questions or need assistance, don't hesitate to reach out through GitHub issues or discussions.</p> <p>Happy contributing!</p>"},{"location":"tutorials/cosmotech-api/","title":"Working with the CosmoTech API","text":"<p>Objective</p> <ul> <li>Understand how to authenticate and connect to the CosmoTech API</li> <li>Learn to work with workspaces for file management</li> <li>Master the Twin Data Layer for graph data operations</li> <li>Implement runner and run data management</li> <li>Build complete workflows integrating multiple API features</li> </ul>"},{"location":"tutorials/cosmotech-api/#introduction-to-the-cosmotech-api-integration","title":"Introduction to the CosmoTech API Integration","text":"<p>The CosmoTech Acceleration Library (CoAL) provides a comprehensive set of tools for interacting with the CosmoTech API. This integration allows you to:</p> <ul> <li>Authenticate with different identity providers</li> <li>Manage workspaces and files</li> <li>Work with the Twin Data Layer for graph data</li> <li>Handle runners and runs</li> <li>Process and transform data</li> <li>Build end-to-end workflows</li> </ul> <p>The API integration is organized into several modules, each focused on specific functionality:</p> <ul> <li>connection: Authentication and API client management</li> <li>workspace: Workspace file operations</li> <li>twin_data_layer: Graph data management</li> <li>runner: Runner and run data operations</li> </ul> <p>API vs CLI</p> <p>While the <code>csm-data</code> CLI provides command-line tools for many common operations, the direct API integration offers more flexibility and programmatic control. Use the API integration when you need to:</p> <ul> <li>Build custom workflows</li> <li>Integrate with other Python code</li> <li>Perform complex operations not covered by the CLI</li> <li>Implement real-time interactions with the platform</li> </ul>"},{"location":"tutorials/cosmotech-api/#authentication-and-connection","title":"Authentication and Connection","text":"<p>The first step in working with the CosmoTech API is establishing a connection. CoAL supports multiple authentication methods:</p> <ul> <li>API Key authentication</li> <li>Azure Entra (formerly Azure AD) authentication</li> <li>Keycloak authentication</li> </ul> <p>The <code>get_api_client()</code> function automatically detects which authentication method to use based on the environment variables you've set.</p> Basic connection setup<pre><code># Example: Setting up connections to the CosmoTech API\nimport os\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Method 1: Using API Key (set these environment variables before running)\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\nfrom cosmotech_api.api.organization_api import OrganizationApi\n\norg_api = OrganizationApi(api_client)\n\n# List organizations\norganizations = org_api.find_all_organizations()\nfor org in organizations:\n    print(f\"Organization: {org.name} (ID: {org.id})\")\n\n# Don't forget to close the client when done\napi_client.close()\n\n# Method 2: Using Azure Entra (set these environment variables before running)\n\"\"\"\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_SCOPE\"] = \"api://your-app-id/.default\"  # Replace with your API scope\nos.environ[\"AZURE_CLIENT_ID\"] = \"your-client-id\"  # Replace with your client ID\nos.environ[\"AZURE_CLIENT_SECRET\"] = \"your-client-secret\"  # Replace with your client secret\nos.environ[\"AZURE_TENANT_ID\"] = \"your-tenant-id\"  # Replace with your tenant ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\n# ...\n\n# Don't forget to close the client when done\napi_client.close()\n\"\"\"\n\n# Method 3: Using Keycloak (set these environment variables before running)\n\"\"\"\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"IDP_BASE_URL\"] = \"https://keycloak.example.com/auth/\"  # Replace with your Keycloak URL\nos.environ[\"IDP_TENANT_ID\"] = \"your-realm\"  # Replace with your realm\nos.environ[\"IDP_CLIENT_ID\"] = \"your-client-id\"  # Replace with your client ID\nos.environ[\"IDP_CLIENT_SECRET\"] = \"your-client-secret\"  # Replace with your client secret\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\n# Use the client with various API instances\n# ...\n\n# Don't forget to close the client when done\napi_client.close()\n\"\"\"\n</code></pre> <p>Environment Variables</p> <p>You can set environment variables in your code for testing, but in production environments, it's better to set them at the system or container level for security.</p>"},{"location":"tutorials/cosmotech-api/#api-key-authentication","title":"API Key Authentication","text":"<p>API Key authentication is the simplest method and requires two environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>CSM_API_KEY</code>: Your API key</li> </ul>"},{"location":"tutorials/cosmotech-api/#azure-entra-authentication","title":"Azure Entra Authentication","text":"<p>Azure Entra authentication uses service principal credentials and requires these environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>CSM_API_SCOPE</code>: The API scope (usually in the format <code>api://app-id/.default</code>)</li> <li><code>AZURE_CLIENT_ID</code>: Your client ID</li> <li><code>AZURE_CLIENT_SECRET</code>: Your client secret</li> <li><code>AZURE_TENANT_ID</code>: Your tenant ID</li> </ul>"},{"location":"tutorials/cosmotech-api/#keycloak-authentication","title":"Keycloak Authentication","text":"<p>Keycloak authentication requires these environment variables:</p> <ul> <li><code>CSM_API_URL</code>: The URL of the CosmoTech API</li> <li><code>IDP_BASE_URL</code>: The base URL of your Keycloak server</li> <li><code>IDP_TENANT_ID</code>: Your realm name</li> <li><code>IDP_CLIENT_ID</code>: Your client ID</li> <li><code>IDP_CLIENT_SECRET</code>: Your client secret</li> </ul> <p>API Client Lifecycle</p> <p>Always close the API client when you're done using it to release resources. The best practice is to use a <code>try</code>/<code>finally</code> block to ensure the client is closed even if an error occurs.</p>"},{"location":"tutorials/cosmotech-api/#working-with-workspaces","title":"Working with Workspaces","text":"<p>Workspaces in the CosmoTech platform provide a way to organize and share files. The CoAL library offers functions for listing, downloading, and uploading files in workspaces.</p> Workspace operations<pre><code># Example: Working with workspaces in the CosmoTech API\nimport os\nimport pathlib\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.workspace import (\n    list_workspace_files,\n    download_workspace_file,\n    upload_workspace_file,\n)\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization and workspace IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Example 1: List files in a workspace with a specific prefix\n    file_prefix = \"data/\"  # List files in the \"data\" directory\n    try:\n        files = list_workspace_files(api_client, organization_id, workspace_id, file_prefix)\n        print(f\"Files in workspace with prefix '{file_prefix}':\")\n        for file in files:\n            print(f\"  - {file}\")\n    except ValueError as e:\n        print(f\"Error listing files: {e}\")\n\n    # Example 2: Download a file from the workspace\n    file_to_download = \"data/sample.csv\"  # Replace with an actual file in your workspace\n    target_directory = pathlib.Path(\"./downloaded_files\")\n    target_directory.mkdir(exist_ok=True, parents=True)\n\n    try:\n        downloaded_file = download_workspace_file(\n            api_client, organization_id, workspace_id, file_to_download, target_directory\n        )\n        print(f\"Downloaded file to: {downloaded_file}\")\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n\n    # Example 3: Upload a file to the workspace\n    file_to_upload = \"./local_data/upload_sample.csv\"  # Replace with a local file path\n    workspace_destination = \"data/uploaded/\"  # Destination in the workspace (ending with / to keep filename)\n\n    try:\n        uploaded_file = upload_workspace_file(\n            api_client,\n            organization_id,\n            workspace_id,\n            file_to_upload,\n            workspace_destination,\n            overwrite=True,  # Set to False to prevent overwriting existing files\n        )\n        print(f\"Uploaded file as: {uploaded_file}\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#listing-files","title":"Listing Files","text":"<p>The <code>list_workspace_files</code> function allows you to list files in a workspace with a specific prefix:</p> <pre><code>files = list_workspace_files(api_client, organization_id, workspace_id, file_prefix)\n</code></pre> <p>This is useful for finding files in a specific directory or with a specific naming pattern.</p>"},{"location":"tutorials/cosmotech-api/#downloading-files","title":"Downloading Files","text":"<p>The <code>download_workspace_file</code> function downloads a file from the workspace to a local directory:</p> <pre><code>downloaded_file = download_workspace_file(\n    api_client, \n    organization_id, \n    workspace_id, \n    file_to_download, \n    target_directory\n)\n</code></pre> <p>If the file is in a subdirectory in the workspace, the function will create the necessary local subdirectories.</p>"},{"location":"tutorials/cosmotech-api/#uploading-files","title":"Uploading Files","text":"<p>The <code>upload_workspace_file</code> function uploads a local file to the workspace:</p> <pre><code>uploaded_file = upload_workspace_file(\n    api_client,\n    organization_id,\n    workspace_id,\n    file_to_upload,\n    workspace_destination,\n    overwrite=True\n)\n</code></pre> <p>The <code>workspace_destination</code> parameter can be: - A specific file path in the workspace - A directory path ending with <code>/</code>, in which case the original filename is preserved</p> <p>Workspace Paths</p> <p>When working with workspace paths:</p> <ul> <li>Use forward slashes (<code>/</code>) regardless of your operating system</li> <li>End directory paths with a trailing slash (<code>/</code>)</li> <li>Use relative paths from the workspace root</li> </ul>"},{"location":"tutorials/cosmotech-api/#twin-data-layer-operations","title":"Twin Data Layer Operations","text":"<p>The Twin Data Layer (TDL) is a graph database that stores nodes and relationships. CoAL provides tools for working with the TDL, particularly for preparing and sending CSV data.</p> Twin Data Layer operations<pre><code># Example: Working with the Twin Data Layer in the CosmoTech API\nimport os\nimport pathlib\nimport csv\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.twin_data_layer import CSVSourceFile\nfrom cosmotech_api.api.twin_graph_api import TwinGraphApi\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization and workspace IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\ntwin_graph_id = \"your-twin-graph-id\"  # Replace with your twin graph ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Create a TwinGraphApi instance\n    twin_graph_api = TwinGraphApi(api_client)\n\n    # Example 1: Create sample CSV files for nodes and relationships\n\n    # Create a directory for our sample data\n    data_dir = pathlib.Path(\"./tdl_sample_data\")\n    data_dir.mkdir(exist_ok=True, parents=True)\n\n    # Create a sample nodes CSV file (Person nodes)\n    persons_file = data_dir / \"Person.csv\"\n    with open(persons_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"id\", \"name\", \"age\", \"city\"])\n        writer.writerow([\"p1\", \"Alice\", \"30\", \"New York\"])\n        writer.writerow([\"p2\", \"Bob\", \"25\", \"San Francisco\"])\n        writer.writerow([\"p3\", \"Charlie\", \"35\", \"Chicago\"])\n\n    # Create a sample relationships CSV file (KNOWS relationships)\n    knows_file = data_dir / \"KNOWS.csv\"\n    with open(knows_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"src\", \"dest\", \"since\"])\n        writer.writerow([\"p1\", \"p2\", \"2020\"])\n        writer.writerow([\"p2\", \"p3\", \"2021\"])\n        writer.writerow([\"p3\", \"p1\", \"2019\"])\n\n    print(f\"Created sample CSV files in {data_dir}\")\n\n    # Example 2: Parse CSV files and generate Cypher queries\n\n    # Parse the nodes CSV file\n    person_csv = CSVSourceFile(persons_file)\n    print(f\"Parsed {person_csv.object_type} CSV file:\")\n    print(f\"  Is node: {person_csv.is_node}\")\n    print(f\"  Fields: {person_csv.fields}\")\n    print(f\"  ID column: {person_csv.id_column}\")\n\n    # Generate a Cypher query for creating nodes\n    person_query = person_csv.generate_query_insert()\n    print(f\"\\nGenerated Cypher query for {person_csv.object_type}:\")\n    print(person_query)\n\n    # Parse the relationships CSV file\n    knows_csv = CSVSourceFile(knows_file)\n    print(f\"\\nParsed {knows_csv.object_type} CSV file:\")\n    print(f\"  Is node: {knows_csv.is_node}\")\n    print(f\"  Fields: {knows_csv.fields}\")\n    print(f\"  Source column: {knows_csv.source_column}\")\n    print(f\"  Target column: {knows_csv.target_column}\")\n\n    # Generate a Cypher query for creating relationships\n    knows_query = knows_csv.generate_query_insert()\n    print(f\"\\nGenerated Cypher query for {knows_csv.object_type}:\")\n    print(knows_query)\n\n    # Example 3: Send data to the Twin Data Layer (commented out as it requires an actual twin graph)\n    \"\"\"\n    # For nodes, you would typically:\n    with open(persons_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": person_query,\n                    \"parameters\": params\n                }\n            )\n\n    # For relationships, you would typically:\n    with open(knows_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": knows_query,\n                    \"parameters\": params\n                }\n            )\n    \"\"\"\n\n    # Example 4: Query data from the Twin Data Layer (commented out as it requires an actual twin graph)\n    \"\"\"\n    # Execute a Cypher query to get all Person nodes\n    result = twin_graph_api.run_twin_graph_cypher_query(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        twin_graph_id=twin_graph_id,\n        twin_graph_cypher_query={\n            \"query\": \"MATCH (p:Person) RETURN p.id, p.name, p.age, p.city\",\n            \"parameters\": {}\n        }\n    )\n\n    # Process the results\n    print(\"\\nPerson nodes in the Twin Data Layer:\")\n    for record in result.records:\n        print(f\"  - {record}\")\n    \"\"\"\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#csv-file-format","title":"CSV File Format","text":"<p>The TDL expects CSV files in a specific format:</p> <ul> <li>Node files: Must have an <code>id</code> column and can have additional property columns</li> <li>Relationship files: Must have <code>src</code> and <code>dest</code> columns and can have additional property columns</li> </ul> <p>The filename (without the <code>.csv</code> extension) becomes the node label or relationship type in the graph.</p>"},{"location":"tutorials/cosmotech-api/#parsing-csv-files","title":"Parsing CSV Files","text":"<p>The <code>CSVSourceFile</code> class helps parse CSV files and determine if they represent nodes or relationships:</p> <pre><code>csv_file = CSVSourceFile(file_path)\nprint(f\"Is node: {csv_file.is_node}\")\nprint(f\"Fields: {csv_file.fields}\")\n</code></pre>"},{"location":"tutorials/cosmotech-api/#generating-cypher-queries","title":"Generating Cypher Queries","text":"<p>The <code>generate_query_insert</code> method creates Cypher queries for inserting data into the TDL:</p> <pre><code>query = csv_file.generate_query_insert()\n</code></pre> <p>These queries can then be executed using the TwinGraphApi:</p> <pre><code>twin_graph_api.run_twin_graph_cypher_query(\n    organization_id=organization_id,\n    workspace_id=workspace_id,\n    twin_graph_id=twin_graph_id,\n    twin_graph_cypher_query={\n        \"query\": query,\n        \"parameters\": params\n    }\n)\n</code></pre> <p>Node References</p> <p>When creating relationships, make sure the nodes referenced by the <code>src</code> and <code>dest</code> columns already exist in the graph. Otherwise, the relationship creation will fail.</p>"},{"location":"tutorials/cosmotech-api/#runner-and-run-management","title":"Runner and Run Management","text":"<p>Runners and runs are central concepts in the CosmoTech platform. CoAL provides functions for working with runner data, parameters, and associated datasets.</p> Runner operations<pre><code># Example: Working with runners and runs in the CosmoTech API\nimport os\nimport pathlib\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.runner import (\n    get_runner_data,\n    get_runner_parameters,\n    download_runner_data,\n    download_datasets,\n)\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization, workspace, and runner IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\nrunner_id = \"your-runner-id\"  # Replace with your runner ID\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Example 1: Get runner data\n    runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n    print(f\"Runner name: {runner_data.name}\")\n    print(f\"Runner ID: {runner_data.id}\")\n    print(f\"Runner state: {runner_data.state}\")\n\n    # Example 2: Get runner parameters\n    parameters = get_runner_parameters(runner_data)\n    print(\"\\nRunner parameters:\")\n    for param in parameters:\n        print(f\"  - {param['parameterId']}: {param['value']} (type: {param['varType']})\")\n\n    # Example 3: Download runner data (parameters and datasets)\n    # Create directories for parameters and datasets\n    param_dir = pathlib.Path(\"./runner_parameters\")\n    dataset_dir = pathlib.Path(\"./runner_datasets\")\n    param_dir.mkdir(exist_ok=True, parents=True)\n    dataset_dir.mkdir(exist_ok=True, parents=True)\n\n    # Download runner data\n    result = download_runner_data(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        runner_id=runner_id,\n        parameter_folder=str(param_dir),\n        dataset_folder=str(dataset_dir),\n        read_files=True,  # Read file contents\n        parallel=True,  # Download datasets in parallel\n        write_json=True,  # Write parameters as JSON\n        write_csv=True,  # Write parameters as CSV\n        fetch_dataset=True,  # Fetch datasets\n    )\n\n    print(\"\\nDownloaded runner data:\")\n    print(f\"  - Parameters saved to: {param_dir}\")\n    print(f\"  - Datasets saved to: {dataset_dir}\")\n\n    # Example 4: Working with specific datasets\n    if result[\"datasets\"]:\n        print(\"\\nDatasets associated with the runner:\")\n        for dataset_id, dataset_info in result[\"datasets\"].items():\n            print(f\"  - Dataset ID: {dataset_id}\")\n            print(f\"    Name: {dataset_info.get('name', 'N/A')}\")\n\n            # List files in the dataset\n            if \"files\" in dataset_info:\n                print(f\"    Files:\")\n                for file_info in dataset_info[\"files\"]:\n                    print(f\"      - {file_info.get('name', 'N/A')}\")\n    else:\n        print(\"\\nNo datasets associated with this runner.\")\n\n    # Example 5: Download specific datasets\n    \"\"\"\n    from cosmotech.coal.cosmotech_api.runner import get_dataset_ids_from_runner\n\n    # Get dataset IDs from the runner\n    dataset_ids = get_dataset_ids_from_runner(runner_data)\n\n    if dataset_ids:\n        # Create a directory for the datasets\n        specific_dataset_dir = pathlib.Path(\"./specific_datasets\")\n        specific_dataset_dir.mkdir(exist_ok=True, parents=True)\n\n        # Download the datasets\n        datasets = download_datasets(\n            organization_id=organization_id,\n            workspace_id=workspace_id,\n            dataset_ids=dataset_ids,\n            read_files=True,\n            parallel=True,\n        )\n\n        print(\"\\nDownloaded specific datasets:\")\n        for dataset_id, dataset_info in datasets.items():\n            print(f\"  - Dataset ID: {dataset_id}\")\n            print(f\"    Name: {dataset_info.get('name', 'N/A')}\")\n    \"\"\"\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#getting-runner-data","title":"Getting Runner Data","text":"<p>The <code>get_runner_data</code> function retrieves information about a runner:</p> <pre><code>runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n</code></pre>"},{"location":"tutorials/cosmotech-api/#working-with-parameters","title":"Working with Parameters","text":"<p>The <code>get_runner_parameters</code> function extracts parameters from runner data:</p> <pre><code>parameters = get_runner_parameters(runner_data)\n</code></pre>"},{"location":"tutorials/cosmotech-api/#downloading-runner-data","title":"Downloading Runner Data","text":"<p>The <code>download_runner_data</code> function downloads all data associated with a runner, including parameters and datasets:</p> <pre><code>result = download_runner_data(\n    organization_id=organization_id,\n    workspace_id=workspace_id,\n    runner_id=runner_id,\n    parameter_folder=str(param_dir),\n    dataset_folder=str(dataset_dir),\n    write_json=True,\n    write_csv=True,\n    fetch_dataset=True,\n)\n</code></pre> <p>This function: - Downloads parameters and writes them as JSON and/or CSV files - Downloads associated datasets - Organizes everything in the specified directories</p> <p>Dataset References</p> <p>Runners can reference datasets in two ways:</p> <ul> <li>Through parameters with the <code>%DATASETID%</code> variable type</li> <li>Through the <code>dataset_list</code> property</li> </ul> <p>The <code>download_runner_data</code> function handles both types of references.</p>"},{"location":"tutorials/cosmotech-api/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Putting it all together, here's a complete workflow that demonstrates how to use the CosmoTech API for a data processing pipeline:</p> Complete workflow<pre><code># Example: Complete workflow using the CosmoTech API\nimport os\nimport pathlib\nimport json\nimport csv\nfrom cosmotech.coal.cosmotech_api.connection import get_api_client\nfrom cosmotech.coal.cosmotech_api.runner import (\n    get_runner_data,\n    download_runner_data,\n)\nfrom cosmotech.coal.cosmotech_api.workspace import (\n    list_workspace_files,\n    download_workspace_file,\n    upload_workspace_file,\n)\nfrom cosmotech.coal.cosmotech_api.twin_data_layer import CSVSourceFile\nfrom cosmotech_api.api.twin_graph_api import TwinGraphApi\nfrom cosmotech_api.api.dataset_api import DatasetApi\nfrom cosmotech.coal.utils.logger import LOGGER\n\n# Set up environment variables for authentication\nos.environ[\"CSM_API_URL\"] = \"https://api.cosmotech.com\"  # Replace with your API URL\nos.environ[\"CSM_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key\n\n# Organization, workspace, and runner IDs\norganization_id = \"your-organization-id\"  # Replace with your organization ID\nworkspace_id = \"your-workspace-id\"  # Replace with your workspace ID\nrunner_id = \"your-runner-id\"  # Replace with your runner ID\ntwin_graph_id = \"your-twin-graph-id\"  # Replace with your twin graph ID\n\n# Create directories for our workflow\nworkflow_dir = pathlib.Path(\"./workflow_example\")\nworkflow_dir.mkdir(exist_ok=True, parents=True)\n\ninput_dir = workflow_dir / \"input\"\nprocessed_dir = workflow_dir / \"processed\"\noutput_dir = workflow_dir / \"output\"\n\ninput_dir.mkdir(exist_ok=True, parents=True)\nprocessed_dir.mkdir(exist_ok=True, parents=True)\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Get the API client\napi_client, connection_type = get_api_client()\nLOGGER.info(f\"Connected using: {connection_type}\")\n\ntry:\n    # Step 1: Download runner data (parameters and datasets)\n    print(\"\\n=== Step 1: Download Runner Data ===\")\n\n    runner_data = get_runner_data(organization_id, workspace_id, runner_id)\n    print(f\"Runner name: {runner_data.name}\")\n\n    result = download_runner_data(\n        organization_id=organization_id,\n        workspace_id=workspace_id,\n        runner_id=runner_id,\n        parameter_folder=str(input_dir / \"parameters\"),\n        dataset_folder=str(input_dir / \"datasets\"),\n        write_json=True,\n        write_csv=True,\n    )\n\n    print(f\"Downloaded runner data to {input_dir}\")\n\n    # Step 2: Process the data\n    print(\"\\n=== Step 2: Process Data ===\")\n\n    # For this example, we'll create a simple transformation:\n    # - Read a CSV file from the input\n    # - Transform it\n    # - Write the result to the processed directory\n\n    # Let's assume we have a \"customers.csv\" file in the input directory\n    customers_file = input_dir / \"datasets\" / \"customers.csv\"\n\n    # If the file doesn't exist for this example, create a sample one\n    if not customers_file.exists():\n        print(\"Creating sample customers.csv file for demonstration\")\n        customers_file.parent.mkdir(exist_ok=True, parents=True)\n        with open(customers_file, \"w\", newline=\"\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"id\", \"name\", \"age\", \"city\", \"spending\"])\n            writer.writerow([\"c1\", \"Alice\", \"30\", \"New York\", \"1500\"])\n            writer.writerow([\"c2\", \"Bob\", \"25\", \"San Francisco\", \"2000\"])\n            writer.writerow([\"c3\", \"Charlie\", \"35\", \"Chicago\", \"1200\"])\n\n    # Read the customers data\n    customers = []\n    with open(customers_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            customers.append(row)\n\n    print(f\"Read {len(customers)} customers from {customers_file}\")\n\n    # Process the data: calculate a loyalty score based on age and spending\n    for customer in customers:\n        age = int(customer[\"age\"])\n        spending = int(customer[\"spending\"])\n\n        # Simple formula: loyalty score = spending / 100 + (age - 20) / 10\n        loyalty_score = round(spending / 100 + (age - 20) / 10, 1)\n        customer[\"loyalty_score\"] = str(loyalty_score)\n\n    # Write the processed data\n    processed_file = processed_dir / \"customers_with_loyalty.csv\"\n    with open(processed_file, \"w\", newline=\"\") as f:\n        fieldnames = [\"id\", \"name\", \"age\", \"city\", \"spending\", \"loyalty_score\"]\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(customers)\n\n    print(f\"Processed data written to {processed_file}\")\n\n    # Step 3: Upload the processed file to the workspace\n    print(\"\\n=== Step 3: Upload Processed Data to Workspace ===\")\n\n    try:\n        uploaded_file = upload_workspace_file(\n            api_client,\n            organization_id,\n            workspace_id,\n            str(processed_file),\n            \"processed_data/\",  # Destination in the workspace\n            overwrite=True,\n        )\n        print(f\"Uploaded processed file as: {uploaded_file}\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\n    # Step 4: Create a dataset from the processed data\n    print(\"\\n=== Step 4: Create Dataset from Processed Data ===\")\n\n    # This step would typically involve:\n    # 1. Creating a dataset in the CosmoTech API\n    # 2. Uploading files to the dataset\n\n    \"\"\"\n    # Create a dataset\n    dataset_api = DatasetApi(api_client)\n\n    new_dataset = {\n        \"name\": \"Customers with Loyalty Scores\",\n        \"description\": \"Processed customer data with calculated loyalty scores\",\n        \"tags\": [\"processed\", \"customers\", \"loyalty\"]\n    }\n\n    try:\n        dataset = dataset_api.create_dataset(\n            organization_id=organization_id,\n            workspace_id=workspace_id,\n            dataset=new_dataset\n        )\n\n        dataset_id = dataset.id\n        print(f\"Created dataset with ID: {dataset_id}\")\n\n        # Upload the processed file to the dataset\n        # This would typically involve additional API calls\n        # ...\n\n    except Exception as e:\n        print(f\"Error creating dataset: {e}\")\n    \"\"\"\n\n    # Step 5: Send data to the Twin Data Layer\n    print(\"\\n=== Step 5: Send Data to Twin Data Layer ===\")\n\n    # Parse the processed CSV file for the Twin Data Layer\n    customer_csv = CSVSourceFile(processed_file)\n\n    # Generate a Cypher query for creating nodes\n    customer_query = customer_csv.generate_query_insert()\n    print(f\"Generated Cypher query for Customer nodes:\")\n    print(customer_query)\n\n    # In a real scenario, you would send this data to the Twin Data Layer\n    \"\"\"\n    twin_graph_api = TwinGraphApi(api_client)\n\n    # For each customer, create a node in the Twin Data Layer\n    with open(processed_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Create parameters for the Cypher query\n            params = {k: v for k, v in row.items()}\n\n            # Execute the query\n            twin_graph_api.run_twin_graph_cypher_query(\n                organization_id=organization_id,\n                workspace_id=workspace_id,\n                twin_graph_id=twin_graph_id,\n                twin_graph_cypher_query={\n                    \"query\": customer_query,\n                    \"parameters\": params\n                }\n            )\n    \"\"\"\n\n    # Step 6: Generate a report\n    print(\"\\n=== Step 6: Generate Report ===\")\n\n    # Calculate some statistics\n    total_customers = len(customers)\n    avg_age = sum(int(c[\"age\"]) for c in customers) / total_customers\n    avg_spending = sum(int(c[\"spending\"]) for c in customers) / total_customers\n    avg_loyalty = sum(float(c[\"loyalty_score\"]) for c in customers) / total_customers\n\n    # Create a report\n    report = {\n        \"report_date\": \"2025-02-28\",\n        \"runner_id\": runner_id,\n        \"statistics\": {\n            \"total_customers\": total_customers,\n            \"average_age\": round(avg_age, 1),\n            \"average_spending\": round(avg_spending, 2),\n            \"average_loyalty_score\": round(avg_loyalty, 1),\n        },\n        \"top_customers\": sorted(customers, key=lambda c: float(c[\"loyalty_score\"]), reverse=True)[\n            :2\n        ],  # Top 2 customers by loyalty score\n    }\n\n    # Write the report to a JSON file\n    report_file = output_dir / \"customer_report.json\"\n    with open(report_file, \"w\") as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Report generated and saved to {report_file}\")\n\n    # Print a summary of the report\n    print(\"\\nReport Summary:\")\n    print(f\"Total Customers: {report['statistics']['total_customers']}\")\n    print(f\"Average Age: {report['statistics']['average_age']}\")\n    print(f\"Average Spending: {report['statistics']['average_spending']}\")\n    print(f\"Average Loyalty Score: {report['statistics']['average_loyalty_score']}\")\n    print(\"\\nTop Customers by Loyalty Score:\")\n    for i, customer in enumerate(report[\"top_customers\"], 1):\n        print(f\"{i}. {customer['name']} (Score: {customer['loyalty_score']})\")\n\n    print(\"\\nWorkflow completed successfully!\")\n\nfinally:\n    # Always close the API client when done\n    api_client.close()\n</code></pre> <p>This workflow:</p> <ol> <li>Downloads runner data (parameters and datasets)</li> <li>Processes the data (calculates loyalty scores for customers)</li> <li>Uploads the processed data to the workspace</li> <li>Prepares the data for the Twin Data Layer</li> <li>Generates a report with statistics and insights</li> </ol> <p>Real-world Workflows</p> <p>In real-world scenarios, you might:</p> <ul> <li>Use more complex data transformations</li> <li>Integrate with external systems</li> <li>Implement error handling and retries</li> <li>Add logging and monitoring</li> <li>Parallelize operations for better performance</li> </ul>"},{"location":"tutorials/cosmotech-api/#best-practices-and-tips","title":"Best Practices and Tips","text":""},{"location":"tutorials/cosmotech-api/#authentication","title":"Authentication","text":"<ul> <li>Use environment variables for credentials</li> <li>Implement proper secret management in production</li> <li>Always close API clients when done</li> </ul>"},{"location":"tutorials/cosmotech-api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # API operations\nexcept cosmotech_api.exceptions.ApiException as e:\n    # Handle API errors\n    print(f\"API error: {e.status} - {e.reason}\")\nexcept Exception as e:\n    # Handle other errors\n    print(f\"Error: {e}\")\nfinally:\n    # Always close the client\n    api_client.close()\n</code></pre>"},{"location":"tutorials/cosmotech-api/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Download datasets in parallel when possible (<code>parallel=True</code>)</li> <li>Batch operations when sending multiple items to the API</li> <li>Use appropriate error handling and retries for network operations</li> </ul>"},{"location":"tutorials/cosmotech-api/#security","title":"Security","text":"<ul> <li>Never hardcode credentials in your code</li> <li>Use the principle of least privilege for API keys and service principals</li> <li>Validate and sanitize inputs before sending them to the API</li> </ul>"},{"location":"tutorials/cosmotech-api/#conclusion","title":"Conclusion","text":"<p>The CosmoTech API integration in CoAL provides a powerful way to interact with the CosmoTech platform programmatically. By leveraging these capabilities, you can:</p> <ul> <li>Automate workflows</li> <li>Integrate with other systems</li> <li>Build custom applications</li> <li>Process and analyze data</li> <li>Create end-to-end solutions</li> </ul> <p>Whether you're building data pipelines, creating custom interfaces, or integrating with existing systems, the CoAL library's API integration offers the tools you need to work effectively with the CosmoTech platform.</p>"},{"location":"tutorials/csm-data/","title":"CSM-DATA","text":"<p>Objective</p> <ul> <li>Understand what the csm-data CLI is and its capabilities</li> <li>Learn how to use the various command groups for different data management tasks</li> <li>Explore common use cases and workflows</li> <li>Master integration with CosmoTech platform services</li> </ul>"},{"location":"tutorials/csm-data/#what-is-csm-data","title":"What is csm-data?","text":"<p><code>csm-data</code> is a powerful Command Line Interface (CLI) bundled inside the CosmoTech Acceleration Library (CoAL). It provides a comprehensive set of commands designed to streamline interactions with various services used within a CosmoTech platform.</p> <p>The CLI is organized into several command groups, each focused on specific types of data operations:</p> <ul> <li>api: Commands for interacting with the CosmoTech API</li> <li>store: Commands for working with the CoAL datastore</li> <li>s3-bucket-*: Commands for S3 bucket operations (download, upload, delete)</li> <li>adx-send-runnerdata: Command for sending runner data to Azure Data Explorer</li> <li>az-storage-upload: Command for uploading to Azure Storage</li> </ul> <p>Getting Help</p> <p>You can get detailed help for any command using the <code>--help</code> flag: <pre><code>csm-data --help\ncsm-data api --help\ncsm-data api run-load-data --help\n</code></pre></p>"},{"location":"tutorials/csm-data/#why-use-csm-data","title":"Why use csm-data?","text":""},{"location":"tutorials/csm-data/#standardized-interactions","title":"Standardized Interactions","text":"<p>The <code>csm-data</code> CLI provides tested, standardized interactions with multiple services used in CosmoTech simulations. This eliminates the need to:</p> <ul> <li>Write custom code for common data operations</li> <li>Handle authentication and connection details for each service</li> <li>Manage error handling and retries</li> <li>Deal with format conversions between services</li> </ul>"},{"location":"tutorials/csm-data/#environment-variable-support","title":"Environment Variable Support","text":"<p>Most commands support environment variables, making them ideal for:</p> <ul> <li>Integration with orchestration tools like <code>csm-orc</code></li> <li>Use in Docker containers and cloud environments</li> <li>Secure handling of credentials and connection strings</li> <li>Consistent configuration across development and production</li> </ul>"},{"location":"tutorials/csm-data/#workflow-automation","title":"Workflow Automation","text":"<p>The commands are designed to work together in data processing pipelines, enabling you to:</p> <ul> <li>Download data from various sources</li> <li>Transform and process the data</li> <li>Store results in different storage systems</li> <li>Send data to visualization and analysis services</li> </ul>"},{"location":"tutorials/csm-data/#command-groups-and-use-cases","title":"Command Groups and Use Cases","text":""},{"location":"tutorials/csm-data/#api-commands","title":"API Commands","text":"<p>The <code>api</code> command group facilitates interaction with the CosmoTech API, allowing you to work with scenarios, datasets, and other API resources.</p>"},{"location":"tutorials/csm-data/#runner-data-management","title":"Runner Data Management","text":"Download run data<pre><code>csm-data api run-load-data \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --runner-id \"r-runner\" \\\n  --dataset-absolute-path \"/path/to/dataset/folder\" \\\n  --parameters-absolute-path \"/path/to/parameters/folder\" \\\n  --write-json \\\n  --write-csv \\\n  --fetch-dataset\n</code></pre> <p>This command: - Downloads scenario parameters and datasets from the CosmoTech API - Writes parameters as JSON and/or CSV files - Fetches associated datasets</p> <p>Common Use Case</p> <p>This command is particularly useful in container environments where you need to initialize your simulation with data from the platform. The environment variables are typically set by the platform when launching the container.</p>"},{"location":"tutorials/csm-data/#twin-data-layer-operations","title":"Twin Data Layer Operations","text":"Load files to Twin Data Layer<pre><code>csm-data api tdl-load-files \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --dataset-id \"d-dataset\" \\\n  --source-folder \"/path/to/source/files\"\n</code></pre> Send files to Twin Data Layer<pre><code>csm-data api tdl-send-files \\\n  --organization-id \"o-organization\" \\\n  --workspace-id \"w-workspace\" \\\n  --dataset-id \"d-dataset\" \\\n  --source-folder \"/path/to/source/files\"\n</code></pre> <p>These commands facilitate working with the Twin Data Layer, allowing you to: - Load data from the Twin Data Layer to local files - Send local files to the Twin Data Layer</p>"},{"location":"tutorials/csm-data/#storage-commands","title":"Storage Commands","text":"<p>The <code>s3-bucket-*</code> commands provide a simple interface for working with S3-compatible storage:</p> DownloadUploadDelete Download from S3 bucket<pre><code>csm-data s3-bucket-download \\\n  --target-folder \"/path/to/download/to\" \\\n  --bucket-name \"my-bucket\" \\\n  --prefix-filter \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> Upload to S3 bucket<pre><code>csm-data s3-bucket-upload \\\n  --source-folder \"/path/to/upload/from\" \\\n  --bucket-name \"my-bucket\" \\\n  --target-prefix \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> Delete from S3 bucket<pre><code>csm-data s3-bucket-delete \\\n  --bucket-name \"my-bucket\" \\\n  --prefix-filter \"folder/prefix/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> <p>Environment Variables</p> <p>All these commands support environment variables for credentials and connection details, making them secure and easy to use in automated workflows: <pre><code>export AWS_ENDPOINT_URL=\"https://s3.example.com\"\nexport AWS_ACCESS_KEY_ID=\"access-key-id\"\nexport AWS_SECRET_ACCESS_KEY=\"secret-access-key\"\nexport CSM_DATA_BUCKET_NAME=\"my-bucket\"\n</code></pre></p>"},{"location":"tutorials/csm-data/#azure-data-explorer-integration","title":"Azure Data Explorer Integration","text":"<p>The <code>adx-send-runnerdata</code> command enables sending runner data to Azure Data Explorer for analysis and visualization:</p> Send runner data to ADX<pre><code>csm-data adx-send-runnerdata \\\n  --dataset-absolute-path \"/path/to/dataset/folder\" \\\n  --parameters-absolute-path \"/path/to/parameters/folder\" \\\n  --runner-id \"runner-id\" \\\n  --adx-uri \"https://adx.example.com\" \\\n  --adx-ingest-uri \"https://ingest-adx.example.com\" \\\n  --database-name \"my-database\" \\\n  --send-datasets \\\n  --wait\n</code></pre> <p>This command: - Creates tables in ADX based on CSV files in the dataset and/or parameters folders - Ingests the data into those tables - Adds a <code>run</code> column with the runner ID for tracking - Optionally waits for ingestion to complete</p> <p>Table Creation</p> <p>This command will create tables in ADX based on the CSV file names and headers. Ensure your CSV files have appropriate headers and follow naming conventions suitable for ADX tables.</p>"},{"location":"tutorials/csm-data/#datastore-commands","title":"Datastore Commands","text":"<p>The <code>store</code> command group provides tools for working with the CoAL datastore:</p> Load CSV folder into datastore<pre><code>csm-data store load-csv-folder \\\n  --folder-path \"/path/to/csv/folder\" \\\n  --reset\n</code></pre> Dump datastore to S3<pre><code>csm-data store dump-to-s3 \\\n  --bucket-name \"my-bucket\" \\\n  --target-prefix \"store-dump/\" \\\n  --s3-url \"https://s3.example.com\" \\\n  --access-id \"access-key-id\" \\\n  --secret-key \"secret-access-key\"\n</code></pre> <p>These commands allow you to: - Load data from CSV files into the datastore - Dump datastore contents to various destinations (S3, Azure, PostgreSQL) - List tables in the datastore - Reset the datastore</p>"},{"location":"tutorials/csm-data/#common-workflows-and-integration-patterns","title":"Common Workflows and Integration Patterns","text":""},{"location":"tutorials/csm-data/#runner-data-processing-pipeline","title":"Runner Data Processing Pipeline","text":"<p>A common workflow combines multiple commands to create a complete data processing pipeline:</p> Complete data processing pipeline<pre><code># 1. Download runner data from the API\ncsm-data api run-load-data \\\n  --organization-id \"$CSM_ORGANIZATION_ID\" \\\n  --workspace-id \"$CSM_WORKSPACE_ID\" \\\n  --runner-id \"$CSM_RUNNER_ID\" \\\n  --dataset-absolute-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --parameters-absolute-path \"$CSM_PARAMETERS_ABSOLUTE_PATH\" \\\n  --write-json \\\n  --fetch-dataset\n\n# 2. Load data into the datastore for processing\ncsm-data store load-csv-folder \\\n  --folder-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --reset\n\n# 3. Run your simulation (using your own code)\n# ...\n\n# 4. Send results to Azure Data Explorer for analysis\ncsm-data adx-send-runnerdata \\\n  --dataset-absolute-path \"$CSM_DATASET_ABSOLUTE_PATH\" \\\n  --parameters-absolute-path \"$CSM_PARAMETERS_ABSOLUTE_PATH\" \\\n  --runner-id \"$CSM_RUNNER_ID\" \\\n  --adx-uri \"$AZURE_DATA_EXPLORER_RESOURCE_URI\" \\\n  --adx-ingest-uri \"$AZURE_DATA_EXPLORER_RESOURCE_INGEST_URI\" \\\n  --database-name \"$AZURE_DATA_EXPLORER_DATABASE_NAME\" \\\n  --send-datasets \\\n  --wait\n</code></pre>"},{"location":"tutorials/csm-data/#integration-with-csm-orc","title":"Integration with csm-orc","text":"<p>The <code>csm-data</code> commands integrate seamlessly with <code>csm-orc</code> for orchestration:</p> run.json for csm-orc<pre><code>{\n  \"steps\": [\n    {\n      \"id\": \"download-scenario-data\",\n      \"command\": \"csm-data\",\n      \"arguments\": [\n        \"api\", \"scenariorun-load-data\",\n        \"--write-json\",\n        \"--fetch-dataset\"\n      ],\n      \"useSystemEnvironment\": true\n    },\n    {\n      \"id\": \"run-simulation\",\n      \"command\": \"python\",\n      \"arguments\": [\"run_simulation.py\"],\n      \"precedents\": [\"download-scenario-data\"]\n    },\n    {\n      \"id\": \"send-results-to-adx\",\n      \"command\": \"csm-data\",\n      \"arguments\": [\n        \"adx-send-scenariodata\",\n        \"--send-datasets\",\n        \"--wait\"\n      ],\n      \"useSystemEnvironment\": true,\n      \"precedents\": [\"run-simulation\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/csm-data/#best-practices-and-tips","title":"Best Practices and Tips","text":"<p>Environment Variables</p> <p>Use environment variables for sensitive information and configuration that might change between environments: <pre><code># API connection\nexport CSM_ORGANIZATION_ID=\"o-organization\"\nexport CSM_WORKSPACE_ID=\"w-workspace\"\nexport CSM_SCENARIO_ID=\"s-scenario\"\n\n# Paths\nexport CSM_DATASET_ABSOLUTE_PATH=\"/path/to/dataset\"\nexport CSM_PARAMETERS_ABSOLUTE_PATH=\"/path/to/parameters\"\n\n# ADX connection\nexport AZURE_DATA_EXPLORER_RESOURCE_URI=\"https://adx.example.com\"\nexport AZURE_DATA_EXPLORER_RESOURCE_INGEST_URI=\"https://ingest-adx.example.com\"\nexport AZURE_DATA_EXPLORER_DATABASE_NAME=\"my-database\"\n</code></pre></p> <p>Error Handling</p> <p>Most commands will exit with a non-zero status code on failure, making them suitable for use in scripts and orchestration tools that check exit codes.</p> <p>Logging</p> <p>Control the verbosity of logging with the <code>--log-level</code> option: <pre><code>csm-data --log-level debug api run-load-data ...\n</code></pre></p>"},{"location":"tutorials/csm-data/#extending-csm-data","title":"Extending csm-data","text":"<p>If the existing commands don't exactly match your needs, you have several options:</p> <ol> <li>Use as a basis: Examine the code of similar commands and use it as a starting point for your own scripts</li> <li>Combine commands: Use shell scripting to combine multiple commands into a custom workflow</li> <li>Environment variables: Customize behavior through environment variables without modifying the code</li> <li>Contribute: Consider contributing enhancements back to the CoAL project</li> </ol>"},{"location":"tutorials/csm-data/#conclusion","title":"Conclusion","text":"<p>The <code>csm-data</code> CLI provides a powerful set of tools for managing data in CosmoTech platform environments. By leveraging these commands, you can:</p> <ul> <li>Streamline interactions with platform services</li> <li>Automate data processing workflows</li> <li>Integrate with orchestration tools</li> <li>Focus on your simulation logic rather than data handling</li> </ul> <p>Whether you're developing locally or deploying to production, <code>csm-data</code> offers a consistent interface for your data management needs.</p>"},{"location":"tutorials/datastore/","title":"Datastore","text":"<p>Objective</p> <ul> <li>Understand what the CoAL datastore is and its capabilities</li> <li>Learn how to store and retrieve data in various formats</li> <li>Master SQL querying capabilities for data analysis</li> <li>Build efficient data processing pipelines</li> </ul>"},{"location":"tutorials/datastore/#what-is-the-datastore","title":"What is the datastore?","text":"<p>The datastore is a powerful data management abstraction that provides a unified interface to a SQLite database. It allows you to store, retrieve, transform, and query tabular data in various formats through a consistent API.</p> <p>The core idea behind the datastore is to provide a robust, flexible system for data management that simplifies working with different data formats while offering persistence and advanced query capabilities.</p> <p>Key Features</p> <ul> <li>Format flexibility (Python dictionaries, CSV files, Pandas DataFrames, PyArrow Tables)</li> <li>Persistent storage in SQLite</li> <li>SQL query capabilities</li> <li>Simplified data pipeline management</li> </ul>"},{"location":"tutorials/datastore/#why-use-the-datastore","title":"Why use the datastore?","text":""},{"location":"tutorials/datastore/#format-flexibility","title":"Format Flexibility","text":"<p>The datastore works seamlessly with multiple data formats:</p> <ul> <li>Python dictionaries and lists</li> <li>CSV files</li> <li>Pandas DataFrames</li> <li>PyArrow Tables</li> </ul> <p>This flexibility eliminates the need for manual format conversions and allows you to work with data in your preferred format.</p>"},{"location":"tutorials/datastore/#persistence-and-performance","title":"Persistence and Performance","text":"<p>Instead of keeping all your data in memory or writing/reading from files repeatedly, the datastore:</p> <ul> <li>Persists data in a SQLite database</li> <li>Provides efficient storage and retrieval</li> <li>Handles large datasets that might not fit in memory</li> <li>Maintains data between application runs</li> </ul>"},{"location":"tutorials/datastore/#sql-query-capabilities","title":"SQL Query Capabilities","text":"<p>The datastore leverages the power of SQL:</p> <ul> <li>Filter, aggregate, join, and transform data using familiar SQL syntax</li> <li>Execute complex queries without writing custom data manipulation code</li> <li>Perform operations that would be cumbersome with file-based approaches</li> </ul>"},{"location":"tutorials/datastore/#simplified-data-pipeline","title":"Simplified Data Pipeline","text":"<p>The datastore serves as a central hub in your data processing pipeline:</p> <ul> <li>Import data from various sources</li> <li>Transform and clean data</li> <li>Query and analyze data</li> <li>Export results in different formats</li> </ul>"},{"location":"tutorials/datastore/#basic-example","title":"Basic example","text":"Basic use of the datastore<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\n# We initialize and reset the data store\nmy_datastore = Store(reset=True)\n\n# We create a simple list of dict data\nmy_data = [{\"foo\": \"bar\"}, {\"foo\": \"barbar\"}, {\"foo\": \"world\"}, {\"foo\": \"bar\"}]\n\n# We use a bundled method to send the py_list to the store\nstore_pylist(\"my_data\", my_data)\n\n# We can make a sql query over our data\n# Store.execute_query returns a pyarrow.Table object so we can make use of Table.to_pylist to get an equivalent format\nresults = my_datastore.execute_query(\"SELECT foo, count(*) as line_count FROM my_data GROUP BY foo\").to_pylist()\n\n# We can print our results now\nprint(results)\n# &gt; [{'foo': 'bar', 'line_count': 2}, {'foo': 'barbar', 'line_count': 1}, {'foo': 'world', 'line_count': 1}]\n</code></pre>"},{"location":"tutorials/datastore/#working-with-different-data-formats","title":"Working with different data formats","text":"<p>The datastore provides specialized adapters for working with various data formats:</p>"},{"location":"tutorials/datastore/#csv-files","title":"CSV Files","text":"Working with CSV files<pre><code>import pathlib\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.csv import store_csv_file, convert_store_table_to_csv\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Load data from a CSV file\ncsv_path = pathlib.Path(\"path/to/your/data.csv\")\nstore_csv_file(\"customers\", csv_path)\n\n# Query the data\nhigh_value_customers = store.execute_query(\n    \"\"\"\n    SELECT * FROM customers \n    WHERE annual_spend &gt; 10000\n    ORDER BY annual_spend DESC\n\"\"\"\n)\n\n# Export results to a new CSV file\noutput_path = pathlib.Path(\"path/to/output/high_value_customers.csv\")\nconvert_store_table_to_csv(\"high_value_customers\", output_path)\n</code></pre>"},{"location":"tutorials/datastore/#pandas-dataframes","title":"Pandas DataFrames","text":"Working with pandas DataFrames<pre><code>import pandas as pd\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.pandas import store_dataframe, convert_store_table_to_dataframe\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(\n    {\n        \"product_id\": [1, 2, 3, 4, 5],\n        \"product_name\": [\"Widget A\", \"Widget B\", \"Gadget X\", \"Tool Y\", \"Device Z\"],\n        \"price\": [19.99, 29.99, 99.99, 49.99, 199.99],\n        \"category\": [\"Widgets\", \"Widgets\", \"Gadgets\", \"Tools\", \"Devices\"],\n    }\n)\n\n# Store the DataFrame\nstore_dataframe(\"products\", df)\n\n# Query the data\nexpensive_products = store.execute_query(\n    \"\"\"\n    SELECT * FROM products\n    WHERE price &gt; 50\n    ORDER BY price DESC\n\"\"\"\n)\n\n# Convert results back to a pandas DataFrame for further analysis\nexpensive_df = convert_store_table_to_dataframe(\"expensive_products\", store)\n\n# Use pandas methods on the result\nprint(expensive_df.describe())\n</code></pre>"},{"location":"tutorials/datastore/#pyarrow-tables","title":"PyArrow Tables","text":"Working with PyArrow Tables directly<pre><code>import pyarrow as pa\nfrom cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.pyarrow import store_table\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Create a PyArrow Table\ndata = {\n    \"date\": pa.array([\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"]),\n    \"value\": pa.array([100, 150, 200]),\n    \"category\": pa.array([\"A\", \"B\", \"A\"]),\n}\ntable = pa.Table.from_pydict(data)\n\n# Store the table\nstore_table(\"time_series\", table)\n\n# Query and retrieve data\nresult = store.execute_query(\n    \"\"\"\n    SELECT date, SUM(value) as total_value\n    FROM time_series\n    GROUP BY date\n\"\"\"\n)\n\nprint(result)\n</code></pre>"},{"location":"tutorials/datastore/#advanced-use-cases","title":"Advanced use cases","text":""},{"location":"tutorials/datastore/#joining-multiple-tables","title":"Joining multiple tables","text":"Joining tables in the datastore<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist\n\nstore = Store(reset=True)\n\n# Store customer data\ncustomers = [\n    {\"customer_id\": 1, \"name\": \"Acme Corp\", \"segment\": \"Enterprise\"},\n    {\"customer_id\": 2, \"name\": \"Small Shop\", \"segment\": \"SMB\"},\n    {\"customer_id\": 3, \"name\": \"Tech Giant\", \"segment\": \"Enterprise\"},\n]\nstore_pylist(\"customers\", customers, store=store)\n\n# Store order data\norders = [\n    {\"order_id\": 101, \"customer_id\": 1, \"amount\": 5000},\n    {\"order_id\": 102, \"customer_id\": 2, \"amount\": 500},\n    {\"order_id\": 103, \"customer_id\": 1, \"amount\": 7500},\n    {\"order_id\": 104, \"customer_id\": 3, \"amount\": 10000},\n]\nstore_pylist(\"orders\", orders, store=store)\n\n# Join tables to analyze orders by customer segment\nresults = store.execute_query(\n    \"\"\"\n    SELECT c.segment, COUNT(o.order_id) as order_count, SUM(o.amount) as total_revenue\n    FROM customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n    GROUP BY c.segment\n\"\"\"\n).to_pylist()\n\nprint(results)\n# &gt; [{'segment': 'Enterprise', 'order_count': 3, 'total_revenue': 22500}, {'segment': 'SMB', 'order_count': 1, 'total_revenue': 500}]\n</code></pre>"},{"location":"tutorials/datastore/#data-transformation-pipeline","title":"Data transformation pipeline","text":"Complete pipelineStep-by-step Building a data transformation pipeline<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.native_python import store_pylist, convert_table_as_pylist\nimport pathlib\nfrom cosmotech.coal.store.csv import store_csv_file, convert_store_table_to_csv\n\n# Initialize the store\nstore = Store(reset=True)\n\n# 1. Load raw data from CSV\nraw_data_path = pathlib.Path(\"path/to/raw_data.csv\")\nstore_csv_file(\"raw_data\", raw_data_path, store=store)\n\n# 2. Clean and transform the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE cleaned_data AS\n    SELECT \n        id,\n        TRIM(name) as name,\n        UPPER(category) as category,\n        CASE WHEN value &lt; 0 THEN 0 ELSE value END as value\n    FROM raw_data\n    WHERE id IS NOT NULL\n\"\"\"\n)\n\n# 3. Aggregate the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE summary_data AS\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value\n    FROM cleaned_data\n    GROUP BY category\n\"\"\"\n)\n\n# 4. Export the results\nsummary_data = convert_table_as_pylist(\"summary_data\", store=store)\nprint(summary_data)\n\n# 5. Save to CSV for reporting\noutput_path = pathlib.Path(\"path/to/output/summary.csv\")\nconvert_store_table_to_csv(\"summary_data\", output_path, store=store)\n</code></pre> Step 1: Load data<pre><code>from cosmotech.coal.store.store import Store\nfrom cosmotech.coal.store.csv import store_csv_file\nimport pathlib\n\n# Initialize the store\nstore = Store(reset=True)\n\n# Load raw data from CSV\nraw_data_path = pathlib.Path(\"path/to/raw_data.csv\")\nstore_csv_file(\"raw_data\", raw_data_path, store=store)\n</code></pre> Step 2: Clean data<pre><code># Clean and transform the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE cleaned_data AS\n    SELECT \n        id,\n        TRIM(name) as name,\n        UPPER(category) as category,\n        CASE WHEN value &lt; 0 THEN 0 ELSE value END as value\n    FROM raw_data\n    WHERE id IS NOT NULL\n\"\"\"\n)\n</code></pre> Step 3: Aggregate data<pre><code># Aggregate the data\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE summary_data AS\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        SUM(value) as total_value\n    FROM cleaned_data\n    GROUP BY category\n\"\"\"\n)\n</code></pre> Step 4: Export results<pre><code>from cosmotech.coal.store.native_python import convert_table_as_pylist\nfrom cosmotech.coal.store.csv import convert_store_table_to_csv\nimport pathlib\n\n# Export to Python list\nsummary_data = convert_table_as_pylist(\"summary_data\", store=store)\nprint(summary_data)\n\n# Save to CSV for reporting\noutput_path = pathlib.Path(\"path/to/output/summary.csv\")\nconvert_store_table_to_csv(\"summary_data\", output_path, store=store)\n</code></pre>"},{"location":"tutorials/datastore/#best-practices-and-tips","title":"Best practices and tips","text":"<p>Store initialization</p> <ul> <li>Use <code>reset=True</code> when you want to start with a fresh database</li> <li>Omit the reset parameter or set it to <code>False</code> when you want to maintain data between runs</li> <li>Specify a custom location with the <code>store_location</code> parameter if needed</li> </ul> Store initialization options<pre><code># Fresh store each time\nstore = Store(reset=True)\n\n# Persistent store at default location\nstore = Store()\n\n# Persistent store at custom location\nimport pathlib\n\ncustom_path = pathlib.Path(\"/path/to/custom/location\")\nstore = Store(store_location=custom_path)\n</code></pre> <p>Table management</p> <ul> <li>Use descriptive table names that reflect the data content</li> <li>Check if tables exist before attempting operations</li> <li>List available tables to explore the database</li> </ul> Table management<pre><code># Check if a table exists\nif store.table_exists(\"customers\"):\n    # Do something with the table\n    pass\n\n# List all tables\nfor table_name in store.list_tables():\n    print(f\"Table: {table_name}\")\n    # Get schema information\n    schema = store.get_table_schema(table_name)\n    print(f\"Schema: {schema}\")\n</code></pre> <p>Performance considerations</p> <ul> <li>For large datasets, consider chunking data when loading</li> <li>Use SQL to filter data early rather than loading everything into memory</li> <li>Index frequently queried columns for better performance</li> </ul> Handling large datasets<pre><code># Example of chunking data load\nchunk_size = 10000\nfor i in range(0, len(large_dataset), chunk_size):\n    chunk = large_dataset[i : i + chunk_size]\n    store_pylist(f\"data_chunk_{i//chunk_size}\", chunk, store=store)\n\n# Combine chunks with SQL\nstore.execute_query(\n    \"\"\"\n    CREATE TABLE combined_data AS\n    SELECT * FROM data_chunk_0\n    UNION ALL\n    SELECT * FROM data_chunk_1\n    -- Add more chunks as needed\n\"\"\"\n)\n</code></pre>"},{"location":"tutorials/datastore/#integration-with-cosmotech-ecosystem","title":"Integration with CosmoTech ecosystem","text":"<p>The datastore is designed to work seamlessly with other components of the CosmoTech Acceleration Library:</p> <ul> <li>Data loading: Load data from various sources into the datastore</li> <li>Runner management: Store runner parameters and results</li> <li>API integration: Exchange data with CosmoTech APIs</li> <li>Reporting: Generate reports and visualizations from stored data</li> </ul> <p>This integration makes the datastore a central component in CosmoTech-based data processing workflows.</p>"},{"location":"tutorials/datastore/#conclusion","title":"Conclusion","text":"<p>The datastore provides a powerful, flexible foundation for data management in your CosmoTech applications. By leveraging its capabilities, you can:</p> <ul> <li>Simplify data handling across different formats</li> <li>Build robust data processing pipelines</li> <li>Perform complex data transformations and analyses</li> <li>Maintain data persistence between application runs</li> <li>Integrate seamlessly with other CosmoTech components</li> </ul> <p>Whether you're working with small datasets or large-scale data processing tasks, the datastore offers the tools you need to manage your data effectively.</p>"}]}